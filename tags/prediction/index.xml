<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Sookocheff</title>
    <link>http://sookocheff.com/tags/prediction/</link>
    <language>en-us</language>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <lastBuildDate>Wed, 05 Aug 2015 05:41:06 UTC</lastBuildDate>
    
    <item>
      <title>Counting N-Grams with Cloud Dataflow</title>
      <link>http://sookocheff.com/post/nlp/counting-n-grams-with-cloud-dataflow/</link>
      <pubDate>Wed, 05 Aug 2015 05:41:06 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/nlp/counting-n-grams-with-cloud-dataflow/</guid>
      <description>

&lt;p&gt;Counting &lt;a href=&#34;http://sookocheff.com/post/nlp/n-gram-modeling/&#34;&gt;n-grams&lt;/a&gt; is a common
pre-processing step for computing sentence and word probabilities over a corpus.
Thankfully, this task is &lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly
parallel&lt;/a&gt; and is a
natural fit for distributed processing frameworks like &lt;a href=&#34;https://cloud.google.com/dataflow/&#34;&gt;Cloud
Dataflow&lt;/a&gt;. This article provides an
implementation of n-gram counting using Cloud Dataflow that is able to
efficiently compute n-grams in parallel over massive datasets.&lt;/p&gt;

&lt;h2 id=&#34;the-algorithm:373238c81a8bcfdf6cdd9d4aa8562076&#34;&gt;The Algorithm&lt;/h2&gt;

&lt;p&gt;Cloud Dataflow uses a programming abstraction called &lt;code&gt;PCollections&lt;/code&gt; which are
collections of data that can be operated on in parallel (Parallel Collections).
When programming for Cloud Dataflow you treat each operation as a transformation
of a parallel collection that returns another parallel collection for further
processing. This style of development is similar to the traditional Unix
philosophy of piping the output of one command to another for further
processing.&lt;/p&gt;

&lt;p&gt;An outline of the algorithm for counting n-grams is presented in the following
figure. The first stage of our dataflow pipeline is reading all lines of our
input. We then proceed to extract n-grams from each individual line, outputting
the results as a &lt;code&gt;PCollection&lt;/code&gt;. We then count the n-grams and take the top
n-grams in our dataset. Lastly, the results are output to a file.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/dataflow-graph.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/dataflow-graph.png&#34; alt=&#34;Dataflow Graph&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;As a concrete example, we can represent the same algorithm as transformations on
a text file. In this example we will count the occurrence of bigrams.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;I am Sam. I am Kevin.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, the file is read as input and bigrams are extracted.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(&#39;I&#39;, &#39;am&#39;)
(&#39;am&#39;, &#39;Sam.&#39;)
(&#39;Sam.&#39;, &#39;I&#39;)
(&#39;I&#39;, &#39;am&#39;)
(&#39;am&#39;, &#39;Kevin.&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, for each element, we count the number of occurrences. This happens in two
stages. First, we group all elements by key. This has the effect of combining
all tuples with the same value to be on one line.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(&#39;I&#39;, &#39;am&#39;), (&#39;I&#39;, &#39;am&#39;)
(&#39;am&#39;, &#39;Sam.&#39;)
(&#39;Sam.&#39;, &#39;I&#39;)
(&#39;am&#39;, &#39;Kevin.&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here, we simply count the number of elements in each group.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(&#39;I&#39;, &#39;am&#39;), 2
(&#39;am&#39;, &#39;Sam.&#39;), 1
(&#39;Sam.&#39;, &#39;I&#39;), 1
(&#39;am&#39;, &#39;Kevin.&#39;), 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In Cloud Dataflow, the previous operations are combined into the
&lt;code&gt;Count.PerElement&lt;/code&gt; operation that counts the number of times an element occurs.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/count-per-element.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/count-per-element.png&#34; alt=&#34;Count.PerElement&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Once all the elements are grouped and counted, we can extract the top &lt;code&gt;x&lt;/code&gt;
elements. To do this, we need to be able to combine elements across machines and
across files. Dataflow provides the &lt;code&gt;Combine.PerKey&lt;/code&gt; operation for this purpose.
This operation merges elements from multiple files into a single file. We can
then take the top &lt;code&gt;x&lt;/code&gt; results to view the top &lt;code&gt;x&lt;/code&gt; bigrams. Dataflow provides a
convenience function &lt;code&gt;Top.Globally&lt;/code&gt; to extract the top &lt;code&gt;x&lt;/code&gt; results from a
&lt;code&gt;PCollection&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/top-globally.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/top-globally.png&#34; alt=&#34;Top.Globally&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;show-me-the-code:373238c81a8bcfdf6cdd9d4aa8562076&#34;&gt;Show Me The Code&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s go ahead and express our algorithm using Cloud Dataflow. The algorithm is
expressed in two parts. First, extracting n-grams from a block of text. This is
a simple transformation that takes a block of text as input and repeatedly
outputs individual n-grams. This list of n-grams serves as our initial
&lt;code&gt;PCollection&lt;/code&gt; for the rest of the algorithm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/**
* This DoFn tokenizes lines of text into individual ngrams;
* we pass it to a ParDo in the pipeline.
*/
static class ExtractNGramsFn extends DoFn&amp;lt;String, String&amp;gt; {
  private static final long serialVersionUID = 0;
  
  private Integer n;
  
  public ExtractNGramsFn(Integer n) {
    this.n = n;
  }
  
  @Override
  public void processElement(ProcessContext c) {
    // Split the line into words.
    String[] words = c.element().split(&amp;quot;\\s+&amp;quot;);
  
    // Group into ngrams
    List&amp;lt;String&amp;gt; ngrams = new ArrayList&amp;lt;String&amp;gt;();
    for (int i = 0; i &amp;lt;= words.length-this.n; i++) {
      StringBuilder ngram = new StringBuilder();
      for (int j = 0; j &amp;lt; this.n; j++) {
        if (j &amp;gt; 0) {
          ngram.append(&amp;quot;\t&amp;quot;);
        }
        ngram.append(words[i+j]);
      }
      ngrams.add(ngram.toString());
    }
  
    // Output each ngram encountered into the output PCollection.
    for (String ngram : ngrams) {
      if (!ngram.isEmpty()) {
        c.output(ngram);
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Second, we use the &lt;code&gt;PCollection&lt;/code&gt; of all n-grams as input to a transform that outputs
the list of most frequently encountered n-grams in the corpus.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/**
* A PTransform that converts a PCollection containing lines of text into a PCollection of
* word counts.
*/
public static class CountNGrams
  extends PTransform&amp;lt;PCollection&amp;lt;String&amp;gt;, PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt;&amp;gt; {
  
  private static final long serialVersionUID = 0;
  
  private Integer n;
  private Integer top;
  
  public CountNGrams(Integer n) {
    this.n = n;
    this.top = new Integer(100);
  }
  
  public CountNGrams(Integer n, Integer top) {
    this.n = n;
    this.top = top;
  }
  
  @Override
  public PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt; apply(PCollection&amp;lt;String&amp;gt; lines) {
  
    // Convert lines of text into individual ngrams.
    PCollection&amp;lt;String&amp;gt; ngrams = lines.apply(
        ParDo.of(new ExtractNGramsFn(this.n)));
  
    // Count the number of times each ngram occurs.
    PCollection&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt; ngramCounts =
        ngrams.apply(Count.&amp;lt;String&amp;gt;perElement());
  
    // Find the top ngrams in the corpus.
    PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt; topNgrams = 
        ngramCounts.apply(Top.of(this.top, new SerializableComparator&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;() {
                  private static final long serialVersionUID = 0;
  
                  @Override
                  public int compare(KV&amp;lt;String, Long&amp;gt; o1, KV&amp;lt;String, Long&amp;gt; o2) {
                    return Long.compare(o1.getValue(), o2.getValue());
                  }
                }).withoutDefaults());
    
    return topNgrams;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;full-source-code:373238c81a8bcfdf6cdd9d4aa8562076&#34;&gt;Full Source Code&lt;/h2&gt;

&lt;p&gt;The rest of the code is boilerplate to setup the pipeline and accept user input.
Feel free to use this code as a basis for your own pipelines.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;package com.sookocheff.cloud.dataflow.examples;

import com.google.cloud.dataflow.sdk.Pipeline;
import com.google.cloud.dataflow.sdk.io.TextIO;
import com.google.cloud.dataflow.sdk.options.DataflowPipelineOptions;
import com.google.cloud.dataflow.sdk.options.Default;
import com.google.cloud.dataflow.sdk.options.DefaultValueFactory;
import com.google.cloud.dataflow.sdk.options.Description;
import com.google.cloud.dataflow.sdk.options.PipelineOptions;
import com.google.cloud.dataflow.sdk.options.PipelineOptionsFactory;
import com.google.cloud.dataflow.sdk.transforms.Aggregator;
import com.google.cloud.dataflow.sdk.transforms.Count;
import com.google.cloud.dataflow.sdk.transforms.DoFn;
import com.google.cloud.dataflow.sdk.transforms.PTransform;
import com.google.cloud.dataflow.sdk.transforms.ParDo;
import com.google.cloud.dataflow.sdk.transforms.Sum;
import com.google.cloud.dataflow.sdk.transforms.Top;
import com.google.cloud.dataflow.sdk.transforms.SerializableComparator;
import com.google.cloud.dataflow.sdk.util.gcsfs.GcsPath;
import com.google.cloud.dataflow.sdk.values.KV;
import com.google.cloud.dataflow.sdk.values.PCollection;

import java.io.IOException;
import java.util.*;


/**
 * Count N-Grams.
 */
public class NGramCount {

  /**
   * This DoFn tokenizes lines of text into individual ngrams; we pass it to a ParDo in the
   * pipeline.
   */
  static class ExtractNGramsFn extends DoFn&amp;lt;String, String&amp;gt; {
    private static final long serialVersionUID = 0;

    private Integer n;

    public ExtractNGramsFn(Integer n) {
      this.n = n;
    }

    private final Aggregator&amp;lt;Long, Long&amp;gt; ngramCount =
        createAggregator(&amp;quot;ngramCount&amp;quot;, new Sum.SumLongFn());

    @Override
    public void processElement(ProcessContext c) {
      // Split the line into words (splits at any whitespace character, grouping
      // whitespace together).
      String[] words = c.element().split(&amp;quot;\\s+&amp;quot;);

      // Group into ngrams
      List&amp;lt;String&amp;gt; ngrams = new ArrayList&amp;lt;String&amp;gt;();
      for (int i = 0; i &amp;lt;= words.length-this.n; i++) {
        StringBuilder ngram = new StringBuilder();
        for (int j = 0; j &amp;lt; this.n; j++) {
          if (j &amp;gt; 0) {
            ngram.append(&amp;quot;\t&amp;quot;);
          }
          ngram.append(words[i+j]);
        }
        ngrams.add(ngram.toString());
      }

      // Output each ngram encountered into the output PCollection.
      for (String ngram : ngrams) {
        if (!ngram.isEmpty()) {
          ngramCount.addValue(1L);
          c.output(ngram);
        }
      }
    }
  }

  /** A DoFn that converts an NGram and Count into a printable string. */
  public static class FormatAsTextFn extends DoFn&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;, String&amp;gt; {
    private static final long serialVersionUID = 0;

    @Override
    public void processElement(ProcessContext c) {

      for (KV&amp;lt;String, Long&amp;gt; item : c.element()) {
        String ngram = item.getKey();
        long count = item.getValue();
        c.output(ngram + &amp;quot;\t&amp;quot; + count);
      }
    }
  }

  /**
   * A PTransform that converts a PCollection containing lines of text into a PCollection of
   * word counts.
   */
  public static class CountNGrams
    extends PTransform&amp;lt;PCollection&amp;lt;String&amp;gt;, PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt;&amp;gt; {

    private static final long serialVersionUID = 0;

    private Integer n;
    private Integer top;

    public CountNGrams(Integer n) {
      this.n = n;
      this.top = new Integer(100);
    }

    public CountNGrams(Integer n, Integer top) {
      this.n = n;
      this.top = top;
    }

    @Override
    public PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt; apply(PCollection&amp;lt;String&amp;gt; lines) {

      // Convert lines of text into individual ngrams.
      PCollection&amp;lt;String&amp;gt; ngrams = lines.apply(
          ParDo.of(new ExtractNGramsFn(this.n)));

      // Count the number of times each ngram occurs.
      PCollection&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt; ngramCounts =
          ngrams.apply(Count.&amp;lt;String&amp;gt;perElement());

      // Find the top ngrams in the corpus
      PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt; topNgrams = 
          ngramCounts.apply(Top.of(this.top, new SerializableComparator&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;() {
                    private static final long serialVersionUID = 0;

                    @Override
                    public int compare(KV&amp;lt;String, Long&amp;gt; o1, KV&amp;lt;String, Long&amp;gt; o2) {
                      return Long.compare(o1.getValue(), o2.getValue());
                    }
                  }).withoutDefaults());
      
      return topNgrams;
    }
  }

  /**
   * Options supported by {@link NGramCount}.
   */
  public static interface NGramCountOptions extends PipelineOptions {
    @Description(&amp;quot;Number of n-grams to model.&amp;quot;)
    @Default.Integer(2)
    Integer getN();
    void setN(Integer value);

    @Description(&amp;quot;Number top n-gram counts to return.&amp;quot;)
    @Default.Integer(100)
    Integer getTop();
    void setTop(Integer value);

    @Description(&amp;quot;Path of the file to read from.&amp;quot;)
    @Default.String(&amp;quot;gs://dataflow-samples/shakespeare/kinglear.txt&amp;quot;)
    String getInputFile();
    void setInputFile(String value);

    @Description(&amp;quot;Path of the file to write to.&amp;quot;)
    @Default.InstanceFactory(OutputFactory.class)
    String getOutput();
    void setOutput(String value);

    /**
     * Returns gs://${STAGING_LOCATION}/&amp;quot;counts.txt&amp;quot; as the default destination.
     */
    public static class OutputFactory implements DefaultValueFactory&amp;lt;String&amp;gt; {
      @Override
      public String create(PipelineOptions options) {
        DataflowPipelineOptions dataflowOptions = options.as(DataflowPipelineOptions.class);
        if (dataflowOptions.getStagingLocation() != null) {
          return GcsPath.fromUri(dataflowOptions.getStagingLocation())
              .resolve(&amp;quot;counts.txt&amp;quot;).toString();
        } else {
          throw new IllegalArgumentException(&amp;quot;Must specify --output or --stagingLocation&amp;quot;);
        }
      }
    }

  }

  public static void main(String[] args) throws IOException {
    NGramCountOptions options = PipelineOptionsFactory.fromArgs(args).withValidation()
      .as(NGramCountOptions.class);
    Pipeline p = Pipeline.create(options);

    p.apply(TextIO.Read.named(&amp;quot;ReadLines&amp;quot;).from(options.getInputFile()))
     .apply(new CountNGrams(options.getN(), options.getTop()))
     .apply(ParDo.of(new FormatAsTextFn()))
     .apply(TextIO.Write.named(&amp;quot;WriteCounts&amp;quot;).to(options.getOutput()));

    p.run();
  }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>N-gram Modeling With Markov Chains</title>
      <link>http://sookocheff.com/post/nlp/ngram-modeling-with-markov-chains/</link>
      <pubDate>Fri, 31 Jul 2015 06:23:43 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/nlp/ngram-modeling-with-markov-chains/</guid>
      <description>

&lt;p&gt;A common method of reducing the complexity of n-gram modeling is using the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_property&#34;&gt;Markov Property&lt;/a&gt;. The Markov
Property states that the probability of future states depends only on the
present state, not on the sequence of events that preceded it. This concept can
be elegantly implemented using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain&#34;&gt;Markov
Chain&lt;/a&gt; storing the probabilities of
transitioning to a next state.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at a simple example of a Markov Chain that models text using bigrams.
The following code creates a list of bigrams from a piece of text.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; s = &amp;quot;I am Sam. Sam I am. I do not like green eggs and ham.&amp;quot;
&amp;gt;&amp;gt;&amp;gt; tokens = s.split(&amp;quot; &amp;quot;)
&amp;gt;&amp;gt;&amp;gt; bigrams = [(tokens[i],tokens[i+1]) for i in range(0,len(tokens)-1)]
&amp;gt;&amp;gt;&amp;gt; bigrams
[(&#39;I&#39;, &#39;am&#39;), (&#39;am&#39;, &#39;Sam.&#39;), (&#39;Sam.&#39;, &#39;Sam&#39;), (&#39;Sam&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;am.&#39;), (&#39;am.&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;do&#39;), (&#39;do&#39;, &#39;not&#39;), (&#39;not&#39;, &#39;like&#39;), (&#39;like&#39;, &#39;green&#39;), (&#39;green&#39;, &#39;eggs&#39;), (&#39;eggs&#39;, &#39;and&#39;), (&#39;and&#39;, &#39;ham.&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Listing the bigrams starting with the word &lt;code&gt;I&lt;/code&gt; results in:
&lt;code&gt;I am&lt;/code&gt;, &lt;code&gt;I am.&lt;/code&gt;, and &lt;code&gt;I do&lt;/code&gt;. If we were to use this data to predict a word that
follows the word &lt;code&gt;I&lt;/code&gt; we have three choices and each of them has the same
probability (&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;) of being a valid choice. Modeling this using a Markov Chain
results in a state machine with an approximately 0.33 chance of transitioning to
any one of the next states.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/transitions-from-I.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/transitions-from-I.png&#34; alt=&#34;Transitions from I&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We can add additional transitions to our Chain by considering additional bigrams
starting with &lt;code&gt;am&lt;/code&gt;, &lt;code&gt;am.&lt;/code&gt;, and &lt;code&gt;do&lt;/code&gt;. In each case, there is only one possible
choice for the next state in our Markov Chain given the bigrams we know from our
input text. Each transition from one of these states therefore has a 1.0
probability.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/following-transitions-from-I.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/following-transitions-from-I.png&#34; alt=&#34;Following Transitions from I&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Now, given a starting point in our chain, say &lt;code&gt;I&lt;/code&gt;, we can follow the transitions
to predict a sequence of words. This sequence follows the probability
distribution of the bigrams we have learned. For example, we can randomly sample
from the possible transitions from &lt;code&gt;I&lt;/code&gt; to arrive at the next possible state in
the machine.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import random
&amp;gt;&amp;gt;&amp;gt; random.sample([&#39;am&#39;, &#39;am.&#39;, &#39;do&#39;], 1)
[&#39;am.&#39;]
&amp;gt;&amp;gt;&amp;gt; random.sample([&#39;am&#39;, &#39;am.&#39;, &#39;do&#39;], 1)
[&#39;do&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Making the first transition, to &lt;code&gt;do&lt;/code&gt;, we can sample from the possible states
following &lt;code&gt;do&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; random.sample([&#39;am&#39;, &#39;am.&#39;, &#39;do&#39;], 1)
[&#39;do&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;writing-a-markov-chain:d97a8c10263ba1350fa7aeccc4b468c0&#34;&gt;Writing a Markov Chain&lt;/h2&gt;

&lt;p&gt;We have all the building blocks we need to write a complete Markov Chain
implementation. The implementation is a simple dictionary with each key being
the current state and the value being the list of possible next states. For
example, after learning the text &lt;code&gt;I am Sam.&lt;/code&gt; our dictionary would look like
this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
    &#39;I&#39;: [&#39;am&#39;],
    &#39;am&#39;: [&#39;Sam.&#39;],
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And after adding the text &lt;code&gt;Sam I am.&lt;/code&gt; our dictionary would look like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
    &#39;I&#39;: [&#39;am&#39;, &#39;am.&#39;],
    &#39;am&#39;: [&#39;Sam.&#39;],
    &#39;Sam&#39;: [&#39;I&#39;],
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can implement a basic Markov Chain that creates a bigram dictionary using the
following code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MarkovChain:

    def __init__(self):
        self.memory = {}

    def _learn_key(self, key, value):
        if key not in self.memory:
            self.memory[key] = []

        self.memory[key].append(value)

    def learn(self, text):
        tokens = text.split(&amp;quot; &amp;quot;)
        bigrams = [(tokens[i], tokens[i + 1]) for i in range(0, len(tokens) - 1)]
        for bigram in bigrams:
            self._learn_key(bigram[0], bigram[1])


if __name__ == &#39;__main__&#39;:
    m = MarkovChain()
    m.learn(&#39;I am Sam. Sam I am. I do not like green eggs and ham.&#39;)
    print(m.memory)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; python markov_chain.py
{&#39;I&#39;: [&#39;am&#39;, &#39;am.&#39;, &#39;do&#39;],
 &#39;Sam&#39;: [&#39;I&#39;],
 &#39;Sam.&#39;: [&#39;Sam&#39;],
 &#39;am&#39;: [&#39;Sam.&#39;],
 &#39;am.&#39;: [&#39;I&#39;],
 &#39;and&#39;: [&#39;ham.&#39;],
 &#39;do&#39;: [&#39;not&#39;],
 &#39;eggs&#39;: [&#39;and&#39;],
 &#39;green&#39;: [&#39;eggs&#39;],
 &#39;like&#39;: [&#39;green&#39;],
 &#39;not&#39;: [&#39;like&#39;]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then transition to a new state in our Markov Chain by randomly
choosing a next state given the current state. If we do not have any information
on the current state we can randomly pick a state to start in.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _next(self, current_state):
    next_possible = self.memory.get(current_state)

    if not next_possible:
        next_possible = self.memory.keys()

    return random.sample(next_possible, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The transition probabilities between states naturally become weighted as we
learn more text.  For example, in the following sequence we learn a few
sentences with the same bigrams and in the final state we are twice as likely to
choose &lt;code&gt;am&lt;/code&gt; as the next word following &lt;code&gt;I&lt;/code&gt; by randomly sampling from the next
possible states.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from markov_chain import MarkovChain
&amp;gt;&amp;gt;&amp;gt; m = MarkovChain()
&amp;gt;&amp;gt;&amp;gt; m.learn(&#39;I am Sam.&#39;)
&amp;gt;&amp;gt;&amp;gt; m.memory
{&#39;I&#39;: [&#39;am&#39;], &#39;am&#39;: [&#39;Sam.&#39;]}
&amp;gt;&amp;gt;&amp;gt; m.learn(&#39;I am Kevin.&#39;)
&amp;gt;&amp;gt;&amp;gt; m.memory
{&#39;I&#39;: [&#39;am&#39;, &#39;am&#39;], &#39;am&#39;: [&#39;Sam.&#39;, &#39;Kevin.&#39;]}
&amp;gt;&amp;gt;&amp;gt; m.learn(&#39;I do.&#39;)
&amp;gt;&amp;gt;&amp;gt; m.memory  # Twice as likely to follow &#39;I&#39; with &#39;am&#39; than &#39;do&#39;.
{&#39;I&#39;: [&#39;am&#39;, &#39;am&#39;, &#39;do&#39;], &#39;am&#39;: [&#39;Sam.&#39;, &#39;Kevin.&#39;]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The state machine produced by our code would have the probabilities in the
following figure.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/learned-probabilities.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/learned-probabilities.png&#34; alt=&#34;Learned Probabilities&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Finally, we can ask our chain to print out some text of an arbitrary length by
following the transitions between the text we have learned.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def babble(self, amount, state=&#39;&#39;):
    if not amount:
        return state

    next_word = self._next(state)

    if not next_word:
        return state

    return state + &#39; &#39; + self.babble(amount - 1, next_word)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting it all together we have a simple Markov Chain that can learn bigrams and
babble text given the probability of bigrams that it has learned. Markov Chain&amp;rsquo;s
are a simple way to store and query n-gram probabilities. Full source code for
this example follows.&lt;/p&gt;

&lt;h2 id=&#34;the-implementation:d97a8c10263ba1350fa7aeccc4b468c0&#34;&gt;The Implementation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random


class MarkovChain:

    def __init__(self):
        self.memory = {}

    def _learn_key(self, key, value):
        if key not in self.memory:
            self.memory[key] = []

        self.memory[key].append(value)

    def learn(self, text):
        tokens = text.split(&amp;quot; &amp;quot;)
        bigrams = [(tokens[i], tokens[i + 1]) for i in range(0, len(tokens) - 1)]
        for bigram in bigrams:
            self._learn_key(bigram[0], bigram[1])

    def _next(self, current_state):
        next_possible = self.memory.get(current_state)

        if not next_possible:
            next_possible = self.memory.keys()

        return random.sample(next_possible, 1)[0]

    def babble(self, amount, state=&#39;&#39;):
        if not amount:
            return state

        next_word = self._next(state)
        return state + &#39; &#39; + self.babble(amount - 1, next_word)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Modeling Natural Language with N-Gram Models</title>
      <link>http://sookocheff.com/post/nlp/n-gram-modeling/</link>
      <pubDate>Sat, 25 Jul 2015 06:41:06 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/nlp/n-gram-modeling/</guid>
      <description>

&lt;p&gt;One of the most widely used methods natural language is n-gram modeling. This
article explains what an n-gram model is, how it is computed, and what the
probabilities of an n-gram model tell us.&lt;/p&gt;

&lt;h2 id=&#34;what-is-an-n-gram:6aa7a6edb627f8b743e3120c4f84c63a&#34;&gt;What is an n-gram?&lt;/h2&gt;

&lt;p&gt;&lt;blockquote&gt;
  &lt;p&gt;An n-gram is a contiguous sequence of n items from a given sequence of text.&lt;/p&gt;
  &lt;footer&gt;Wikipedia &lt;cite title=&#34;https://en.wikipedia.org/wiki/N-gram&#34;&gt;https://en.wikipedia.org/wiki/N-gram&lt;/cite&gt;&lt;/footer&gt;
&lt;/blockquote&gt;
&lt;/p&gt;

&lt;p&gt;Given a sentence, &lt;code&gt;s&lt;/code&gt;, we can construct a list of n-grams from &lt;code&gt;s&lt;/code&gt; by finding
pairs of words that occur next to each other. For example, given the sentence &amp;ldquo;I
am Sam&amp;rdquo; you can construct bigrams (n-grams of length 2) by finding consecutive
pairs of words.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; s = &amp;quot;I am Sam.&amp;quot;
&amp;gt;&amp;gt;&amp;gt; tokens = s.split(&amp;quot; &amp;quot;)
&amp;gt;&amp;gt;&amp;gt; bigrams = [(tokens[i],tokens[i+1]) for i in range(0,len(tokens)-1)]
&amp;gt;&amp;gt;&amp;gt; bigrams
[(&#39;I&#39;, &#39;am&#39;), (&#39;am&#39;, &#39;Sam.&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;calculating-n-gram-probability:6aa7a6edb627f8b743e3120c4f84c63a&#34;&gt;Calculating n-gram Probability&lt;/h2&gt;

&lt;p&gt;Given a list of n-grams we can count the number of occurrences of each n-gram;
this count determines the frequency with which an n-gram occurs throughout our
document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from collections import Counter
&amp;gt;&amp;gt;&amp;gt; count = Counter(bigrams)
&amp;gt;&amp;gt;&amp;gt; count
[((&#39;am&#39;, &#39;Sam.&#39;), 1), ((&#39;I&#39;, &#39;am&#39;), 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this small corpus we only count one occurrence of each n-gram. By dividing
these counts by the size of all n-grams in our list we would get a probability
of 0.5 of each n-gram occurring.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look a larger corpus of words and see what the probabilities can tell us.
The following sequence of bigrams was computed from data downloaded from &lt;a href=&#34;http://www.corpora.heliohost.org/&#34;&gt;HC
Corpora&lt;/a&gt;. It lists the 20 most frequently
encountered bigrams out of 97,810,566 bigrams in the entire corpus.&lt;/p&gt;

&lt;p&gt;This data represents the most frequently used pairs of words in the corpus along
with the number of times they occur.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;of	the	421560
in	the	380608
to	the	207571
for	the	190683
on	the	184430
to	be	153285
at	the	128980
and	the	114232
in	a	109527
with	the	99141
is	a	99053
for	a	90209
from	the	82223
with	a	78918
will	be	78049
of	a	78009
I	was	76788
I	have	76621
going	to	75088
is	the	70045
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By consulting our frequency table of bigrams, we can tell that the sentence
&lt;code&gt;There was heavy rain last night&lt;/code&gt; is much more likely to be grammatically
correct than the sentence &lt;code&gt;There was large rain last night&lt;/code&gt; by the fact that the
bigram &lt;code&gt;heavy rain&lt;/code&gt; occurs much more frequently than &lt;code&gt;large rain&lt;/code&gt; in our corpus.
Said another way, the probability of the bigram &lt;code&gt;heavy rain&lt;/code&gt; is larger than the
probability of the bigram &lt;code&gt;large rain&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;sentences-as-probability-models:6aa7a6edb627f8b743e3120c4f84c63a&#34;&gt;Sentences as probability models&lt;/h2&gt;

&lt;p&gt;More precisely, we can use n-gram models to derive a probability of the sentence
,&lt;code&gt;W&lt;/code&gt;, as the joint probability of each individual word in the sentence, &lt;code&gt;wi&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(W) = P(w1, w2, ..., wn)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be reduced to a sequence of n-grams using the Chain Rule of
conditional probability.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(x1, x2, ..., xn) = P(x1)P(x2|x1)...P(xn|x1,...xn-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a concrete example, let&amp;rsquo;s predict the probability of the sentence &lt;code&gt;There was
heavy rain&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(&#39;There was heavy rain&#39;) = P(&#39;There&#39;, &#39;was&#39;, &#39;heavy&#39;, &#39;rain&#39;)
P(&#39;There was heavy rain&#39;) = P(&#39;There&#39;)P(&#39;was&#39;|&#39;There&#39;)P(&#39;heavy&#39;|&#39;There was&#39;)P(&#39;rain&#39;|&#39;There was heavy&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each of the terms on the right hand side of this equation are n-gram
probabilities that we can estimate using the counts of n-grams in our corpus. To
calculate the probability of the entire sentence, we just need to lookup the
probabilities of each component part in the conditional probability.&lt;/p&gt;

&lt;p&gt;Unfortunately, this formula does not scale since we cannot compute n-grams of
every length. For example, consider the case where we have solely bigrams in our
model; we have no way of knowing the probability `P(&amp;lsquo;rain&amp;rsquo;|&amp;lsquo;There was&amp;rsquo;) from
bigrams.&lt;/p&gt;

&lt;p&gt;By using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_property&#34;&gt;Markov Assumption&lt;/a&gt;,
we can simplify our equation by assuming that future states in our model only
depend upon the present state of our model. This assumption means that we can
reduce our conditional probabilities to be approximately equal so that&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(&#39;rain&#39;|&#39;There was heavy&#39;) ~ P(&#39;rain&#39;|&#39;heavy&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;More generally, we can estimate the probability of a sentence by the
probabilities of each component part. In the equation that follows, the
probability of the sentence is reduced to the probabilities of the sentence&amp;rsquo;s
individual bigrams.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(&#39;There was heavy rain&#39;) ~ P(&#39;There&#39;)P(&#39;was&#39;|&#39;There&#39;)P(&#39;heavy&#39;|&#39;was&#39;)P(&#39;rain&#39;|&#39;heavy&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;applications:6aa7a6edb627f8b743e3120c4f84c63a&#34;&gt;Applications&lt;/h2&gt;

&lt;p&gt;What can we use n-gram models for? Given the probabilities of a sentence we can
determine the likelihood of an automated machine translation being correct, we
could predict the next most likely word to occur in a sentence, we could
automatically generate text from speech, automate spelling correction, or
determine the relative sentiment of a piece of text.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using the Google Prediction API to Predict the Sentiment of a Tweet</title>
      <link>http://sookocheff.com/post/prediction-api/predicting-sentiment/</link>
      <pubDate>Mon, 20 Oct 2014 06:23:04 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/prediction-api/predicting-sentiment/</guid>
      <description>

&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/prediction/&#34;&gt;Google Prediction API&lt;/a&gt; offers the
power of Google&amp;rsquo;s machine learning algorithms over a RESTful API interface. The
machine learning algorithms themselves are a complete black box. As a user you
upload the training data and, once it has been analyzed, start classifying new
observations based on the analysis of the training data. I recently spent some
time investigating how to use the API to determine the sentiment of a tweet.
This article collects my thoughts on the experience and a few recommendations
for future work.&lt;/p&gt;

&lt;h2 id=&#34;the-data:f6b93bab65a5a37625cdf99557b98292&#34;&gt;The Data&lt;/h2&gt;

&lt;p&gt;For our experiment we took the text and rating of one million online reviews and
normalized them within a scale of zero to 1000 &amp;ndash; ratings on a scale of one to
four and ratings on a scale of one to ten would be roughly equivalent. We then
segmented the reviews into five broad categories: very negative (0-200),
negative (200-400), neutral (400-600), positive (600-800), very
positive (800-1000). The prediction API requires the data to be in a
specific format; following their guidelines, we stripped the review
text of all punctuation except the apostrophe and lower
cased all characters. What was left was a one million row table with
two columns: the review category and the review content.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;very negative, &amp;quot;the waiter was so mean&amp;quot;
positive, &amp;quot;the bisque is the best in town&amp;quot;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our data was roughly 1 GB. We uploaded this file to Google Cloud Storage
and used the Prediction API to train our model given this dataset.&lt;/p&gt;

&lt;h2 id=&#34;examples:f6b93bab65a5a37625cdf99557b98292&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Once we had a trained model it was time to make predictions. For our application
we took tweets from Twitter mentioning a business and asked the Prediction API
to classify the text of the tweet for sentiment between very negative to very
positive using the normalized review categories of our model. The results were
decidedly mixed as the following examples show. In the first example we attempt
to classify the text &amp;ldquo;this restaurant has the best soup in town&amp;rdquo; and correctly
receive a &amp;ldquo;very positive&amp;rdquo; result.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-10-20-prediction-api/bestsoup.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-10-20-prediction-api/bestsoup.png&#34; alt=&#34;The Best Soup in Town&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;As a counter example, the text &amp;ldquo;this restaurant has the worst soup in town&amp;rdquo; also
recieves a &amp;ldquo;very positive&amp;rdquo; result, although with less confidence and with &amp;ldquo;very
negative&amp;rdquo; being the most likely second choice.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-10-20-prediction-api/worstsoup.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-10-20-prediction-api/worstsoup.png&#34; alt=&#34;The Worst Soup in Town&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;conclusions:f6b93bab65a5a37625cdf99557b98292&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Most of the tweets were categorized as very positive, regardless of content. In
addition, most of the tweets had almost equal likelihood of being in the very
negative or very positive category with very positive being more likely most of
the time.&lt;/p&gt;

&lt;p&gt;Why is this?&lt;/p&gt;

&lt;p&gt;Most Internet reviews are either very positive or very negative so most of the
content from the tweet will fall into one of these categories in our model. I
believe that by adjusting our training data to have equal amounts of reviews for
each category we would get better results.&lt;/p&gt;

&lt;p&gt;My recommendation is that if you intend to use the Prediction API for a serious
business task that you also have a strong enough background in machine learning
to tweak your model &lt;em&gt;before&lt;/em&gt; using the Prediction API to analyze and host it. In
short, use the Prediction API as cloud-based access to your existing model that
you already know works. Don&amp;rsquo;t use the Prediction API to help you build a working
model. The black box nature of the Prediction API makes it difficult to diagnose
and correct any data problems you may have.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
