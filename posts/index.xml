<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Sookocheff</title>
    <link>http://sookocheff.com/posts/</link>
    <language>en-us</language>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <lastBuildDate>Tue, 12 May 2015 05:57:19 CST</lastBuildDate>
    
    <item>
      <title>App Engine Pipelines API - Part 2: Connecting Pipelines</title>
      <link>http://sookocheff.com/posts/2015-05-12-app-engine-pipelines-part-two-connecting-pipelines/</link>
      <pubDate>Tue, 12 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-05-12-app-engine-pipelines-part-two-connecting-pipelines/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://sookocheff.com/posts/2015-05-05-app-engine-pipelines-part-one-the-basics/&#34;&gt;Last time&lt;/a&gt;,
we discussed basic pipeline instantiation and execution. This time, we will
cover sequential pipelines, answering the question &amp;ldquo;How do I connect the output
of one pipeline with the input of another pipeline&amp;rdquo;?&lt;/p&gt;

&lt;p&gt;To begin, let&amp;rsquo;s review a basic pipeline that squares its input. If any of this
does not make sense refer to the &lt;a href=&#34;http://sookocheff.com/posts/2015-05-05-app-engine-pipelines-part-one-the-basics/&#34;&gt;first part of this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        stage = SquarePipeline(10)
        stage.start()


class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        return number * number

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.default.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first step in passing data between two pipelines is updating our pipeline to
use the generator interface. The generator interface uses the &lt;code&gt;yield&lt;/code&gt; keyword as
a means of connecting pipelines together. For this contrived example, let&amp;rsquo;s
create a &lt;em&gt;parent&lt;/em&gt; pipeline that executes &lt;code&gt;SquarePipeline&lt;/code&gt; twice in succession.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What now? We need a way to access the value stored in &lt;code&gt;second_square&lt;/code&gt;. When
execution hits a &lt;code&gt;yield&lt;/code&gt; statement a task is started to run the pipeline and a
&lt;code&gt;PipelineFuture&lt;/code&gt; is returned. The &lt;code&gt;PipelineFuture&lt;/code&gt; will have a value &lt;em&gt;after&lt;/em&gt; the
task has finished executing but not immediately. So how do we access the value?
With a &lt;em&gt;child&lt;/em&gt; pipeline that can read the result. In this example, we simply log
the value of the computation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
        yield LogResult(second_square)

class LogResult(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;All done! Value is %s&#39;, number)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The rule of thumb here is that &lt;em&gt;anything you instantiate your pipeline with (and
subsequently pass to the &lt;code&gt;run&lt;/code&gt; method) is accessible within your
pipeline&lt;/em&gt;. These are called &lt;em&gt;immediate values&lt;/em&gt; and you can treat them as regular
Python values. When this code is executed, each pipeline started by a &lt;code&gt;yield&lt;/code&gt;
call is a separate App Engine Task that executes in the Task Queue. The Pipeline
runtime coordinates running these tasks and shares the results of execution
between tasks, allowing you to safely connect pipelines together.&lt;/p&gt;

&lt;p&gt;Full source code for this example follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        stage = TwiceSquaredPipeline(10)
        stage.start()


class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        return number * number


class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
        yield LogResult(second_square)


class LogResult(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;All done! Value is %s&#39;, number)


routes = [
    webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)
]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 1: The Basics</title>
      <link>http://sookocheff.com/posts/2015-05-05-app-engine-pipelines-part-one-the-basics/</link>
      <pubDate>Tue, 05 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-05-05-app-engine-pipelines-part-one-the-basics/</guid>
      <description>

&lt;p&gt;The &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines&#34;&gt;Pipelines API&lt;/a&gt;
is a general purpose workflow engine for App Engine applications. With the
Pipelines API we can connect together complex workflows into a coherent run time
backed by the Datastore. This article provides a basic overview of the Pipelines
API and how it can be used for abritrary computational workflows.&lt;/p&gt;

&lt;p&gt;In the most basic sense a Pipeline is an object that takes input, performs some
logic or computation on that input, and produces output. Pipelines can take two
general forms &amp;ndash; synchronous or asynchronous. Synchronous pipelines act as basic
functions that must complete during a single request. Asynchronous pipelines
spawn child pipelines and connect them together into a workflow by passing input
and output parameters around.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A word of warning.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pipelines must be idempotent and it is up to the developer to ensure that they
are &amp;ndash; this is not enforced by the run-time. A pipeline may fail and be retried
and it is important that running the same pipeline with the same set of inputs
will product the same results.&lt;/p&gt;

&lt;h2 id=&#34;getting-started:6f756ce5c8adfdb788373dfc5173280d&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;The first step is to grab the latest version of the Pipelines API (and its
        dependencies) using pip. The following assumes you install third party
App Engine dependencies in the lib directory relative to where pip is being run.
You can also grab the source code from
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install GoogleAppEnginePipeline -t lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pipeline requests need to be handled by the Pipeline application. We set that up
by adding a handler to &lt;code&gt;app.yaml&lt;/code&gt;. Since these are internal application requrest
we can secure them using the &lt;code&gt;login: admin&lt;/code&gt; directive.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;handlers:
- url: /_ah/pipeline.*
  script: pipeline.handlers._APP
  login: admin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;basic-synchronous-pipelines:6f756ce5c8adfdb788373dfc5173280d&#34;&gt;Basic Synchronous Pipelines&lt;/h2&gt;

&lt;p&gt;A synchronous pipeline runs within the bounds of a single App Engine request.
Once the request has been made the pipeline starts and pipeline processing
happens automatically. We can set up this pipeline by defining a handler
responsible for starting the pipeline. For now, create a default handler that
will receive a request at the URL of your choosing.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2

class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        logging.info(&#39;Launch pipeline&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A request processed by this handler will kick off our Pipeline. To define a
pipeline we inherit from the Pipeline object and the method &lt;code&gt;run&lt;/code&gt;. The pipeline
is launched via the &lt;code&gt;start&lt;/code&gt; method. The code below instantiates a custom
pipeline and launches it. Accessing the URL for the RunPipelineHandler will
print the message &amp;lsquo;Do something here&amp;rsquo; to the logs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline

class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        logging.info(&#39;Launch pipeline&#39;)
        pipeline = MyPipeline()
        pipeline.start()


class MyPipeline(pipeline.Pipeline):
    def run(self, *args, **kwargs):
        logging.info(&#39;Do something here.&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can update our pipeline to do a simple operation, like squaring a number.
You&amp;rsquo;ll notice in the code that follows that the arguments passed when
initializing the pipeline are accessible as parameters to the &lt;code&gt;run&lt;/code&gt; method
within the pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this pipeline will show that the pipeline executes correctly. But where
does our return value go? How can we access the output of &lt;code&gt;SquarePipeline&lt;/code&gt;?&lt;/p&gt;

&lt;h2 id=&#34;accessing-pipeline-output:6f756ce5c8adfdb788373dfc5173280d&#34;&gt;Accessing Pipeline Output&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll notice that in &lt;code&gt;SquarePipeline&lt;/code&gt; we are returning a value directly but
we never actually access it. Pipeline output can only ever be accessed after the
pipeline has finished executing. We can check for the end of pipeline execution
using the &lt;code&gt;has_finalized&lt;/code&gt; property. This property will be set to &lt;code&gt;True&lt;/code&gt; when all
stages of a pipeline have finished executing. At this point in time our output
will be available as a value on the Pipeline object. Let&amp;rsquo;s see what happens when
we try to check if our pipeline has finalized. To do this we need to store the
pipeline_id generated from our start method and check the &lt;code&gt;has_finalized&lt;/code&gt;
property.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()

        pipeline_id = square_stage.pipeline_id

        stage = SquarePipeline.from_id(pipeline_id)
        if stage.has_finalized:
            logging.info(&#39;Finalized&#39;)
        else:
            logging.info(&#39;Not finalized&#39;)


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the preceding code we see that our pipeline is not finalized. What
happened here? The pipeline is executed as an ayschronous task after it has been
started and may or may not complete by the time we check that it has finalized.
The pipeline itself is a future whose value has not materialized. Any output
from a pipeline is not actually available until all child pipeline tasks are
executed. So how do we get the final value of the SquarePipeline?&lt;/p&gt;

&lt;h2 id=&#34;finalized:6f756ce5c8adfdb788373dfc5173280d&#34;&gt;Finalized&lt;/h2&gt;

&lt;p&gt;The finalized method is called by the pipeline API once a Pipeline has completed
its work (by filling all of is slots &amp;ndash; to be described later). By overriding
the &lt;code&gt;finalized&lt;/code&gt; method we can see the result of our pipeline and do further
processing on that result if necessary. By default our output is set to
&lt;code&gt;self.outputs.default.value&lt;/code&gt;. As an example, executing the following code will
log the message &amp;ldquo;All done! Square is 100&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.default.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will see in a later article how to connect the output of one pipeline with
another.&lt;/p&gt;

&lt;h2 id=&#34;named-outputs:6f756ce5c8adfdb788373dfc5173280d&#34;&gt;Named outputs&lt;/h2&gt;

&lt;p&gt;Pipelines also allow you to explicitly name outputs, this is useful in the case
where you have more than one output to return or as a means of passing data
between one pipeline execution and the next. When using named outputs, instead
of returning a value from the &lt;code&gt;run&lt;/code&gt; method we fill a pipeline slot with our
value. To use named outputs we define an &lt;code&gt;output_names&lt;/code&gt; class variable listing
the names of our outputs. By calling &lt;code&gt;self.fill&lt;/code&gt; on our named output we store
the return value of our pipeline for later access in the &lt;code&gt;run&lt;/code&gt; method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):

    output_names = [&#39;square&#39;]

    def run(self, number):
        self.fill(self.outputs.square, number * number)

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.square.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-a-pipeline:6f756ce5c8adfdb788373dfc5173280d&#34;&gt;Testing a pipeline&lt;/h2&gt;

&lt;p&gt;Sometimes our pipelines call out over the wire or perform expensive data
operations. The Pipeline API provides a convenient way to test pipelines. By
calling &lt;code&gt;start_test&lt;/code&gt; instead of &lt;code&gt;start&lt;/code&gt;. In our example we verify the
expected output of our squaring pipeline by calling &lt;code&gt;start_test&lt;/code&gt;. The final
value of our pipeline is available immediately.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start_test()
        assert stage.outputs.square.value == 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we need to mock out any behaviour from our &lt;code&gt;run&lt;/code&gt; method, we can supply a
&lt;code&gt;run_test&lt;/code&gt; method that is executed whenever we run our pipeline with
&lt;code&gt;start_test&lt;/code&gt;. Within this method we can mock out or adjust the behaviour of the
pipeline to work under test.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:6f756ce5c8adfdb788373dfc5173280d&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This article gives a basic outline of how to start and execute pipelines. Full
source code for the final example is listed below. In the next article we will
see how to pass the output of one pipeline to another and understand how parent
and child pipelines interact.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):

    output_names = [&#39;square&#39;]

    def run(self, number):
        self.fill(self.outputs.square, number * number)

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.square.value)

routes = [
    webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)
]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Durabledict for App Engine</title>
      <link>http://sookocheff.com/posts/2015-04-28-durabledict-for-app-engine/</link>
      <pubDate>Wed, 29 Apr 2015 06:19:23 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-04-28-durabledict-for-app-engine/</guid>
      <description>

&lt;h2 id=&#34;tldr:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;tldr;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vendasta/datastoredict&#34;&gt;DatastoreDict&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-s-a-durabledict:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;What&amp;rsquo;s a durabledict?&lt;/h2&gt;

&lt;p&gt;Good question. &lt;a href=&#34;https://github.com/disqus/durabledict&#34;&gt;Durabledict&lt;/a&gt; is a Python
implementation of a persistent dictionary. The dictionary values are cached
locally and sync with the datastore whenever a value in the datastore changes.&lt;/p&gt;

&lt;p&gt;Disqus provides concrete implementations for Redis, Django, ZooKeeper and in
memory. This blog post details an implementation using the App Engine datastore
and memcache.&lt;/p&gt;

&lt;h2 id=&#34;creating-your-own-durabledict:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;Creating your own durabledict&lt;/h2&gt;

&lt;p&gt;By following the &lt;a href=&#34;https://github.com/disqus/durabledict&#34;&gt;guide the durabledict
README&lt;/a&gt; we can create our own
implementation. We need to subclass &lt;code&gt;durabledict.base.DurableDict&lt;/code&gt; and implement
the following interface methods. Strictly speaking, &lt;code&gt;_pop&lt;/code&gt; and &lt;code&gt;_setdefault&lt;/code&gt; do
not have to be implemented but doing so makes your durabledict behave like a
base dict in all cases.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;persist(key, value)&lt;/code&gt; - Persist value at key to your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;depersist(key)&lt;/code&gt; - Delete the value at key from your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;durables()&lt;/code&gt; - Return a key=val dict of all keys in your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;last_updated()&lt;/code&gt; - A comparable value of when the data in your data store was last updated.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_pop(key, default=None)&lt;/code&gt; - If key is in the dictionary, remove it and return its value, else return default. If default is not given and key is not in the dictionary, a KeyError is raised.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_setdefault(key, default=None)&lt;/code&gt; - If key is in the dictionary, return its value. If not, insert key with a value of default and return default. default defaults to None.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s implement these one-by-one.&lt;/p&gt;

&lt;h3 id=&#34;persist-key-value:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;persist(key, value)&lt;/h3&gt;

&lt;p&gt;Persisting a value to the datastore is a relatively simple operation. If the key
already exists we update it&amp;rsquo;s value. If the key does not already exist we create
it. To aid with this operation we create a &lt;code&gt;get_or_create&lt;/code&gt; method that will
return an existing entity if one exists or create a new entity if one does not
exist.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def persist(self, key, val):
    instance, created = get_or_create(self.model, key, val)

    if not created and instance.value != val:
        instance.value = val
        instance.put()

    self.touch_last_updated()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last line of this function updates the last time this durabledict was
changed. This is used for caching. We create the &lt;code&gt;last_updated&lt;/code&gt; and
&lt;code&gt;touch_last_updated&lt;/code&gt; functions now.&lt;/p&gt;

&lt;h3 id=&#34;last-updated-key-value:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;last_updated(key, value)&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def last_updated(self):
    return self.cache.get(self.cache_key)

def touch_last_updated(self):
    self.cache.incr(self.cache_key, initial_value=self.last_synced + 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;init:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;&lt;strong&gt;init&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We now have the building blocks to create our initial durabledict. Within the
&lt;code&gt;__init__&lt;/code&gt; method we set a manager and cache instance. The manager is
responsible for ndb datastore operations to decouple the ndb interface from the
durabledict implementation. We decouple our caching method in a similar fashion.
We also set the initial value of the cache whenever we create a new instance of
the durabledict.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import memcache

from durabledict.base import DurableDict
from durabledict.encoding import NoOpEncoding


class DatastoreDict(DurableDict):

    def __init__(self,
                 model,
                 value_col=&#39;value&#39;,
                 cache=memcache,
                 cache_key=&#39;__DatastoreDict:LastUpdated__&#39;):

        self.model = model
        self.value_col = value_col
        self.cache = cache
        self.cache_key = cache_key

        self.cache.add(self.cache_key, 1)

        super(DatastoreDict, self).__init__(encoding=NoOpEncoding)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;depersist-key:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;depersist(key)&lt;/h3&gt;

&lt;p&gt;Depersist implies deleting a key from the dictionary (and datastore). Here we
assume a helper method &lt;code&gt;delete&lt;/code&gt; that, given an ndb model and a string
representing it&amp;rsquo;s key deletes the model. Since the data has changed we also
update the last touched value to force a cache invalidation and data refresh.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def depersist(self, key):
    delete(self.model, key)
    self.touch_last_updated()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;durables:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;durables()&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;durables()&lt;/code&gt; returns the entire dictionary. Since we are all matching entities
from the datastore it is important to keep your dictionary relatively small &amp;ndash;
as the dictionary grows in size, resyncing it&amp;rsquo;s state with the datastore will
get more and more expensive. This function assumes a &lt;code&gt;get_all&lt;/code&gt; method that will
return all instances of a model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def durables(self):
    encoded_models = get_all(self.model)
    return dict((model.key.id(), getattr(model, self.value_col)) for model in encoded_models)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;setdefault-key-default-none:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;setdefault(key, default=None)&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;_setdefault()&lt;/code&gt; overrides the dictionary built-in &lt;code&gt;setdefault&lt;/code&gt; which allows you
to insert a key into the dictionary, creating the key with the default value if
it does not exist and returning the existing value if it does exist.&lt;/p&gt;

&lt;p&gt;For example, the following sequence of code creates a key for &lt;code&gt;y&lt;/code&gt;, which does not
exist, and returns the existing value for &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; d = {&#39;x&#39;: 1}
&amp;gt;&amp;gt;&amp;gt; d.setdefault(&#39;y&#39;, 2)
2
&amp;gt;&amp;gt;&amp;gt; d
{&#39;y&#39;: 2, &#39;x&#39;: 1}
&amp;gt;&amp;gt;&amp;gt; d.setdefault(&#39;x&#39;, 3)
1
&amp;gt;&amp;gt;&amp;gt; d
{&#39;y&#39;: 2, &#39;x&#39;: 1}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can implement &lt;code&gt;_setdefault&lt;/code&gt; using the &lt;code&gt;get_or_create&lt;/code&gt; helper method, updating
the cache if we have changed the dictionary.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _setdefault(self, key, default=None):
    instance, created = get_or_create(self.model, key, default)

    if created:
        self.touch_last_updated()

    return getattr(instance, self.value_col)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pop-key-default-none:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;pop(key, default=None)&lt;/h3&gt;

&lt;p&gt;pop returns the value for a key and deletes the key. This is fairly straight
forward given a &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; helper method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _pop(self, key, default=None):
    instance = get(self.model, key)
    if instance:
        value = getattr(instance, self.value_col)
        delete(self.model, key)
        self.touch_last_updated()
        return value
    else:
        if default is not None:
            return default
        else:
            raise KeyError
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-help:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;The Help&lt;/h3&gt;

&lt;p&gt;The previous discussion uses a few helper methods that we haven&amp;rsquo;t defined yet.
Each of these methods takes an arbitrary ndb model and performs an operation on
it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_key(cls, key):
    return ndb.Key(DatastoreDictAncestorModel,
                   DatastoreDictAncestorModel.generate_key(cls).string_id(),
                   cls, key.lower(),
                   namespace=&#39;&#39;)


@ndb.transactional
def get_all(cls):
    return cls.query(
        ancestor=DatastoreDictAncestorModel.generate_key(cls)).fetch()


@ndb.transactional
def get(cls, key):
    return build_key(cls, key).get()


@ndb.transactional
def get_or_create(cls, key, value=None):
    key = build_key(cls, key)

    instance = key.get()
    if instance:
        return instance, False

    instance = cls(key=key, value=value)
    instance.put()

    return instance, True


@ndb.transactional
def delete(cls, key):
    key = build_key(cls, key)
    return key.delete()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last item of note is the use of a parent for each DatastoreDict. This common
ancestor forces strong read consistency for the &lt;code&gt;get_all&lt;/code&gt; method, allowing us to
update a dictionary and have a consistent view of the data on subsequent reads.
We use an additional model to provide the strong read consistency.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DatastoreDictAncestorModel(ndb.Model):

    @classmethod
    def generate_key(cls, child_cls):
        key_name = &#39;__%s-%s__&#39; % (&#39;ancestor&#39;, child_cls.__name__)
        return ndb.Key(cls, key_name, namespace=&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Continuous Delivery Distilled</title>
      <link>http://sookocheff.com/posts/2015-04-23-continuous-delivery-distilled/</link>
      <pubDate>Thu, 23 Apr 2015 08:32:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-04-23-continuous-delivery-distilled/</guid>
      <description>

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/-B0ZEHmBCH8&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;What if you could deliver more value, with more speed and with more stability?&lt;/p&gt;

&lt;p&gt;What if you could triage bugs faster?&lt;/p&gt;

&lt;p&gt;What if you could fix bugs easier and with less user facing impact?&lt;/p&gt;

&lt;p&gt;You can, with continuous delivery.&lt;/p&gt;

&lt;h2 id=&#34;terminology:682641b8f6aff1fdce173832bfccddae&#34;&gt;Terminology&lt;/h2&gt;

&lt;p&gt;First, some terminology. What distinguishes continuous integration, continuous
deployment and continuous delivery? Continuous integration revolves around the
continuous automated testing of software whenever change to the software is
made. Continuous deployment is the practice of automatically deploying any
change to the code. Continuous delivery implies that you can deploy any change
to production but for any number of reasons you may choose not to. The focus of
this article is on continuous delivery.&lt;/p&gt;

&lt;h2 id=&#34;what-is-continuous-delivery:682641b8f6aff1fdce173832bfccddae&#34;&gt;What is continuous delivery?&lt;/h2&gt;

&lt;p&gt;Continuous delivery is a set of development practices that allow you to release
software to production at any time (&lt;a href=&#34;http://martinfowler.com/bliki/ContinuousDelivery.html&#34; title=&#34;Continuous Delivery&#34;&gt;Fowler, 2014&lt;/a&gt;). By following
these practices you can reduce the cost, development time and risk  of
delivering features to users (&lt;a href=&#34;http://www.thoughtworks.com/insights/blog/case-continuous-delivery&#34; title=&#34;The Case for Continuous Delivery&#34;&gt;Humble, 2014&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s expand upon these definitions by talking about what differentiates
continuous delivery from traditional software development. With continuous
delivery, at any point in time any stakeholder in the business can ask for the
current version of the software to be deployed to production. This implies that
your software is deployable throughout the development lifecycle and that the
development team prioritizes keeping the software stable and deployable over
working on new features (&lt;a href=&#34;http://martinfowler.com/bliki/ContinuousDelivery.html&#34; title=&#34;Continuous Delivery&#34;&gt;Fowler, 2014&lt;/a&gt;). It also implies some level
of testing and deployment automation.&lt;/p&gt;

&lt;p&gt;The following diagram of the continuous delivery process helps to visualize the
automation steps that typically accompany a software change and the subsequent
release of the software to production. The red bars in the diagram signal that
failures at this stage of the process halt the entire process.&lt;/p&gt;

&lt;p&gt;First, the delivery team or development team makes a change and commits that
change to a version control system. This check in triggers automated unit tests
that verify the commit. If those unit tests pass, automated acceptance tests run
and if those pass we move on to manual user testing. Once the user has tested
and approved the change a release can go out. From a development standpoint it
is important to understand that any code commit that passes testing may be
released to customers at &lt;em&gt;any&lt;/em&gt; point in time.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://commons.wikimedia.org/wiki/File:Continuous_Delivery_process_diagram.png#/media/File:Continuous_Delivery_process_diagram.png&#34;&gt;
  &lt;img src=&#34;http://upload.wikimedia.org/wikipedia/commons/7/74/Continuous_Delivery_process_diagram.png&#34; alt=&#34;Continuous Delivery process diagram - Jez Humble&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;why-continuous-delivery:682641b8f6aff1fdce173832bfccddae&#34;&gt;Why Continuous Delivery?&lt;/h2&gt;

&lt;p&gt;Google, Facebook, LinkedIn, Netflix, Etsy, Ebay, Github and Hewlett-Packard,
among many others, have adopted continuous delivery in their products. On
average, Amazon makes changes to production code every 11.6 seconds
(&lt;a href=&#34;https://www.youtube.com/watch?v=dxk8b9rSKOo&#34; title=&#34;Velocity Culture&#34;&gt;Jenkins, 2011&lt;/a&gt;) &amp;ndash; that&amp;rsquo;s 3000 production deployments every
day. Facebook commits to master 5000 times a day deploys to production twice
a day (&lt;a href=&#34;http://www.infoq.com/presentations/Facebook-Release-Process&#34; title=&#34;The Facebook Release Process&#34;&gt;Rossi, 2011&lt;/a&gt;). Etsy deploys more than 50 times a day
(&lt;a href=&#34;http://www.infoq.com/news/2014/03/etsy-deploy-50-times-a-day&#34; title=&#34;How Etsy Deploys More Than 50 Times a Day&#34;&gt;Miranda, 2014&lt;/a&gt;). Why would companies do this? What is the
benefit?&lt;/p&gt;

&lt;h3 id=&#34;reduced-risk:682641b8f6aff1fdce173832bfccddae&#34;&gt;Reduced Risk&lt;/h3&gt;

&lt;p&gt;The first benefit is reduced risk. Each deployment is a smaller change that can
easily be understood in isolation. If an error occurs it is trivially easy to
roll-back a single change or push a new release on top of the change.&lt;/p&gt;

&lt;h3 id=&#34;believable-progress:682641b8f6aff1fdce173832bfccddae&#34;&gt;Believable Progress&lt;/h3&gt;

&lt;p&gt;Developers are generally quite bad at estimating software delivery projects
([Milstein, 2013][Milstein2103]). If the definition of &amp;ldquo;done&amp;rdquo; means &amp;ldquo;developers
declare it to be done&amp;rdquo; that is much less believable than if it&amp;rsquo;s safely deployed
into a production environment.&lt;/p&gt;

&lt;h3 id=&#34;faster-iteration-towards-product-fit:682641b8f6aff1fdce173832bfccddae&#34;&gt;Faster Iteration Towards Product Fit&lt;/h3&gt;

&lt;p&gt;Generally speaking, the biggest risk in software development is building
something that the user doesn&amp;rsquo;t want. Continuous delivery is a great enabler of
A/B testing and allows you to frequently get working software in front of real
users to assess user behaviour and performance impact of software changes.&lt;/p&gt;

&lt;h3 id=&#34;expose-inefficiencies:682641b8f6aff1fdce173832bfccddae&#34;&gt;Expose Inefficiencies&lt;/h3&gt;

&lt;p&gt;Continuous delivery enforces discipline on the software development team to
always keep the product in deployable condition. This discipline naturally
exposes inefficiencies in the development process &amp;ndash; anything that gets in the
way of the goal of releasing working software quickly is an impediment to
development that will quickly be brought to light with continuous delivery.&lt;/p&gt;

&lt;h3 id=&#34;encourage-responsibility:682641b8f6aff1fdce173832bfccddae&#34;&gt;Encourage Responsibility&lt;/h3&gt;

&lt;p&gt;With continuous delivery, the developer making a change and the developer
deploying the code is the same person. This avoids any problems with handing
your deployment &amp;lsquo;over the wall&amp;rsquo; and allowing another person or team to test,
deploy and verify the code. It keeps the onus on working software with the
people most knowledge about how the software works.&lt;/p&gt;

&lt;h2 id=&#34;does-continuous-delivery-actually-work:682641b8f6aff1fdce173832bfccddae&#34;&gt;Does Continuous Delivery Actually Work?&lt;/h2&gt;

&lt;p&gt;Rather than reflect on a few abstract benefits, let&amp;rsquo;s look at some of the
available data on continuous delivery.&lt;/p&gt;

&lt;p&gt;ThoughtWorks (&lt;a href=&#34;http://www.thoughtworks.com/insights/blog/case-continuous-delivery&#34; title=&#34;The Case for Continuous Delivery&#34;&gt;Humble, 2014&lt;/a&gt;) analyzed their data on high performing
companies and found that those practicing continuous delivery ship code 30 times
faster, have 50% fewer failed deployments, and restore service 12 times faster
than their peers.&lt;/p&gt;

&lt;p&gt;In A Practical Approach to Large-Scale Agile Development (&lt;a href=&#34;http://www.amazon.ca/Practical-Approach-Large-Scale-Agile-Development/dp/0321821726&#34; title=&#34;A Practical Approach to Large-Scale Agile Development: How HP Transformed LaserJet FutureSmart Firmware&#34;&gt;Gruver,
2012&lt;/a&gt;). Hewlett-Packard, who had been practicing more traditional
software delivery process, experimented with continuous deployment in an
organization having roughly 400 developers working over 3 continents. After
switching to continuous delivery, they integrated small changesets over 100
times a day and deployed at least ten times a day. What happened? The number
of features under active development increased by 140% and development costs
per feature reduced by 78%. This amounted to a total development cost
reduction of 40%.&lt;/p&gt;

&lt;h2 id=&#34;how-to-do-continuous-delivery:682641b8f6aff1fdce173832bfccddae&#34;&gt;How to do Continuous Delivery?&lt;/h2&gt;

&lt;p&gt;At this point, you may be sold on the benefits of continuous delivery and are
asking how to get started. Continuous delivery requires a few components to be
effective.&lt;/p&gt;

&lt;h4 id=&#34;1-continuous-integration:682641b8f6aff1fdce173832bfccddae&#34;&gt;1. Continuous Integration&lt;/h4&gt;

&lt;p&gt;A build server performing continuous intergration of every commit is a
necessity. Once a code change is committed, the build server triggers the
testing and deployment pipeline ultimately leading to successfully deploying a
production release.&lt;/p&gt;

&lt;h4 id=&#34;2-automated-testing:682641b8f6aff1fdce173832bfccddae&#34;&gt;2. Automated Testing&lt;/h4&gt;

&lt;p&gt;Automated unit testing, and, where applicable, automated performance testing,
makes it easy to spot issues in code about to be deployed. If any of
these tests fail the current release is rejected. This is not a silver
bullet. Manual QA is still needed to verify a build and test before
releasing.&lt;/p&gt;

&lt;h4 id=&#34;3-feature-flags:682641b8f6aff1fdce173832bfccddae&#34;&gt;3. Feature Flags&lt;/h4&gt;

&lt;p&gt;Some features are too big to commit as one chunk. In these cases a &lt;a href=&#34;http://martinfowler.com/bliki/FeatureToggle.html&#34;&gt;feature
flag&lt;/a&gt; is used to hide
functionality that is not ready for general release, while still allowing code
to be released to production.&lt;/p&gt;

&lt;h4 id=&#34;4-monitoring:682641b8f6aff1fdce173832bfccddae&#34;&gt;4. Monitoring&lt;/h4&gt;

&lt;p&gt;Monitoring systems allow the development and test teams to easily see the effect
a given change has on user behaviour, system performance or system stability.&lt;/p&gt;

&lt;h4 id=&#34;5-one-click-deployment-and-roll-back:682641b8f6aff1fdce173832bfccddae&#34;&gt;5. &amp;ldquo;One-Click&amp;rdquo; Deployment and Roll-Back.&lt;/h4&gt;

&lt;p&gt;Deployments and roll-backs must be easy enough for anyone to do at a moments
notice.&lt;/p&gt;

&lt;h2 id=&#34;continuous-delivery-in-practice:682641b8f6aff1fdce173832bfccddae&#34;&gt;Continuous Delivery in Practice&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s run through three examples of how continuous delivery would look like in
practice, contrasting continuous delivery with a more traditional release
process. In these examples, the traditional release process assumes that any
changes scheduled to be released are held in a development environment for one
week, a staging environment for one week, and finally deployed to a production
environment after one week on the staging environment. Each envrionment
corresponds to a unique code branch (develop, test, master) and weekly merges
take place to push the development code up to the test branch (and staging
environment) and the test code up to the master branch (and production
environment).&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/continuous-delivery-in-practice.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/continuous-delivery-in-practice.png&#34; alt=&#34;Traditional Release Structure&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h3 id=&#34;scenario-1-bug-fix:682641b8f6aff1fdce173832bfccddae&#34;&gt;Scenario 1: Bug Fix&lt;/h3&gt;

&lt;p&gt;Imagine a scenario where a customer reports a bug in the system. The bug is
simple enough for a single developer to work on and the fix is small enough to
understand within a single code commit. Let&amp;rsquo;s begin by examining the traditional
release process to see how this bug fix reaches the customer.&lt;/p&gt;

&lt;p&gt;The developer, Brad, begins by checking out the latest copy of the development
branch, and begins work on the bug. Once he is confident that the bug has been
fixed he goes over the changes with QA and merges the bug into the develop
branch where it waits for the weekly deployment to test.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-1.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-1.png&#34; alt=&#34;Merging to Develop&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Brad is now free to pick up another issue and commit the code for that issue to
develop, where it waits once again for the weekly deployment to test. Test now
has two issues that have been committed to development that will be released to
the test environment in one week.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-2.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-2.png&#34; alt=&#34;Second Merge to Develop&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Meanwhile, other developers are working on issues and committing the code to
develop. By the time the weekly deployment to the testing environment comes
around we end up with 13 disparate issues being pushed to the test environment.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-3.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-3.png&#34; alt=&#34;Group of Merges to Develop&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Now, the QA team can perform regression testing of all of these 13 issues for
the week that this release is held in the test environment for staging. After
one week has passed, the test branch is merged with the master branch and a
deployment to production is done.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-4.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-4.png&#34; alt=&#34;Group of Merges to Develop&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also important to note that &lt;em&gt;we still do not know that the bug fix will
address the customer&amp;rsquo;s issues on production&lt;/em&gt;. We can&amp;rsquo;t know for sure that the
bug fix not complete until it fixes the issue on the production environment. So,
after the release a prudent developer will check back to make sure the bug
is no longer an issue and it can be marked as resolved.&lt;/p&gt;

&lt;p&gt;At this point we delivered the bug fix to the customer after a two week waiting
period. We also dedicated testing time to this bug fix before merging it to the
development branch, to regression test the release in the staging environment,
and to test this bug fix on the production environment. For
arguments sake, let&amp;rsquo;s say this testing time took 1 hour on each
environment.&lt;/p&gt;

&lt;h4 id=&#34;total-customer-time-waiting-for-this-bug-fix:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Customer Time Waiting For This Bug Fix:&lt;/h4&gt;

&lt;p&gt;2 weeks&lt;/p&gt;

&lt;h4 id=&#34;total-testing-time:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Testing Time:&lt;/h4&gt;

&lt;p&gt;3 hours&lt;/p&gt;

&lt;p&gt;Now imagine we have 13 issues that have been delivered with this release, we can
compound the total waiting time and total testing time.&lt;/p&gt;

&lt;h4 id=&#34;total-customer-time-waiting-for-this-release:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Customer Time Waiting For This Release:&lt;/h4&gt;

&lt;p&gt;26 weeks&lt;/p&gt;

&lt;h4 id=&#34;total-testing-time-for-this-release:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Testing Time For This Release:&lt;/h4&gt;

&lt;p&gt;39 hours&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s contrast this with a continuous delivery approach. In this scenario, Alice
works on an issue by first checking out the latest production code from the
master branch. Once she is confident she has fixed the bug and it has passed QA,
she merges the bug in to the master branch and deploys the fix to
production. Let&amp;rsquo;s assume that she took two hours to fix the bug and that
the bug required one hour of testing.&lt;/p&gt;

&lt;h4 id=&#34;total-customer-time-waiting-for-this-release-1:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Customer Time Waiting For This Release:&lt;/h4&gt;

&lt;p&gt;2 hours&lt;/p&gt;

&lt;h4 id=&#34;total-testing-time-for-this-release-1:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Testing Time For This Release:&lt;/h4&gt;

&lt;p&gt;1 hour&lt;/p&gt;

&lt;p&gt;We can compound this by assuming we have 13 issues that are being worked on for the week.&lt;/p&gt;

&lt;h4 id=&#34;total-customer-time-waiting-for-this-week:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Customer Time Waiting For This Week:&lt;/h4&gt;

&lt;p&gt;26 hours&lt;/p&gt;

&lt;h4 id=&#34;total-testing-time-for-this-week:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Testing Time For This Week:&lt;/h4&gt;

&lt;p&gt;13 hours&lt;/p&gt;

&lt;h3 id=&#34;scenario-2-regressions-and-rollback:682641b8f6aff1fdce173832bfccddae&#34;&gt;Scenario 2: Regressions and Rollback&lt;/h3&gt;

&lt;p&gt;Now imagine that the bug fix in the scenario above actually causes a regression
on production that needs to be fixed immediately or rolled back.&lt;/p&gt;

&lt;p&gt;In Brad&amp;rsquo;s case (the weekly release process), someone on the devops or change
management team packages the production release and pushes it to the production
environment. And something goes wrong. Devops knows that one of 13 different
change sets have been released but have no way of knowing which of those change
sets is causing the regression. A critical issue is created identifying the
problem and this issue is handed off to the development team. The team works to
triage the issue, notices that Brad&amp;rsquo;s change caused the problem and Brad is now
in charge of fixing it. But the last time Brad worked on this piece of code was
two weeks ago and his memory is a bit fuzzy about why the change was made. Or
maybe Brad is on holiday and someone else needs to pick up his work without
fully understanding the intricacies and risks involved with the chage.
Ultimately, the team decides they can&amp;rsquo;t go forward and all 13 change sets are
rolled back until they can properly fix the problem.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/weekly-regression.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/weekly-regression.png&#34; alt=&#34;Regression in Weekly Release&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Contrast this with a continuous delivery approach. Alice works with QA to verify
her change. Alice and QA deploy the change to production and immediately verify
that the integrity of the fix. And something goes wrong. In this case, there is
only one change set that could have cause the problem &amp;ndash; Alice&amp;rsquo;s. Alice has
immediate knowledge of the changes she just committed and possible reasons for a
failure. She can choose at this point to fix the issue and release her fix or to
roll-back her single change. In this scenario, Alice is responsible for the
integrity of her changes and for verifying that her work was done correctly. She
is able to work in concert with QA to test the issue and does not simply push
her issue &amp;lsquo;over the wall&amp;rsquo; for someone else to test and deploy.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/continuous-regression.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/continuous-regression.png&#34; alt=&#34;Regression With Continuous Delivery&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;With continuous delivery, each deployment is a smaller change that can be easily
understood, fixed or, when necessary, rolled-back.&lt;/p&gt;

&lt;h3 id=&#34;scenario-3-new-features:682641b8f6aff1fdce173832bfccddae&#34;&gt;Scenario 3: New Features&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve seen how continuous delivery can aid in deploying bug fixes, but what
about delivery new features? Remember that with continuous delivery any commit
at any time can be deployed directly to the production environment. So how can
you deploy partially complete features? The answer is &lt;a href=&#34;http://martinfowler.com/bliki/FeatureToggle.html&#34;&gt;feature
flags&lt;/a&gt;. Feature flags allow
the developer to write a new feature or edit an existing feature without
exposing those changes to the end user.&lt;/p&gt;

&lt;p&gt;For a brand new feature, it&amp;rsquo;s relatively easy to develop the entire feature
behind a feature flag that is inaccessible to the user by not exposing the new
page, button or widget at all. Once the feature matures it can be opened up to
QA or product managers for testing and eventually rolled out to a small
percentage of users. These users are able to test the feature with real
production data and real production load &amp;ndash; making sure everything works as
expected.&lt;/p&gt;

&lt;p&gt;Gradually rolling out the feature also gives you the ability to &lt;em&gt;measure user
behaviour&lt;/em&gt; and &lt;em&gt;gather feedback&lt;/em&gt; before committing to a certain path of action.&lt;/p&gt;

&lt;p&gt;When enhancing existing features or doing refactoring, feature flags work best
with continuous delivery when following a &lt;a href=&#34;http://martinfowler.com/bliki/ParallelChange.html&#34;&gt;parallel
change&lt;/a&gt; design pattern, where
both the old and the new code is run during a request, but only one version of
the result is returned to the user. As a concrete example, imagine we are trying
to improve the performance of a page through a refactoring. When a request comes
in, we route the request to both the old and new code and can measure &amp;ndash; on
production &amp;ndash; the performance of the new code. We can easily see if our proposed
refactoring has measurable performance improvements in a real-world setting.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/parallel-code.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/parallel-code.png&#34; alt=&#34;Parallel Code&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;For example, by extracting performance measurements over each iteration of the
code we visually compare the effect of a code change.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/performance-comparison.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/performance-comparison.png&#34; alt=&#34;Performance Comparison&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We can also use the same pattern to ensure we have confidence in the results of
our new code. For example, on each request, we can run both the old and the new
code, and compare the results on real-world production data. When we are
confident that the differences between the new and old code are within an
acceptable error range the new code is ready to go live. We also have the
ability to use production data to inform our unit tests and guard against future
regressions.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/accuracy-comparison.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/accuracy-comparison.png&#34; alt=&#34;Accuracy Comparison&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Caution must be exercised whenever using feature flags. Every feature flag that
is in use within the product is technical debt that should be short lived.&lt;/p&gt;

&lt;h2 id=&#34;towards-continuous-delivery:682641b8f6aff1fdce173832bfccddae&#34;&gt;Towards Continuous Delivery&lt;/h2&gt;

&lt;p&gt;Continuous delivery is not a panacea &amp;ndash; it requires diligence and responsibility
on behalf of the development team. However, if the team is able to cross these
hurdles continuous delivery can be used to deliver stable software to customers
faster than ever before.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a BigQuery Table using the Java Client Library</title>
      <link>http://sookocheff.com/posts/2015-03-23-creating-a-big-query-table-java-api/</link>
      <pubDate>Mon, 23 Mar 2015 15:32:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-03-23-creating-a-big-query-table-java-api/</guid>
      <description>&lt;p&gt;I haven&amp;rsquo;t been able to find great documentation on creating a BigQuery
TableSchema using the Java Client Library. This blog post hopes to rectify that
:).&lt;/p&gt;

&lt;p&gt;You can use the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/bigquery-samples-java&#34;&gt;BigQuery sample
code&lt;/a&gt; for an idea
of how to create a client connection to BigQuery. Assuming you have the
connection set up you can start by creating a new &lt;code&gt;TableSchema&lt;/code&gt;. The
&lt;code&gt;TableSchema&lt;/code&gt; provides a method for setting the list of fields that make up the
columns of your BigQuery Table. Those columns are defined as an Array of
&lt;code&gt;TableFieldSchema&lt;/code&gt; objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;ArrayList&amp;lt;TableFieldSchema&amp;gt; fieldSchema = new ArrayList&amp;lt;TableFieldSchema&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For simple types you can populate your columns with the correct type and mode
according to the &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/v2/tables#resource&#34;&gt;BigQuery API
documentation&lt;/a&gt;.
For example, to create a STRING field that is NULLABLE you can use the
following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(new TableFieldSchema().setName(&amp;quot;username&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;NULLABLE&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And for repeated fields you can use the REPEATED mode.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(new TableFieldSchema().setName(&amp;quot;email&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;REPEATED&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create nested records you specify the parent as a RECORD mode and then call
&lt;code&gt;setFields&lt;/code&gt; for each column of nested data you want to insert. The columns of a
nested type are the same format as for the parent &amp;ndash; a list of TableFieldSchema
objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(
  new TableFieldSchema().setName(&amp;quot;location&amp;quot;).setType(&amp;quot;RECORD&amp;quot;).setFields(
    new ArrayList&amp;lt;TableFieldSchema&amp;gt;() {
      {
        add(new TableFieldSchema().setName(&amp;quot;city&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        add(new TableFieldSchema().setName(&amp;quot;address&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        add(new TableFieldSchema().setName(&amp;quot;zipcode&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
      }
    }
  )
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last step is to set the entire schema as the fields of our table schema.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;TableSchema schema = new TableSchema();
schema.setFields(fieldSchema);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we set a &lt;code&gt;TableReference&lt;/code&gt; that holds the current project id, dataset id and
table id. We use this &lt;code&gt;TableReference&lt;/code&gt; to create our &lt;code&gt;Table&lt;/code&gt; using the &lt;code&gt;TableSchema&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;TableReference ref = new TableReference();
ref.setProjectId(PROJECT_ID);
ref.setDatasetId(&amp;quot;pubsub&amp;quot;);
ref.setTableId(&amp;quot;review_test&amp;quot;);

Table content = new Table();
content.setTableReference(ref);
content.setSchema(schema);

client.tables().insert(ref.getProjectId(), ref.getDatasetId(), content).execute();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together gives you a working sample of creating a BigQuery Table using the Java Client Library.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) throws IOException, InterruptedException {
  Bigquery client = createAuthorizedClient(); // As per the BQ sample code
  
  ArrayList&amp;lt;TableFieldSchema&amp;gt; fieldSchema = new ArrayList&amp;lt;TableFieldSchema&amp;gt;();
  
  fieldSchema.add(new TableFieldSchema().setName(&amp;quot;username&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;NULLABLE&amp;quot;));
  fieldSchema.add(new TableFieldSchema().setName(&amp;quot;email&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;REPEATED&amp;quot;));
  fieldSchema.add(
    new TableFieldSchema().setName(&amp;quot;location&amp;quot;).setType(&amp;quot;RECORD&amp;quot;).setFields(
      new ArrayList&amp;lt;TableFieldSchema&amp;gt;() {
        {
          add(new TableFieldSchema().setName(&amp;quot;city&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
          add(new TableFieldSchema().setName(&amp;quot;address&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
          add(new TableFieldSchema().setName(&amp;quot;zipcode&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        }
  }));
  
  TableSchema schema = new TableSchema();
  schema.setFields(fieldSchema);
  
  TableReference ref = new TableReference();
  ref.setProjectId(&amp;quot;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;quot;);
  ref.setDatasetId(&amp;quot;&amp;lt;YOUR_DATASET_ID&amp;gt;&amp;quot;);
  ref.setTableId(&amp;quot;&amp;lt;YOUR_TABLE_ID&amp;gt;&amp;quot;);
  
  Table content = new Table();
  content.setTableReference(ref);
  content.setSchema(schema);
  
  client.tables().insert(ref.getProjectId(), ref.getDatasetId(), content).execute();
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Deploying R Studio on Compute Engine</title>
      <link>http://sookocheff.com/posts/2015-03-30-deploying-r-studio-to-compute-engine/</link>
      <pubDate>Mon, 23 Mar 2015 15:32:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-03-30-deploying-r-studio-to-compute-engine/</guid>
      <description>

&lt;p&gt;Sometimes you have a data analysis problem that is just too big for your desktop
or laptop. The limiting factor here is generally RAM. Thankfully, services like
Google Compute Engine allow you to lease servers with up to 208GB of RAM, large
enough for a wide variety of intensive tasks. An ancillary benefit of using a
service like Compute Engine is that it allows you to easily load your data from
a Cloud Storage Bucket, meaning you don&amp;rsquo;t need to keep a copy of the large
dataset locally at all times.&lt;/p&gt;

&lt;p&gt;R Studio has a remote mode allowing you to install it on a server with access
through a remote interface. This tutorial details how to start a Compute Engine
instance, install R Studio on it and access R Studio from the remote interface.&lt;/p&gt;

&lt;p&gt;The rest of this tutorial assumes that you have a Google Cloud Platform account
with billing enabled and have installed the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud
SDK&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;deploying-a-compute-engine-instance:fe69c3491bab8cb03451503db88c7996&#34;&gt;Deploying a Compute Engine Instance&lt;/h2&gt;

&lt;p&gt;The first step is to deploy your Compute Engine instance. The &lt;code&gt;gcloud compute&lt;/code&gt;
command allows you to create instances. The only required parameter to create an
instance is the instance name. We will call our instance &lt;code&gt;r-studio&lt;/code&gt; but you can
choose any name you like. R Studio Server is typically built on Ubuntu so it is
safest to use the Ubuntu distribution for your server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute instances create r-studio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will be prompted to choose a
&lt;a href=&#34;https://cloud.google.com/compute/docs/zones&#34;&gt;Zone&lt;/a&gt;. Just choose a zone close to
you. You can also specify the zone when creating the instance using the &lt;code&gt;--zone&lt;/code&gt;
parameter. For example.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute instances create r-studio --zone us-central1-a
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;installing-r-studio:fe69c3491bab8cb03451503db88c7996&#34;&gt;Installing R Studio&lt;/h2&gt;

&lt;p&gt;Once we have our Compute Engine instance set up, we log in to the machine using ssh.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute ssh r-studio --zone us-central1-a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we are logged in to the Compute Engine instance, it&amp;rsquo;s time to install
R by first updating the Debian apt-get repository and then installing R.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update
sudo apt-get install r-base r-base-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R Studio currently requries OpenSSL version 0.9.8. We need to install this
separately and then install install R Studio&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget http://ftp.us.debian.org/debian/pool/main/o/openssl/libssl0.9.8_0.9.8o-4squeeze14_amd64.deb
sudo dpkg -i libssl0.9.8_0.9.8o-4squeeze14_amd64.deb
sudo apt-get install gdebi-core
wget http://download2.rstudio.org/rstudio-server-0.98.1103-amd64.deb
sudo gdebi rstudio-server-0.98.1103-amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should be up and running with R Studio on your compute engine instance. To
verify, navigate to the IP address of your Compute Engine instance on port 8787
(the default R Studio port).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;http://&amp;lt;ipaddress&amp;gt;:8787
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R Studio only permits access to users of the system, we can add a user with
standard Linux tools like adduser. For example, to create a new user named
rstudio and specify the password you could execute the following commands.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo adduser rstudio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will be prompted to enter a password for the user and confirm the users name
and phone number.&lt;/p&gt;

&lt;p&gt;Afterwards, logging in with the user you created will present a web UI of the
familiar R Studio. You can now perform analysis on those larger data sets using
the R Studio that just weren&amp;rsquo;t possible on a laptop.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Keeping App Engine Search Documents and Datastore Entities In Sync</title>
      <link>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</link>
      <pubDate>Mon, 23 Feb 2015 08:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</guid>
      <description>

&lt;p&gt;At Vendasta the App Engine Datastore serves as the single point of truth for
most operational data and the majority of interactions are against this single
point of truth. However, a piece of required functionality in many of our
products is to provide a searchable view of the data in the App Engine
Datastore. Search is difficult using the Datastore and so we have moved to using
the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/search/&#34;&gt;Search API&lt;/a&gt; as a
managed solution for searching datastore entities. In this use case, every edit
to an entity in the Datastore is reflected as a change to a Search Document.
This article details an architecture for keeping Datastore entities and Search
Documents in sync throughout failure and race conditions.&lt;/p&gt;

&lt;h2 id=&#34;updating-the-search-document-using-a-post-put-hook:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Updating the Search Document Using a _post_put_hook&lt;/h2&gt;

&lt;p&gt;To ensure that every put of an entity to the Datastore results in an update to
the associated search document, we update the search document in the
_post_put_hook of the entity. The _post_put_hook is executed every time
the entity is put so each time the entity has changed we will put a new and
updated search document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def _post_put_hook(self, future):
        document = search.Document(
            doc_id = self.username,
            fields=[
               search.TextField(name=&#39;username&#39;, value=self.username),
               search.TextField(name=&#39;email&#39;, value=self.email),
               ])
        try:
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)
        except search.Error:
            logging.exception(&#39;Put failed&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Updating the search document during every put as part of the post put hook is a
light weight way to keep the search document up-to-date with changes to the
entity. However, this design does not account for the potential error conditions
where putting the search document or the Datastore entity fails. We will need
some additional functionality to handle these cases.&lt;/p&gt;

&lt;h2 id=&#34;handling-search-document-put-failures:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Search Document Put Failures&lt;/h2&gt;

&lt;p&gt;The first obstacle to overcome is handling failures when putting the search
document. One method for handling failures is retrying. We can add retrying to
our workflow by separating updating the search document into its own task and
deferring that task using the deferred library. This accomplishes two things.
First, moving the search document functionality into its own function makes our
code more modular. Second, the App Engine task queue mechanism allows us to
specify our retry semantics, handling backoff and failure conditions gracefully.
In this example, we allow infinite retries of failed tasks, allowing DevOps to find
search documents that may have become out of sync with their Datastore entities
and correct any problems that may arise. We assume in the example below that the
username acts as the ndb Key.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document, self.username)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;handling-datastore-put-failures:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Datastore Put Failures&lt;/h2&gt;

&lt;p&gt;The second obstacle to overcome is safely handling Datastore put failures. In
this architecture, each change to a Datastore entity is required to run within a
transaction. We update the _post_put_hook to queue a transactional task &amp;ndash; which
forces the task to only be queued if the current transaction has successfully
completed. This guarantees that failed Datastore puts will not result in search
documents being updated and becoming out of sync with the Datastore.&lt;/p&gt;

&lt;p&gt;We specify that the task should be run as a transaction by passing the result of
the &lt;code&gt;in_transaction&lt;/code&gt; function to the &lt;code&gt;_transactional&lt;/code&gt; parameter of &lt;code&gt;defer&lt;/code&gt;.
&lt;code&gt;in_transaction&lt;/code&gt; returns &lt;code&gt;True&lt;/code&gt; if the currently executing code is running in a
transaction and &lt;code&gt;False&lt;/code&gt; otherwise.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have sastisfied the case where either the search document or the
Datastore put has failed. If the search document put has failed we retry, if the
Datastore put has failed we do not put the search document. We still have one
remaining problem: Dirty Reads.&lt;/p&gt;

&lt;h2 id=&#34;handling-dirty-reads:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Dirty Reads&lt;/h2&gt;

&lt;p&gt;The last obstacle to overcome is dealing with race conditions that could lead to
reading stale data and writing that data to the search document. Consider the
case where two subsequent puts to the Datastore occur back-to-back within a
short time frame. Each of these puts will write new data to the Datastore and
queue a task to put the updated search document to the Datastore. The dirty
read problem arises when the second task to update the search document reads old
data from the Datastore that may not have been fully replicated throughout the
Datastore.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34; alt=&#34;Syncing Search Documents&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We can overcome this problem by versioning our tasks to coincide with the
version of our Datastore entity. We add a version number to the entity and
update the version number during a _pre_put_hook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    def _pre_put_hook(self):
        self.version = self.version + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now during the _post_put_hook we queue a task corresponding to the version number
of the Datastore entity we are putting. This ties the task to the point in time
when the Datastore entity was put.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    @classmethod
    def put_search_document(cls, username, version):
        model = ndb.Key(cls, username).get()
        if model:
            if version &amp;lt; model.version:
                logging.warning(&#39;Attempting to write stale data. Ignore&#39;)
                return

            if version &amp;gt; model.version:
                msg = &#39;Attempting to write future data. Retry to await consistency.&#39;
                logging.warning(msg)
                raise Exception(msg)

            # Versions match. Update the search document
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=model.username),
                   search.TextField(name=&#39;email&#39;, value=model.email),
                   search.TextField(name=&#39;version&#39;, value=model.version),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _pre_put_hook(self):
        self.version = self.version + 1

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       self.version,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the version number of the task being executed is less than the version number
written to the Datastore, we are attempting to write stale data and do not need
to process this request. If the version number is greater than the task being
executed, we are attempting to write data to the search document that has not
been fully replicated throughout the Datastore. In this case, we raise an
exception to retry putting the search document. In subsequent retries the data
will have propagated and our put will succeed. Note that if another task is
executed while the current task is retrying, the version number of our retrying
task will become stale and when the task is next executed we do not write the
now stale data to the search document.&lt;/p&gt;

&lt;p&gt;This still handles the case when a search document put fails &amp;ndash; whenever our
version number becomes out of sync due to the failed put, we do not write the
data to the search document. Furthermore, if our Datastore put fails then our
task to put the search document will not be queued &lt;em&gt;as long as the Datastore put
is run within a transaction&lt;/em&gt;. The version number will not be incremented in this
case because the value set during the _pre_put_hook will not be persisted during
a failed transaction.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Putting this all together, we&amp;rsquo;ve developed a solution for keeping search
documents in sync with Datastore entities that is robust to failure and race
conditions. This same technique can be used for syncing the state of any number
of datasets that are dependent on the Datastore being the single point of truth
in your system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Halting Python unittest Execution on First Error</title>
      <link>http://sookocheff.com/posts/2015-02-12-halting-unittest-execution-at-first-error/</link>
      <pubDate>Thu, 12 Feb 2015 06:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-12-halting-unittest-execution-at-first-error/</guid>
      <description>&lt;p&gt;We all know the importance of unit tests. Especially in a dynamic language like
Python. Occasionally you have a set of unit tests that are failing in a
cascading fashion where the first error case causes subsequent tests to fail
(these tests are likely no longer unit tests, but that&amp;rsquo;s a different
 discussion). To help isolate the offending test case in a see of failures you
can set the &lt;code&gt;unittest.TestCase&lt;/code&gt; class to halt after the first error by
overriding the &lt;code&gt;run&lt;/code&gt; method as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class HaltingTestCase(unittest.TestCase):

    def run(self, result=None):
        &amp;quot;&amp;quot;&amp;quot; Stop after first error &amp;quot;&amp;quot;&amp;quot;
        if not result.errors:
            super(HaltingTestCase, self).run(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this block of code, if we do not have any errors we call the super class to
continue running tests. If we have an error execution stops after this method
call. This allows you to pinpoint the first error case, fix it, and continue on
fixing subsequent tests.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Create a Google Cloud Dataflow Project with Gradle</title>
      <link>http://sookocheff.com/posts/2015-02-11-cloud-dataflow-with-gradle/</link>
      <pubDate>Wed, 11 Feb 2015 06:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-11-cloud-dataflow-with-gradle/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been experimenting with the &lt;a href=&#34;https://cloud.google.com/dataflow/&#34;&gt;Google Cloud
Dataflow&lt;/a&gt; &lt;a href=&#34;https://github.com/GoogleCloudPlatform/DataflowJavaSDK&#34;&gt;Java
SDK&lt;/a&gt; for running managed
data processing pipelines. One of the first tasks is getting a build environment
up and running. For this I chose Gradle.&lt;/p&gt;

&lt;p&gt;We start by declaring this a java application and listing the configuration
variables that declare the source compatibility level (which for now must be
1.7) and the main class to be executed by the &lt;code&gt;run&lt;/code&gt; task to be defined later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apply plugin: &#39;java&#39;
apply plugin: &#39;application&#39;

sourceCompatibility = &#39;1.7&#39;

mainClassName = &#39;com.sookocheff.dataflow.Main&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then declare the mavenCentral repository where the dependencies are located
and the basic dependencies for a Cloud Dataflow application.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;repositories {
    mavenCentral()
}

dependencies {
    compile &#39;com.google.guava:guava:18.0&#39;
    compile &#39;com.google.cloud.dataflow:google-cloud-dataflow-java-sdk-all:0.3.150109&#39;
    
    testCompile &#39;junit:junit:4.11&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last, we create our run task that will launch the Cloud Dataflow application.
The Cloud Dataflow runtime expects the folder &lt;code&gt;resources/main&lt;/code&gt; to exist in your
build. If you are not actually shipping any resources with your application you
will need to tell Gradle to create the correct directory. We also pass any
parameters to our main class using the -P flag.  These two steps are
encapsulated below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;task resources {
    def resourcesDir = new File(&#39;build/resources/main&#39;)
    resourcesDir.mkdirs()
}

run {
    if (project.hasProperty(&#39;args&#39;)) {
        args project.args.split(&#39;\\s&#39;)
    }
}

run.mustRunAfter &#39;resources&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now be able to launch your Cloud Dataflow application using the
&lt;code&gt;gradle run&lt;/code&gt; task, passing your project identifiers as parameters. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gradle run -Pargs=&amp;quot;--project=&amp;lt;your-project&amp;gt; --runner=BlockingDataflowPipelineRunner --stagingLocation=gs://&amp;lt;staging-location&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A pypiserver Deployment Script</title>
      <link>http://sookocheff.com/posts/2015-02-01-a-pypiserver-deployment-script/</link>
      <pubDate>Sun, 01 Feb 2015 14:53:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-01-a-pypiserver-deployment-script/</guid>
      <description>&lt;p&gt;At Vendasta we&amp;rsquo;ve been slowly adopting pypi and pip for our internal code
libraries and the time has come to deploy our own private pypi server. After
evaluating a few options I settled on the simplistic
&lt;a href=&#34;https://pypi.python.org/pypi/pypiserver&#34;&gt;pypiserver&lt;/a&gt; &amp;ndash; a barebones
implementation of the &lt;a href=&#34;https://pypi.python.org/simple/&#34;&gt;simple HTTP API&lt;/a&gt; to
pypi.&lt;/p&gt;

&lt;p&gt;The deployment uses nginx as a front-end to pypiserver. pypiserver itself is ran
and monitored using supervisord. I created a bash script that creates a user and
group to run pypiserver and installs and runs nginx, supervisord and pypiserver.
I&amp;rsquo;ve been running this bash script through Vagrant to deploy a custom pypiserver
for private use. I wanted to save this code for posterity and hopefully help
someone else working on the same task.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

STARTUP_VERSION=1
STARTUP_MARK=/var/startup.script.$STARTUP_VERSION

# Exit if this script has already ran
if [[ -f $STARTUP_MARK ]]; then
  exit 0  
fi

set -o nounset
set -o pipefail
set -o errexit

# Install prerequesites
sudo apt-get update
sudo apt-get install -y vim
sudo apt-get install -y apache2-utils
sudo apt-get install -y nginx
sudo apt-get install -y supervisor

# Install pip
wget &amp;quot;https://bootstrap.pypa.io/get-pip.py&amp;quot;
sudo python get-pip.py

# Install pypiserver with passlib for upload support
sudo pip install passlib
sudo pip install pypiserver

# Set the port configuration
proxy_port=8080
pypi_port=7201

# Create a user and group to run pypiserver
user=pypiusername
password=pypipasswrd
group=$user

sudo groupadd &amp;quot;$group&amp;quot;
sudo useradd $user -m -g $group -G $group
sudo -u $user -H -s eval &#39;htpasswd -scb $HOME/.htaccess&#39; &amp;quot;$user $password&amp;quot;
sudo -u $user -H -s eval &#39;mkdir -p $HOME/packages&#39;

##############
# nginx config
##############
echo &amp;quot;$user:$(openssl passwd -crypt $password)&amp;quot; &amp;gt; /etc/nginx/user.pwd

# nginx can&#39;t run as a daemon to work with supervisord
echo &amp;quot;daemon off;&amp;quot; &amp;gt;&amp;gt; /etc/nginx/nginx.conf

cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/nginx/sites-enabled/pypi-server.conf
server {
  listen $proxy_port;
  location / {
    proxy_pass http://localhost:$pypi_port;
    auth_basic &amp;quot;PyPi Authentication&amp;quot;;
    auth_basic_user_file /etc/nginx/user.pwd;
  }
}
EOF

rm /etc/nginx/sites-enabled/default

###################
# supervisor config
###################
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/supervisor/conf.d/pypi-server.conf
[program:pypi-server]
command=pypi-server -p $pypi_port -P /home/$user/.htaccess /home/$user/packages
directory=/home/$user
user=$user
autostart=true
autorestart=true
stderr_logfile=/var/log/pypi-server.err.log
stdout_logfile=/var/log/pypi-server.out.log
EOF

cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/supervisor/conf.d/nginx.conf
[program:nginx]
command=/usr/sbin/nginx
autostart=true
autorestart=true
stdout_events_enabled=true
stderr_events_enabled=true
EOF

sudo supervisorctl reread
sudo supervisorctl update

touch $STARTUP_MARK
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Downloading files from Google Cloud Storage with webapp2</title>
      <link>http://sookocheff.com/posts/2015-01-27-webapp2-download-handler/</link>
      <pubDate>Tue, 27 Jan 2015 06:07:12 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-27-webapp2-download-handler/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been working on a simple App Engine application that offers upload and
download functionality to and from Google Cloud Storage. When it came time to
actually download the content I needed to write a webapp2 &lt;code&gt;RequestHandler&lt;/code&gt; that
will retrieve the file from Cloud Storage and return it to the client.&lt;/p&gt;

&lt;p&gt;The trick to this is to set the proper content type in your response header. In
the example below I used the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/googlecloudstorageclient/&#34;&gt;Cloud Storage Client
Library&lt;/a&gt;
to open and read the file, then set the response appropriately.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import webapp2
import cloudstorage

class FileDownloadHandler(webapp2.RequestHandler):

  def get(self, filename):
    self.response.headers[&#39;Content-Type&#39;] = &#39;application/x-gzip&#39;
    self.response.headers[&#39;Content-Disposition&#39;] = &#39;attachment; filename=%s&#39; % filename

    filename = &#39;/bucket/&#39; + filename
    gcs_file = cloudstorage.open(filename)
    data = gcs_file.read()
    gcs_file.close()

    self.response.write(data)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Querying App Engine Logs with Elasticsearch</title>
      <link>http://sookocheff.com/posts/2015-01-23-querying-app-engine-logs-with-elasticsearch/</link>
      <pubDate>Fri, 23 Jan 2015 06:15:07 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-23-querying-app-engine-logs-with-elasticsearch/</guid>
      <description>

&lt;p&gt;From a DevOps perspective having a historical record of application logs can aid
immensely in tracking down bugs, responding to customer questions, or finding
out when and why that critical piece of data was updated to the wrong value. One
of the biggest grievances with the built-in log handling of Google App Engine is
that historical logs are only available for the previous three days. We wanted
to do a little bit better and have logs available for a 30 day time period. This
article outlines a method we&amp;rsquo;ve developed for pushing App Engine logs to an
elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;A side benefit of this approach is that if you have multiple App Engine
projects, all of their logs can be searched at the same time. This provides an
immediate benefit when tracking down systems integration problems or parsing API
traffic between applications.&lt;/p&gt;

&lt;p&gt;The solution we chose for this problem revolves around the MapReduce API. If you
need a refresher on this API please check out my &lt;a href=&#34;http://sookocheff.com/series/mapreduce-api/&#34;&gt;MapReduce tutorial
series&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;overview:910e0d5fe0b44f1ddf1d486376d42c6d&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The gist of this solution is to run a MapReduce job that reads data from the
&lt;a href=&#34;https://cloud.google.com/appengine/docs/python/logs/&#34;&gt;App Engine Logs API&lt;/a&gt; using the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/master/python/src/mapreduce/input_readers.py#L1952&#34;&gt;LogInputReader&lt;/a&gt;,
converts the data to a JSON format for ingestion into elasticsearch, and finally
write the parsed data to the elasticsearch cluster using a &lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;custom MapReduce
OutputWriter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We execute this MapReduce job on a timer using cron to push logs to
elasticsearch on a specific schedule. In our case, we run this job every 15
minutes to provide a relatively recent view of current operational data.&lt;/p&gt;

&lt;p&gt;The following diagram presents the architecture of our solution.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-01-23-querying-app-engine-logs-with-elasticsearch/elasticsearch-sequence-diagram.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-01-23-querying-app-engine-logs-with-elasticsearch/elasticsearch-sequence-diagram.png&#34; alt=&#34;Architecture for Logging to elasticsearch&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;example:910e0d5fe0b44f1ddf1d486376d42c6d&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;The majority of the solution is contained in a MapperPipeline. The following
code illustrates how to setup the MapperPipeline. What&amp;rsquo;s remaining is to write a
&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;custom MapReduce OutputWriter&lt;/a&gt; that pushes data to
elasticsearch and a function that converts a RequestLog object to JSON suitable
for elasticsearch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class CronHandler(webapp2.RequestHandler):

    def get(self):
        run()


def run():
    start_time, end_time = get_time_range()
    logging.debug(&#39;Dumping logs for date range (%s, %s).&#39;, start_time, end_time)

    start_time = float(start_time.strftime(&#39;%s.%f&#39;))
    end_time = float(end_time.strftime(&#39;%s.%f&#39;))

    p = Log2ElasticSearch(start_time, end_time)
    p.start()


class Log2Elasticsearch(pipeline.Pipeline):

    def run(self, start_time, end_time, module_name, module_versions):
        &amp;quot;&amp;quot;&amp;quot;
        Args:
            module_versions: A list of tuples of the form (module, version), that
                indicate that the logs for the given module/version combination should be
                fetched.  Duplicate tuples will be ignored.
        &amp;quot;&amp;quot;&amp;quot;
        yield mapreduce_pipeline.MapperPipeline(
            &amp;quot;vlogs-elasticsearch-injestion&amp;quot;,
            handler_spec=&amp;quot;log2json&amp;quot;,
            input_reader_spec=&amp;quot;mapreduce.input_readers.LogInputReader&amp;quot;,
            output_writer_spec=&amp;quot;mapreduce.output_writers.ElasticSearchOutputWriter&amp;quot;,
            params={
                &amp;quot;input_reader&amp;quot;: {
                    &amp;quot;start_time&amp;quot;: start_time,
                    &amp;quot;end_time&amp;quot;: end_time,
                    &amp;quot;include_app_logs&amp;quot;: True,
                },
            },
            shards=16
        )
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Parsing bash script options with getopts</title>
      <link>http://sookocheff.com/posts/2015-01-04-parsing-bash-script-arguments-with-shopts/</link>
      <pubDate>Sun, 04 Jan 2015 12:31:51 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-04-parsing-bash-script-arguments-with-shopts/</guid>
      <description>

&lt;p&gt;A common task in shell scripting is to parse command line arguments to your
script. Bash provides the &lt;code&gt;getopts&lt;/code&gt; built-in function to do just that. This
tutorial explains how to use the &lt;code&gt;getopts&lt;/code&gt; built-in function to parse arguments and options to a bash script.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;getopts&lt;/code&gt; function takes three parameters. The first is a specification of
which options are valid, listed as a sequence of letters. For example, the
string &lt;code&gt;&#39;ht&#39;&lt;/code&gt; signifies that the options &lt;code&gt;-h&lt;/code&gt; and &lt;code&gt;-t&lt;/code&gt; are valid.&lt;/p&gt;

&lt;p&gt;The second argument to &lt;code&gt;getopts&lt;/code&gt; is a variable that will be populated with the
option or argument to be processed next. In the following loop, &lt;code&gt;opt&lt;/code&gt; will hold
the value of the current option that has been parsed by &lt;code&gt;getopts&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:ht&amp;quot; opt; do
  case ${opt} in
    h ) # process option a
      ;;
    t ) # process option l
      ;;
    \? ) echo &amp;quot;Usage: cmd [-h] [-t]
      ;;
  esac
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example shows a few additional features of &lt;code&gt;getopts&lt;/code&gt;. First, if an invalid
option is provided, the option variable is assigned the value &lt;code&gt;?&lt;/code&gt;. You can catch
this case and provide an appropriate usage message to the user. Second, this
behaviour is only true when you prepend the list of valid options with &lt;code&gt;:&lt;/code&gt; to
disable the default error handling of invalid options. It is recommended to
always disable the default error handling in your scripts.&lt;/p&gt;

&lt;p&gt;The third argument to &lt;code&gt;getopts&lt;/code&gt; is the list of arguments and options to be
processed. When not provided, this defaults to the arguments and options
provided to the application (&lt;code&gt;$@&lt;/code&gt;). You can provide this third argument to use
&lt;code&gt;getopts&lt;/code&gt; to parse any list of arguments and options you provide.&lt;/p&gt;

&lt;h2 id=&#34;shifting-processed-options:12703913772a307ca27d6fbba691325e&#34;&gt;Shifting processed options&lt;/h2&gt;

&lt;p&gt;The variable &lt;code&gt;OPTIND&lt;/code&gt; holds the number of options parsed by the last call to
&lt;code&gt;getopts&lt;/code&gt;. It is common practice to call the &lt;code&gt;shift&lt;/code&gt; command at the end of your
processing loop to remove options that have already been handled from &lt;code&gt;$@&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;parsing-options-with-arguments:12703913772a307ca27d6fbba691325e&#34;&gt;Parsing options with arguments&lt;/h2&gt;

&lt;p&gt;Options that themselves have arguments are signified with a &lt;code&gt;:&lt;/code&gt;. The argument to
an option is placed in the variable &lt;code&gt;OPTARG&lt;/code&gt;. In the following example, the
option &lt;code&gt;t&lt;/code&gt; takes an argument. When the argument is provided, we copy its value
to the variable &lt;code&gt;target&lt;/code&gt;. If no argument is provided &lt;code&gt;getopts&lt;/code&gt; will set &lt;code&gt;opt&lt;/code&gt; to
&lt;code&gt;:&lt;/code&gt;. We can recognize this error condition by catching the &lt;code&gt;:&lt;/code&gt; case and printing
an appropriate error message.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:t:&amp;quot; opt; do
  case ${opt} in 
    t )
      target=$OPTARG
      ;;
    \? )
      echo &amp;quot;Invalid option: $OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
      ;;
    : )
      echo &amp;quot;Invalid option: $OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
      ;;
  esac
done
shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;an-extended-example-parsing-nested-arguments-and-options:12703913772a307ca27d6fbba691325e&#34;&gt;An extended example &amp;ndash; parsing nested arguments and options&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s walk through an extended example of processing a command that takes
options, has a sub-command, and whose sub-command takes an additional option
that has an argument. This is a mouthful so let&amp;rsquo;s break it down using an
example. Let&amp;rsquo;s say we are writing our own version of the &lt;a href=&#34;https://pip.pypa.io/en/latest/&#34;&gt;&lt;code&gt;pip&lt;/code&gt;
command&lt;/a&gt;. In this version you can call &lt;code&gt;pip&lt;/code&gt;
with the &lt;code&gt;-h&lt;/code&gt; option to display a help message.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip -h
Usage: 
    pip -h                      Display this help message.
    pip install                 Install a Python package.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use &lt;code&gt;getopts&lt;/code&gt; to parse the &lt;code&gt;-h&lt;/code&gt; option with the following &lt;code&gt;while&lt;/code&gt; loop.
In it we catch invalid options with &lt;code&gt;\?&lt;/code&gt; and &lt;code&gt;shift&lt;/code&gt; all arguments that have
been processed with &lt;code&gt;shift $((OPTIND -1))&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:h&amp;quot; opt; do
  case ${opt} in
    h )
      echo &amp;quot;Usage:&amp;quot;
      echo &amp;quot;    pip -h                      Display this help message.&amp;quot;
      echo &amp;quot;    pip install                 Install a Python package.&amp;quot;
      exit 0
      ;;
    \? )
      echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
      exit 1
      ;;
  esac
done
shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s add the sub-command &lt;code&gt;install&lt;/code&gt; to our script.  &lt;code&gt;install&lt;/code&gt; takes as an
argument the Python package to install.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip install urllib3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;install&lt;/code&gt; also takes an option, &lt;code&gt;-t&lt;/code&gt;. &lt;code&gt;-t&lt;/code&gt; takes as an argument the location to
install the package to relative to the current directory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip install urllib3 -t ./src/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To process this line we must find the sub-command to execute. This value is the
first argument to our script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;subcommand=$1
shift # Remove `pip` from the argument list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can process the sub-command &lt;code&gt;install&lt;/code&gt;. In our example, the option &lt;code&gt;-t&lt;/code&gt; is
actually an option that follows the package argument so we begin by removing
&lt;code&gt;install&lt;/code&gt; from the argument list and processing the remainder of the line.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;case &amp;quot;$subcommand&amp;quot; in
  install)
    package=$1
    shift # Remove `install` from the argument list
    ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After shifting the argument list we can process the remaining arguments as if
they are of the form &lt;code&gt;package -t src/lib&lt;/code&gt;. The &lt;code&gt;-t&lt;/code&gt; option takes an argument
itself. This argument will be stored in the variable &lt;code&gt;OPTARG&lt;/code&gt; and we save it to
the variable &lt;code&gt;target&lt;/code&gt; for further work.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;case &amp;quot;$subcommand&amp;quot; in
  install)
    package=$1
    shift # Remove `install` from the argument list

  while getopts &amp;quot;:t:&amp;quot; opt; do
    case ${opt} in
      t )
        target=$OPTARG
        ;;
      \? )
        echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
        exit 1
        ;;
      : )
        echo &amp;quot;Invalid Option: -$OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
        exit 1
        ;;
    esac
  done
  shift $((OPTIND -1))
  ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together, we end up with the following script that parses
arguments to our version of &lt;code&gt;pip&lt;/code&gt; and its sub-command &lt;code&gt;install&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;package=&amp;quot;&amp;quot;  # Default to empty package
target=&amp;quot;&amp;quot;  # Default to empty target

# Parse options to the `pip` command
while getopts &amp;quot;:h&amp;quot; opt; do
  case ${opt} in
    h )
      echo &amp;quot;Usage:&amp;quot;
      echo &amp;quot;    pip -h                      Display this help message.&amp;quot;
      echo &amp;quot;    pip install &amp;lt;package&amp;gt;       Install &amp;lt;package&amp;gt;.&amp;quot;
      exit 0
      ;;
   \? )
     echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
     exit 1
     ;;
  esac
done
shift $((OPTIND -1))

subcommand=$1; shift  # Remove &#39;pip&#39; from the argument list
case &amp;quot;$subcommand&amp;quot; in
  # Parse options to the install sub command
  install)
    package=$1; shift  # Remove &#39;install&#39; from the argument list

    # Process package options
    while getopts &amp;quot;:t:&amp;quot; opt; do
      case ${opt} in
        t )
          target=$OPTARG
          ;;
        \? )
          echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
          exit 1
          ;;
        : )
          echo &amp;quot;Invalid Option: -$OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
          exit 1
          ;;
      esac
    done
    shift $((OPTIND -1))
    ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After processing the above sequence of commands, the variable &lt;code&gt;package&lt;/code&gt; will
hold the package to install and the variable &lt;code&gt;target&lt;/code&gt; will hold the target to
install the package to. You can use this as a template for processing any set of
arguments and options to your scripts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing App Engine Dependencies Using pip</title>
      <link>http://sookocheff.com/posts/2014-12-40-managing-app-engine-dependencies-with-pip/</link>
      <pubDate>Tue, 30 Dec 2014 20:35:48 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-40-managing-app-engine-dependencies-with-pip/</guid>
      <description>&lt;p&gt;One unfortunate difficulty when working with App Engine is managing your local
dependencies. You don&amp;rsquo;t have access to your Python environment so all libraries
you wish to use must be &lt;em&gt;vendored&lt;/em&gt; with your installation. That is, you need to
copy all of your library code into a local folder to ship along with your app.&lt;/p&gt;

&lt;p&gt;This usually doesn&amp;rsquo;t cause any problems but difficulties start to crop up when
you manage multiple dependencies that rely on each other. For example, the
official &lt;a href=&#34;https://github.com/elasticsearch/elasticsearch-py&#34;&gt;elasticsearch
client&lt;/a&gt; requires
&lt;a href=&#34;https://github.com/shazow/urllib3&#34;&gt;urllib3&lt;/a&gt; between version &lt;code&gt;1.8&lt;/code&gt; and &lt;code&gt;2.0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Traditionally, &lt;a href=&#34;https://pip.pypa.io/en/latest/&#34;&gt;pip&lt;/a&gt; is used to install these
dependencies on your behalf. The command &lt;code&gt;pip install elasticsearch&lt;/code&gt; will
automatically fetch the urllib3 dependency for you and install it to your local
Python environment. By adding the &lt;code&gt;-t&lt;/code&gt; flag you can provide a destination folder
to install your libraries. As an example, we can install the elasticsearch
and urllib3 libraries to the folder &lt;code&gt;src/lib&lt;/code&gt; with the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install elasticsearch -t src/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works great for App Engine which requires the source of your libraries to
be shipped with your application. Unfortunately, it starts to break down when
you need to upgrade your dependencies. Installing with the &lt;code&gt;-t&lt;/code&gt; flag does not
overwrite the contents of the existing folder so running the same command again
results in an error.&lt;/p&gt;

&lt;p&gt;A solution to this can be found with some basic shell scripting. The first portion of our script installs the requested package and it&amp;rsquo;s
dependencies to a temporary directory and removes any extra files that we don&amp;rsquo;t
need.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install elasticsearch -t $TEMP_DIRECTORY
rm -r $TEMP_DIRECTORY/*.egg-info &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
rm -r $TEMP_DIRECTORY/*.dist-info &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to remove the specific libraries being installed from our App
Engine library directory and copy the contents of our temporary directory in their place.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TARGET=src/lib
for i in $(ls $TEMP_DIRECTORY); do
  rm -r $TARGET/$i &amp;gt;/dev/null 2&amp;gt;&amp;amp;1  # remove existing module
  cp -R $TEMP_DIRECTORY/$i $TARGET  # copy the replacement in
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code can be used as a starting point to write a more user friendly and
robust script. Although this does not truly solve the problem of dependency
management with App Engine it does provide a way to seamlessly vendor Python
libraries and all of their dependencies with your application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine MapReduce API - Part 7: Writing a Custom Output Writer</title>
      <link>http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/</link>
      <pubDate>Mon, 22 Dec 2014 07:07:35 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/</guid>
      <description>

&lt;h2 id=&#34;mapreduce-api-series:5ca834111719e09d1ef6cc6ef5cbc0cd&#34;&gt;MapReduce API Series&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-15-app-engine-mapreduce-api-part-1-the-basics/&#34;&gt;Part 1: The Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-22-app-engine-mapreduce-api-part-2-running-a-mapreduce-job-using-mapreduceyaml/&#34;&gt;Part 2: Running a MapReduce Job Using mapreduce.yaml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;Part 3: Programmatic MapReduce using Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-13-app-engine-mapreduce-api-part-4-combining-sequential-mapreduce-jobs/&#34;&gt;Part 4: Combining Sequential MapReduce Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput/&#34;&gt;Part 5: Using Combiners to Reduce Data Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/&#34;&gt;Part 6: Writing a Custom Input Reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;Part 7: Writing a Custom Output Writer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The MapReduce library supports a number of default output writers. You can also
write your own that implements the output writer interface. This article
examines how to write a custom output writer that pushes data from the App
Engine datastore to an elasticsearch cluster. A similar pattern can be followed
to push the output from your MapReduce job to any number of places.&lt;/p&gt;

&lt;p&gt;An output writer must implement the abstract interface defined by the MapReduce
library. You can find the interface
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/a1844a2652d51c3bef4448c9265c7c5790c9e476/python/src/mapreduce/output_writers.py#L95&#34;&gt;here&lt;/a&gt;.
It may be a good idea to keep a reference to that interface available while
reading this article.&lt;/p&gt;

&lt;p&gt;The most important methods of the interface are &lt;code&gt;create&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.  &lt;code&gt;create&lt;/code&gt;
is used to create a new OutputWriter that will handle writing for a single
shard. Our elasiticsearch OutputWriter takes parameters specifying the
elasticsearch index to write to and the document type. We take advantage of a
helper function provided by the library (&lt;code&gt;_get_params&lt;/code&gt;) to get the parameters of
a MapReduce job given the MapReduce specification.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mapreduce.output_writers import OutputWriter, _get_params

class ElasticSearchOutputWriter(OutputWriter):

    def __init__(self, default_index_name=None, default_doc_type=None):
        super(ElasticSearchOutputWriter, self).__init__()
        self.default_index_name = default_index_name
        self.default_doc_type = default_doc_type
        
    @classmethod
    def create(cls, mr_spec, shard_number, shard_attempt, _writer_state=None):
        params = _get_params(mr_spec)
        return cls(default_index_name=params.get(&#39;default_index_name&#39;,
                   default_doc_type=params.get(&#39;default_doc_type&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we can create an instance of our OutputWriter we can implement the
&lt;code&gt;write&lt;/code&gt; method to write data to elasticsearch. We use a MutationPool for this
(the MutationPool itself will be discussed shortly). The MutationPool is
attached to the current execution context of this MapReduce job. Every MapReduce
job has it&amp;rsquo;s own persistent context that can store information required for the
current execution of the job. This allows multiple OutputWriter shards to write
into the MutationPool and have the MutationPool write data out to its final
destination.&lt;/p&gt;

&lt;p&gt;In this piece of code we check if we have a MutationPool associated with our
context and create a new MutationPool if we don&amp;rsquo;t.  Once we&amp;rsquo;ve retrieved or
created the MutationPool we add the output operation to the pool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mapreduce import context

def write(self, data):
   ctx = context.get()
   es_pool = ctx.get_pool(&#39;elasticsearch_pool&#39;)
   if not es_pool:
       es_pool = _ElasticSearchPool(ctx=ctx,
                                    default_index_name=default_index_name,
                                    default_doc_type=default_doc_type)
       ctx.register_pool(&#39;elasticsearch_pool&#39;, es_pool)

   es_pool.append(data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two methods provide the basis of our OutputWriter, implementing the
&lt;code&gt;to_json&lt;/code&gt;, &lt;code&gt;from_json&lt;/code&gt; and &lt;code&gt;finalize&lt;/code&gt; methods is left up to the reader.
&lt;code&gt;finalize&lt;/code&gt; does not need any functionality but you may want to log a message
upon completion.&lt;/p&gt;

&lt;p&gt;Now on to the MutationPool. The MutationPool acts as a buffered writer of data
changes. It acts as an abstraction that collects any sequence of operations that
are to be performed together. After &lt;code&gt;x&lt;/code&gt; number of operations have been collected
we operate on them all at once.  Mutation pools are strictly a performance
improvement but they can quickly become essential when processing large amounts
of data. For example, rather than writing to the datastore after each map
operation with &lt;code&gt;ndb.put&lt;/code&gt; we can collect a sequence of writes and put them all at
once with &lt;code&gt;ndb.put_multi&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For an &lt;code&gt;elasticsearch&lt;/code&gt; OutputWriter our mutation pool will collect and buffer
indexing tasks and perform them all during a single &lt;a href=&#34;http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/bulk.html&#34;&gt;streaming
bulk&lt;/a&gt;
operation. Within our OutputWriter we collect our sequence of operations in a
private list variable &lt;code&gt;_actions&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class _ElasticSearchPool(context.Pool):
    def __init__(self, ctx=None, default_index_name=None, default_doc_type=None):
        self._actions = []
        self._size = 0
        self._ctx = ctx
        self.default_index_name = default_index_name
        self.default_doc_type = default_doc_type
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then implement the &lt;code&gt;append&lt;/code&gt; method to add an action to the current
MutationPool. In this example we simply add the action to our list. If our list
is greater than &lt;code&gt;200&lt;/code&gt; elements we flush our MutationPool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def append(self, action):
    self._actions.append(action)
    self._size += 1
    if self._size &amp;gt; 200:
        self.flush()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, to flush the MutationPool we write all the data collected so far to
elasticsearch and clear our list of actions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def flush(self):
   es_client = elasticsearch(hosts=[&amp;quot;127.0.0.1&amp;quot;])  # instantiate elasticsearch client
   if self._actions:
       results = helpers.streaming_bulk(es_client,
                                                                   self._actions,
                                                                   chunk_size=200)
    self._actions = []
    self._size = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, as long as the map function of our MapReduce job outputs operations in a
format recognizeable by elasticsearch the OutputWriter will collect those
operations into a MutationPool and periodically flush the results to our
elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;You can use this code as the basis for writing OutputWriters for almost any
custom destination.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
