<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
      <title>Kevin Sookocheff</title>
    <link>http://sookocheff.com/index.xml</link>
    <language>en-us</language>
    <author>Kevin Sookocheff</author>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <updated>Mon, 20 Oct 2014 06:23:04 UTC</updated>
    
    <item>
      <title>Using the Google Prediction API to Predict the Sentiment of a Tweet</title>
      <link>http://sookocheff.com/posts/2014-10-20-prediction-api</link>
      <pubDate>Mon, 20 Oct 2014 06:23:04 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-10-20-prediction-api</guid>
      <description>

&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/prediction/&#34;&gt;Google Prediction API&lt;/a&gt; offers the
power of Google&amp;rsquo;s machine learning algorithms over a RESTful API interface. The
machine learning algorithms themselves are a complete black box. As a user you
upload the training data and, once it has been analyzed, start classifying new
observations based on the analysis of the training data. I recently spent some
time investigating how to use the API to determine the sentiment of a tweet.
This article collects my thoughts on the experience and a few recommendations
for future work.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;The Data&lt;/h2&gt;

&lt;p&gt;For our experiment we took the text and rating of one million online reviews and
normalized them within a scale of zero to 1000 &amp;ndash; ratings on a scale of one to
four and ratings on a scale of one to ten would be roughly equivalent. We then
segmented the reviews into five broad categories: very negative (0-200),
negative (200-400), neutral (400-600), positive (600-800), very
positive (800-1000). The prediction API requires the data to be in a
specific format; following their guidelines, we stripped the review
text of all punctuation except the apostrophe and lower
cased all characters. What was left was a one million row table with
two columns: the review category and the review content.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;very negative, &amp;quot;the waiter was so mean&amp;quot;
positive, &amp;quot;the bisque is the best in town&amp;quot;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our data was roughly 1 GB. We uploaded this file to Google Cloud Storage
and used the Prediction API to train our model given this dataset.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Once we had a trained model it was time to make predictions. For our application
we took tweets from Twitter mentioning a business and asked the Prediction API
to classify the text of the tweet for sentiment between very negative to very
positive using the normalized review categories of our model. The results were
decidedly mixed as the following examples show. In the first example we attempt
to classify the text &amp;ldquo;this restaurant has the best soup in town&amp;rdquo; and correctly
receive a &amp;ldquo;very positive&amp;rdquo; result.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-20-prediction-api/bestsoup.png&#34;&gt;
&lt;img src=&#34;/img/2014-10-20-prediction-api/bestsoup.png&#34; alt=&#34;The Best Soup in Town&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As a counter example, the text &amp;ldquo;this restaurant has the worst soup in town&amp;rdquo; also
recieves a &amp;ldquo;very positive&amp;rdquo; result, although with less confidence and with &amp;ldquo;very
negative&amp;rdquo; being the most likely second choice.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-20-prediction-api/worstsoup.png&#34;&gt;
&lt;img src=&#34;/img/2014-10-20-prediction-api/worstsoup.png&#34; alt=&#34;The Worst Soup in Town&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Most of the tweets were categorized as very positive, regardless of content. In
addition, most of the tweets had almost equal likelihood of being in the very
negative or very positive category with very positive being more likely most of
the time.&lt;/p&gt;

&lt;p&gt;Why is this?&lt;/p&gt;

&lt;p&gt;Most Internet reviews are either very positive or very negative so most of the
content from the tweet will fall into one of these categories in our model. I
believe that by adjusting our training data to have equal amounts of reviews for
each category we would get better results.&lt;/p&gt;

&lt;p&gt;My recommendation is that if you intend to use the Prediction API for a serious
business task that you also have a strong enough background in machine learning
to tweak your model &lt;em&gt;before&lt;/em&gt; using the Prediction API to analyze and host it. In
short, use the Prediction API as cloud-based access to your existing model that
you already know works. Don&amp;rsquo;t use the Prediction API to help you build a working
model. The black box nature of the Prediction API makes it difficult to diagnose
and correct any data problems you may have.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Suggested Searches with Google App Engine</title>
      <link>http://sookocheff.com/posts/2014-10-06-suggested-searches</link>
      <pubDate>Mon, 06 Oct 2014 05:52:29 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-10-06-suggested-searches</guid>
      <description>

&lt;p&gt;At &lt;a href=&#34;http://www.vendasta.com&#34;&gt;VendAsta&lt;/a&gt; we have a few APIs that are backed by
search documents built using the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/search/&#34;&gt;App Engine Search
API&lt;/a&gt;. These APIs are
queried using a search string entered in a text box. One way to improve the user
experience of this text box is to offer the user suggestions of popular searches
to use as their query. This article describes how to achieve this.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Finding the most likely search terms&lt;/h2&gt;

&lt;p&gt;Before presenting suggestions to the user we need to collect the data
determining which searches are popular. This data contains the most likely
choice of search term given a prefix (i.e., an incomplete search term). For
example, given the incomplete search term &lt;code&gt;ne&lt;/code&gt; we need to return the most
frequent searches that have been made using that prefix.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search1.png&#34;&gt;
&lt;img src=&#34;/img/2014-10-06-search-suggestions/search1.png&#34; alt=&#34;Incomplete Search.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Suppose the user searches for the term &lt;code&gt;Netflix&lt;/code&gt;. Given the search term we
increment the frequency of a &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple for each prefix of
the search term &lt;code&gt;Netflix&lt;/code&gt;. The end result is a datastore model with entries for
each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search2.png&#34;&gt;
&lt;img src=&#34;/img/2014-10-06-search-suggestions/search2.png&#34; alt=&#34;Netflix Search.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If that term &lt;code&gt;Netflix&lt;/code&gt; is searched for a second time we increment the frequency
count of each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search3.png&#34;&gt;
&lt;img src=&#34;/img/2014-10-06-search-suggestions/search3.png&#34; alt=&#34;Tuples of Netflix.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now suppose one person searched for the term &lt;code&gt;news&lt;/code&gt;. We build up our frequency
table with each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple again, using &lt;code&gt;news&lt;/code&gt; as the search
term.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search4.png&#34;&gt;
&lt;img src=&#34;/img/2014-10-06-search-suggestions/search4.png&#34; alt=&#34;Tuples of news.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once we&amp;rsquo;ve assembled the data we can go back to our original problem of finding
the most likely searches for a given incomplete search. Given our dataset this
is lookup for each record matching our prefix in the dataset ordered by
frequency.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search5.png&#34;&gt;
&lt;img src=&#34;/img/2014-10-06-search-suggestions/search5.png&#34; alt=&#34;Ordered table.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;A sample implementation&lt;/h2&gt;

&lt;p&gt;The following is a sample implementation encapsulating the ideas presented
above.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;from google.appengine.ext import ndb


class SearchSuggestionModel(ndb.Model):
    &amp;quot;&amp;quot;&amp;quot; Model class for scoring of search frequency. &amp;quot;&amp;quot;&amp;quot;

    created = ndb.DateTimeProperty(auto_now_add=True)
    updated = ndb.DateTimeProperty(auto_now=True)

    prefix = ndb.StringProperty(required=True)
    search_term = ndb.StringProperty(required=True)
    frequency = ndb.IntegerProperty(required=True, default=0)

    @classmethod
    def build_key(cls, prefix, search_term, pid):
        &amp;quot;&amp;quot;&amp;quot; Builds a key in the default namespace. &amp;quot;&amp;quot;&amp;quot;
        id_ = &amp;quot;%s:%s&amp;quot; % (prefix, search_term)
        return ndb.Key(cls, id_, namespace=pid.upper())

    @classmethod
    def prefix_query(cls, prefix, pid):
        &amp;quot;&amp;quot;&amp;quot; Return all models with the matching prefix. Ordered by frequency. &amp;quot;&amp;quot;&amp;quot;
        return cls.query(cls.prefix == prefix, namespace=pid).order(-cls.frequency)

    @classmethod
    def increment(cls, search_term, partner_id):
        &amp;quot;&amp;quot;&amp;quot;
        Given a search_term, increment each (prefix, search_term) combination for all prefixes of that search_term
        &amp;quot;&amp;quot;&amp;quot;
        if not search_term:
            return

        entities = []

        for index, _ in enumerate(search_term):
            prefix = search_term[0:index]
            if prefix:
                key = cls.build_key(prefix, search_term, partner_id)
                entity = key.get()
                if entity:
                    entity.frequency = entity.frequency + 1
                else:
                    # Put new entity
                    entity = cls(key=key, prefix=prefix, search_term=search_term, frequency=1)

                entities.append(entity)

        ndb.put_multi(entities)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Composing Asynchronous Functions With Tasklets</title>
      <link>http://sookocheff.com/posts/2014-09-27-composing-asynchronous-functions-with-tasklets</link>
      <pubDate>Sat, 27 Sep 2014 15:25:29 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-09-27-composing-asynchronous-functions-with-tasklets</guid>
      <description>

&lt;p&gt;Asynchronous functions can provide a boon to application performance by allowing time consuming functions to operate in parallel and without blocking the main execution thread. This article explains how to use the Tasklet API to compose and execute asynchronous functions in Google App Engine.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;ndb.Future&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&#34;https://developers.google.com/appengine/docs/python/ndb/futureclass&#34;&gt;Future&lt;/a&gt; is a class representing an asynchronous I/O operation. &lt;code&gt;ndb&lt;/code&gt; provides asynchronous versions of datastore operations that will return a future instead of immediately returning data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;future = User.get_by_id_async(uid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When a Future is first created it has no data while the I/O operation is running. By calling &lt;code&gt;get_result()&lt;/code&gt; on the Future the application will stop execution of the current thread until the data is available from the I/O operation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;future = User.get_by_id_async(uid)
user = future.get_result()  # Return the data, blocking execution until the data is ready.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code is equivalent to calling the non-asynchronous &lt;code&gt;ndb.get&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;user = User.get_by_id(uid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using futures in this way allows you to run multiple I/O operations in parallel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;# Run two asynchronous operations in parallel
user_future = User.get_by_id_async(uid)
accounts_future = Account.query(Account.user_id==uid).fetch_async()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;ndb.tasklet&lt;/h2&gt;

&lt;p&gt;Tasklets allow you to create your own asynchronous functions that return a Future. The application can call &lt;code&gt;get_result()&lt;/code&gt; on that Future to return the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;tasklet_future = my_tasklet()  # A tasklet
result = tasklet_future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use Tasklets to create fine grained asynchronous functions, in some cases simplifying how a method is programmed.&lt;/p&gt;

&lt;p&gt;When AppEngine encounters a tasklet function the Tasklet framework inserts the tasklet into an event loop. The event loop will cycle through all tasklets and execute them until a &lt;code&gt;yield&lt;/code&gt; statement is reached within the tasklet. The &lt;code&gt;yield&lt;/code&gt; statement is where you put the asynchronous work so that the framework can execute your &lt;code&gt;yield&lt;/code&gt; statement (asynchronously) and then move on to another tasklet to resume execution until its &lt;code&gt;yield&lt;/code&gt; statement is reached. In this way all of the &lt;code&gt;yield&lt;/code&gt; statements are done asynchronously. For even more performance, NDB implements a batch job framework that will bundle up multiple requests in a single batch RPC to the server.&lt;/p&gt;

&lt;p&gt;As a simple example, we can use a tasklet to define an asynchronous query and return the result.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;@ndb.tasklet
def query_tasklet():
    result = yield Model.query().fetch_async()
    raise ndb.Return(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The line &lt;code&gt;result = yield Model.query().fetch_async()&lt;/code&gt; will alert the tasklet framework that this is an asynchronous line of code and that the framework can wait here and execute other code while the asynchronous line completes. To force the asynchronous code to complete you call &lt;code&gt;get_result()&lt;/code&gt; on the return value of the tasklet function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;future = query_tasklet()
future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So how do we use this in our code? There are three distinct cases.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Case 1: Processing an asynchronous result&lt;/h2&gt;

&lt;p&gt;Suppose that you have an asynchronous function that returns a Future and you want to do some processing on the result before returning from your function. In that case you may have code like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;def process_a_query():
    future = Model.query().fetch_async()
    return process_result(future.get_result())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To turn this into an asynchronous tasklet function you can add the tasklet decorator and yield your asynchronous fetch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;@ndb.tasklet
def process_a_query():
    result = yield Model.query().fetch_async()
    raise ndb.Return(process_result(result))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now your function &lt;code&gt;process_a_query&lt;/code&gt; can be called asynchronously.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;future = process_a_query()
# ...
future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Case 2: Composing two asynchronous functions&lt;/h2&gt;

&lt;p&gt;In this case, suppose you have two asynchronous functions that depend on each other and you want to combine them with the tasklet framework.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;def multiple_query():
    future_a = ModelA.query().fetch_async()
    a = future_a.get_result()
    future_b = ModelB.query(ModelB.id==a).fetch_async()
    return future_b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code becomes simpler with tasklets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;@ndb.tasklet
def multiple_query():
    a = yield ModelA.query().fetch_async()
    b = yield ModelB.query(ModelB.id==a).fetch_async()
    raise ndb.Return(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Case 3: Parallel Computation&lt;/h2&gt;

&lt;p&gt;The last case to discuss is parallel computation. In this scenario you have two independent asynchronous functions that you want to run in parallel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;@ndb.tasklet
def parallel_query():
      a, b = yield ModelA.query().fetch_async(), yield ModelB.query().fetch_async()
      raise ndb.Return((a,b))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In all of these cases we show how to combine and compose asynchronous functions using the tasklet framework. This allows you to define your own asynchronous functions that are can be used just like the ndb asynchronous functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working From Home: A Retrospective</title>
      <link>http://sookocheff.com/posts/2014-09-15-working-from-home-retrospective</link>
      <pubDate>Thu, 18 Sep 2014 12:57:32 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-09-15-working-from-home-retrospective</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve spent the past eight months working from home thanks to some great support
from my &lt;a href=&#34;http://www.vendasta.com&#34;&gt;employer&lt;/a&gt;, allowing me to support my wife and
children and still contribute as a meaningful employee. Working from home with
four small children and a loving and supportive wife has brought its fair share
of both challenges and delights. This post will describe the working from home
experience and the lessons I&amp;rsquo;ve learned along the way.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Personal Productivity&lt;/h2&gt;

&lt;p&gt;Without a doubt my personal productivity went up. Working from home meant fewer
distractions, noise and interruptions. VendAsta follows an open office plan
which can be great for ad-hoc communication but terrible for worker satisfaction
and personal productivity. In fact, a recent New Yorker article summarizing the
issue (&lt;a href=&#34;http://www.newyorker.com/business/currency/the-open-office-trap&#34;&gt;The Open-Office
        Trap&lt;/a&gt;)
details numerous studies showing that open office plans are detrimental to
productivity and satisfaction at work. In one study, researchers from the
University of Calgary found that moving to an open office design is &amp;ldquo;negatively
related to workersâ€™ satisfaction with their physical environment and perceived
productivity&amp;rdquo;. In another, Researchers at the University of Sidney found that
&lt;a href=&#34;http://theconversation.com/open-plan-offices-attract-highest-levels-of-worker-dissatisfaction-study-18246&#34;&gt;open office plans have the lowest level of worker
satisfaction&lt;/a&gt;
stating that &amp;ldquo;the disadvantages brought by noise disruption were bigger than the
predicted benefits of increased interaction&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;My experience working from home confirmed these findings and I found that the
decreasing number of interruptions meant I could perform more and better work in
less time.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Team Communication&lt;/h2&gt;

&lt;p&gt;Team communication suffered. At VendAsta, we practice &lt;a href=&#34;http://scrummethodology.com/&#34;&gt;agile development using
scrum&lt;/a&gt; with all of the encompassing roles and
responsibilities. As a team member my role is to estimate stories in the
backlog, commit to a set number of stories per sprint, and to execute on those
stories during the sprint. When the stories were written with careful business
analysis and clear expectations of the acceptance criteria I felt no discernable
difference between working from home and working remotely. When the stories were
written with vague acceptance criteria or without careful thought of the
business implications it was significantly harder to contribute to team
productivity. Our usual approach in this case is to get together as a team in an
ad-hoc meeting to hash out the details of the particular story so that work on
it can resume. Unfortunately, even with modern video chat, screen sharing and
other remote working tools, communicating to a group of people is very difficult
as a remote employee. There is just no substitute to sitting together in the
same room working out a problem on a shared white board.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Work-Life Balance&lt;/h2&gt;

&lt;p&gt;The difference in work-life balance is difficult to analyze. I&amp;rsquo;ll start with the
biggest and most glaring postive &amp;ndash; I was home and helpful to our family during
breakfast, lunch, and dinner for almost a year. You really can&amp;rsquo;t over value how
important that is to a young family. Living in a small city I don&amp;rsquo;t have a very
long commute but the difference between a 20 minute commute and no commute at
all is striking. Being around for lunch or to help put a child to sleep. Just
being in the house so my wife could grab some groceries while everyone was
napping. All of these minor benefits add up to really make the experience of
raising our children that much easier.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Guilt&lt;/h2&gt;

&lt;p&gt;Working from home with my wife and children in the same house meant I was always
aware of the sheer pile of &lt;em&gt;work&lt;/em&gt; my wife was doing each day to help our family
&amp;ndash; while I had the much easier task of sitting at a computer typing on a
keyboard. The dichotomy between the pace and stress level of her day and mine
meant that I was, more often than not, feeling guilty about not helping her
more.  At the same time, if I did stay an extra 15 minutes after lunch to help
her get a particularly fussy child to nap I felt guilty to my employer and team
for not being back at my desk and working. The reality is that, as a remote
worker, I assume the rest of my team thinks I&amp;rsquo;m relaxing in a hammock with a
cocktail if I&amp;rsquo;m unavailable for a few minutes. I&amp;rsquo;m not sure how to assuage that
guilt.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Helpful Tools&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.google.ca/chrome/business/solutions/for-meetings.html&#34;&gt;ChromeBox&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our company was sent a set of ChromeBox units for free as part of a trial
program. They are, without a doubt, excellent for holding meetings with remote
employees.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.hipchat.com/&#34;&gt;HipChat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We started using HipChat internally and the area where it excels is group to
group communication. HipChat makes it effortless to jump into a different teams
chat and ask a quick question.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Not So Helpful Tools&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Google Hangouts.&lt;/p&gt;

&lt;p&gt;This is a tough item to put on the list because it is so great at what it is
meant to do &amp;ndash; one to one video chat. Unfortunately, when you get more than one
person in the same Hangout and that Hangout is being shared on a projector it
quickly degrades into a useability problem. I dare not count the number of
minutes and hours wasted getting Hangouts to properly  share the presenters
screen or to get the proper microphone unmuted for a presentation.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pen and Paper. Whiteboards.&lt;/p&gt;

&lt;p&gt;By far my most used and most useful communication tool is
rendered useless by remote work. I&amp;rsquo;m the type of person that needs time writing,
reading and thinking to come up with solutions to a problem. By
stripping those tools away during remote meetings I found it very
difficult to contribute to joint discussions in a meaningful way.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Qualities of a Remote Worker&lt;/h2&gt;

&lt;p&gt;Remote working is not for everyone. Although I can&amp;rsquo;t claim to have any authority
on the subject I did compile a list of qualities that make a successful remote
worker.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Action Oriented&lt;/p&gt;

&lt;p&gt;A great remote worker needs to gravitate towards taking action rather than
waiting for instruction. Being remote implies that you may not always be up to
date on the discussions and decisions being made in the office. At times it is
up to you to pick up a task and start working on it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Able to Prioritize Independently&lt;/p&gt;

&lt;p&gt;Closely linked to being action oriented is the ability to prioritize. Not being
privy or available for every in office conversation means you need to be able to
judge independently what tasks are high priority. It&amp;rsquo;s important to note that
you &lt;em&gt;may be wrong&lt;/em&gt; in your choice of tasks. As long as you have some level of
trust with your manager or employer and you aren&amp;rsquo;t too far off the mark this
shouldn&amp;rsquo;t be an issue.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Strong Written Communicator&lt;/p&gt;

&lt;p&gt;As a remote worker one of your primary communication channels is e-mail. Being a
strong writer complements so many other professional attributes that you really
cannot value this enough. It&amp;rsquo;s my opinion that every knowledge worker should be
comfortable writing and presenting their ideas.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_7&#34;&gt;Qualities of a Remote Manager&lt;/h2&gt;

&lt;p&gt;Remote work is not for every job. The remote employee&amp;rsquo;s job should be well
defined with clear goals. There also must be some level of trust between the
employee and the employer. In the end, you are a professional and it is up to
you to act as a professional no matter where you are working. That said, there
are a few qualities that the employer needs to bring to the table as well.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Trust&lt;/p&gt;

&lt;p&gt;The employer has to trust their employee to get the job down to the best of
their ability. You hired the person to begin with so some level of trust must be
present. The trust level cannot be different for remote and in office workers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Planning&lt;/p&gt;

&lt;p&gt;The product manager is generally responsible for determining the project goals
and how they are prioritized. Remote workers need a manager with a strong
ability to distill and communicate the project goals so that the worker can
independently choose tasks without unnecessary communication barriers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;I really enjoyed my time as a remote worker, but equally enjoy being back in the
office. Things are not better or worse in either case. Just different.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Converting an ndb model to a BigQuery schema</title>
      <link>http://sookocheff.com/posts/2014-08-14-converting-an-ndb-model-to-a-bigquery-schema</link>
      <pubDate>Thu, 14 Aug 2014 17:58:03 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-08-14-converting-an-ndb-model-to-a-bigquery-schema</guid>
      <description>

&lt;p&gt;I have been working on the problem of recording changes to an ndb model. One way to accomplish this is to stream data changes to a BigQuery table corresponding to the ndb model. It would be great to do this in a generic way which gives us the problem of generating a BigQuery table given an ndb model. This article will describe one solution to this problem.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Accessing the properties of an ndb class&lt;/h2&gt;

&lt;p&gt;The first step in the process is to find all the properties of the class via the
ndb &lt;code&gt;_properties&lt;/code&gt; accessor. By iterating over this field we can find all of the
properties on the class and their ndb types.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;def tablify(ndb_model):
    &amp;quot;&amp;quot;&amp;quot;
    Convert ndb_model into a BigQuery table schema.
    &amp;quot;&amp;quot;&amp;quot;
    for name, ndb_type in ndb_model.__class__._properties.iteritmes():
       print name, ndb_type
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Converting properties to BigQuery schema types&lt;/h2&gt;

&lt;p&gt;Now that we have the set of properties on the class we can map from the type of
each property to a BigQuery type. Here is a helper function that provides a
simple mapping.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;def ndb_type_to_bigquery_type(_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert a python type to a bigquery type.
    &amp;quot;&amp;quot;&amp;quot;
    if isinstance(_type, ndb.IntegerProperty):
        return &amp;quot;INTEGER&amp;quot;
    elif isinstance(_type, ndb.FloatProperty):
        return &amp;quot;FLOAT&amp;quot;
    elif isinstance(_type, ndb.BooleanProperty):
        return &amp;quot;BOOLEAN&amp;quot;
    elif type(_type) in [ndb.StringProperty, ndb.TextProperty, ndb.ComputedProperty]:
        return &amp;quot;STRING&amp;quot;
    elif type(_type) in [ndb.DateTimeProperty, ndb.DateProperty, ndb.TimeProperty]:
        return &amp;quot;TIMESTAMP&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last task is to format everything as a &lt;a href=&#34;https://developers.google.com/bigquery/docs/reference/v2/tables&#34;&gt;BigQuery table
resource&lt;/a&gt;. This
involves adding some boiler-plate around each field in our BigQuery schema and
fleshing out the structure of the JSON.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;
from google.appengine.ext import ndb

SUPPORTED_TYPES = [ndb.IntegerProperty,
                   ndb.FloatProperty,
                   ndb.BooleanProperty,
                   ndb.StringProperty,
                   ndb.TextProperty,
                   ndb.DateTimeProperty,
                   ndb.DateProperty,
                   ndb.TimeProperty,
                   ndb.ComputedProperty]


def ndb_type_to_bigquery_type(_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert a python type to a bigquery type.
    &amp;quot;&amp;quot;&amp;quot;
    if isinstance(_type, ndb.IntegerProperty):
        return &amp;quot;INTEGER&amp;quot;
    elif isinstance(_type, ndb.FloatProperty):
        return &amp;quot;FLOAT&amp;quot;
    elif isinstance(_type, ndb.BooleanProperty):
        return &amp;quot;BOOLEAN&amp;quot;
    elif type(_type) in [ndb.StringProperty, ndb.TextProperty, ndb.ComputedProperty]:
        return &amp;quot;STRING&amp;quot;
    elif type(_type) in [ndb.DateTimeProperty, ndb.DateProperty, ndb.TimeProperty]:
        return &amp;quot;TIMESTAMP&amp;quot;


def ndb_property_to_bigquery_field(name, ndb_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert from ndb property to a BigQuery schema table field.
    &amp;quot;&amp;quot;&amp;quot;
    if type(ndb_type) not in SUPPORTED_TYPES:
        raise ValueError(&#39;Unsupported object property&#39;)

    field = {
        &amp;quot;description&amp;quot;: name,
        &amp;quot;name&amp;quot;: name,
        &amp;quot;type&amp;quot;: ndb_type_to_bigquery_type(ndb_type)
    }

    if ndb_type._repeated:
        field[&#39;mode&#39;] = &#39;REPEATED&#39;

    return field


def tablify_schema(obj):
    &amp;quot;&amp;quot;&amp;quot;
    Convert ndb_model into a BigQuery table schema.
    &amp;quot;&amp;quot;&amp;quot;
    table_schema = {&#39;fields&#39;: []}
    
    for name, ndb_type in obj.__class__._properties.iteritems():
        table_schema[&#39;fields&#39;].append(ndb_property_to_bigquery_field(name, ndb_type))

    return table_schema


def tablify(obj, project_id, dataset_id, table_id):
    &amp;quot;&amp;quot;&amp;quot;
    Return a BigQuery table resource representing an ndb object.
    &amp;quot;&amp;quot;&amp;quot;
    return {
        &amp;quot;kind&amp;quot;: &amp;quot;bigquery#table&amp;quot;,
        &amp;quot;id&amp;quot;: table_id,
        &amp;quot;tableReference&amp;quot;: {
            &amp;quot;projectId&amp;quot;: project_id,
            &amp;quot;datasetId&amp;quot;: dataset_id,
            &amp;quot;tableId&amp;quot;: table_id
        },
        &amp;quot;description&amp;quot;: &amp;quot;Table Resource&amp;quot;,
        &amp;quot;schema&amp;quot;: tablify_schema(obj)
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Creating the new table on BigQuery.&lt;/h2&gt;

&lt;p&gt;Now that we have a BigQuery schema we can create the table in BigQuery using the BigQuery api client.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;from oauth2client.appengine import AppAssertionCredentials
from apiclient.discovery import build

credentials = AppAssertionCredentials(scope=&#39;https://www.googleapis.com/auth/bigquery&#39;)
http = credentials.authorize(httplib2.Http())
big_query_service = build(&#39;bigquery&#39;, &#39;v2&#39;, http=http)
        
table_resource = tablify(ndb_model, project_id, dataset_id, table_id)
                response = big_query_service.tables().insert(projectId=project_id,
                                                             datasetId=dataset_id,
                                                             body=table_resource).execute()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it. This article outlined a quick method of generating a BigQuery
table scheme from an ndb model. If you found this useful let me know in the
comments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Restoring an App Engine backup into a Big Query table</title>
      <link>http://sookocheff.com/posts/2014-08-04-restoring-an-app-engine-backup</link>
      <pubDate>Mon, 04 Aug 2014 21:18:13 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-08-04-restoring-an-app-engine-backup</guid>
      <description>

&lt;p&gt;An unfortunate DevOps task for any team running App Engine is restoring data
from backups. One way to do this is by accessing the Google Cloud Storage URL
for a given App Engine backup and importing that backup into BigQuery. This
article will show you to get the Cloud Storage URL for an App Engine backup and
manually perform that import.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Getting the Cloud Storage URL&lt;/h2&gt;

&lt;p&gt;The first thing you need to do is access the cloud storage URL for a given App
Engine backup. First, log in to the Google Developer Console and navigate to
your backup. The filename of the backup will be a long sequence of characters
followed by the name of your model. The file extension will be &lt;code&gt;.backup_info&lt;/code&gt;.
As an example, this is the filename of backup for an Account model used in one
of our projects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right click on your backup and copy the URL to your clipboard. The URL will be
of the form below. The name of your cloud storage bucket and the identifier for
you app have been highlighted below. Replace these with appropriate values for
your project.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;https://console.developers.google.com/m/cloudstorage/b/**bucket**/o/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To get the cloud storage URL in the format expected by a BigQuery import remove
everything up to the bucket name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;**bucket**/o/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now remove the &lt;code&gt;o&lt;/code&gt; between the bucket name and your app identifier.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;**bucket**/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, append &lt;code&gt;gs://&lt;/code&gt; to the file to arrive at your final Google Cloud Storage
URL.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;gs://**bucket**/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to import the backup into BigQuery. To do this, navigate to
your project and create a new table in your desired dataset.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-08-04-restoring-an-app-engine-backup/create-new-table.png&#34;&gt;
&lt;img src=&#34;/img/2014-08-04-restoring-an-app-engine-backup/create-new-table.png&#34; alt=&#34;Create new table.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In the &lt;code&gt;Choose destination&lt;/code&gt; tab pick a name for your new table. In my case I&amp;rsquo;ll
name the table with the date of my backup for reference.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-08-04-restoring-an-app-engine-backup/choose-destination.png&#34;&gt;
&lt;img src=&#34;/img/2014-08-04-restoring-an-app-engine-backup/choose-destination.png&#34; alt=&#34;Choose destination&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Next, choose App Engine Datastore Backup as the source format and paste the
Cloud Storage URL you arrived at above in the appropriate field.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-08-04-restoring-an-app-engine-backup/select-data.png&#34;&gt;
&lt;img src=&#34;/img/2014-08-04-restoring-an-app-engine-backup/select-data.png&#34; alt=&#34;Select Data Source&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You can choose the defaults for the next tabs and, finally, import your App
Engine backup into BigQuery and watch it being fully restored.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bypassing ndb hooks with the RawDatastoreInputReader</title>
      <link>http://sookocheff.com/posts/2014-07-29-bypassing-ndb-hooks-with-the-raw-datastore-input-reader</link>
      <pubDate>Tue, 29 Jul 2014 20:32:42 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-07-29-bypassing-ndb-hooks-with-the-raw-datastore-input-reader</guid>
      <description>

&lt;p&gt;When doing a MapReduce operation there are times when you want to edit a set of entities without triggering the post or pre put hooks associated with those entities. On such ocassions using the raw datastore entity allows you to process the data without unwanted side effects.&lt;/p&gt;

&lt;p&gt;For the sake of this discussion let&amp;rsquo;s assume we want to move a &lt;code&gt;phone_number&lt;/code&gt; field to a &lt;code&gt;work_number&lt;/code&gt; field for all entities of a certain Kind in the datastore.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Getting the raw datastore entity&lt;/h2&gt;

&lt;p&gt;The MapReduce library provides a &lt;code&gt;RawDatastoreInputReader&lt;/code&gt; that will feed raw datastore entities to your mapping function. We can set our MapReduce operation to use the &lt;code&gt;RawDatastoreInputReader&lt;/code&gt; using a &lt;code&gt;mapreduce.yaml&lt;/code&gt; declaration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- name: move_phone_numbers
  mapper:
    input_reader: mapreduce.input_readers.RawDatastoreInputReader
    handler: app.pipelines.move_phone_numbers_map
    params:
    - name: entity_kind
      default: MyModel
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Manipulating a raw datastore entity&lt;/h2&gt;

&lt;p&gt;Our &lt;code&gt;raw_datastore_map&lt;/code&gt; function to use the datastore entity in its raw form. The raw form of the datastore entity provides a dictionary like interface that we can use to manipulate the entity. With this interface we can move the phone number to the correct field.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def move_phone_numbers_map(entity):
    phone_number = entity.get(&#39;phone_number&#39;)
    if phone_number:
        entity[&#39;work_number&#39;] = phone_number
    del entity[&#39;phone_number&#39;]
    
    yield op.db.Put(entity)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using &lt;code&gt;op.db.Put&lt;/code&gt; will put the entity to the datastore using the raw datastore
API, thereby bypassing any ndb hooks that are in place.  For more information on
the raw datastore API the best resource is the source code itself, available
from the &lt;a href=&#34;https://code.google.com/p/googleappengine/source/browse/trunk/python/google/appengine/api/datastore.py&#34;&gt;App Engine SDK
repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generating a C# client for an App Engine Cloud Endpoints API</title>
      <link>http://sookocheff.com/posts/2014-07-22-generating-a-c-sharp-client</link>
      <pubDate>Tue, 22 Jul 2014 06:29:56 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-07-22-generating-a-c-sharp-client</guid>
      <description>&lt;p&gt;The Cloud Endpoints API comes packaged with endpointscfg.py to generate client
libraries in JavaScript, Objective-C (for iOS) and Java (for Android). You can
also generate a few additional client libraries using the &lt;a href=&#34;https://code.google.com/p/google-apis-client-generator/&#34;&gt;Google APIs client
generator&lt;/a&gt;. This
article will show you how to use the generator to create a C# client library.&lt;/p&gt;

&lt;p&gt;The client generator is a Python application you can install with &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;pip install google-apis-client-generator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The client generator works by taking an API discovery document, parsing it into
an object model, and then using a language template to transform the object
model to running code.&lt;/p&gt;

&lt;p&gt;To run the generator you will need the discovery document for your API. You can
find this document from the root API discovery URL. First, download the root API
discovery document using &lt;a href=&#34;https://github.com/jakubroztocil/httpie&#34;&gt;httpie&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;http --download https://example.appspot.com/_ah/api/discovery/v1/apis
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;json&#34;&gt;&amp;quot;items&amp;quot;: [
    {
        &amp;quot;description&amp;quot;: &amp;quot;Example Api&amp;quot;,
        &amp;quot;discoveryLink&amp;quot;: &amp;quot;./apis/example/v1/rest&amp;quot;,
        &amp;quot;discoveryRestUrl&amp;quot;: &amp;quot;https://example.appspot.com/_ah/api/discovery/v1/apis/example/v1/rest&amp;quot;,
        &amp;quot;icons&amp;quot;: {
            &amp;quot;x16&amp;quot;: &amp;quot;http://www.google.com/images/icons/product/search-16.gif&amp;quot;,
            &amp;quot;x32&amp;quot;: &amp;quot;http://www.google.com/images/icons/product/search-32.gif&amp;quot;
        },
        &amp;quot;id&amp;quot;: &amp;quot;example:v1&amp;quot;,
        &amp;quot;kind&amp;quot;: &amp;quot;discovery#directoryItem&amp;quot;,
        &amp;quot;name&amp;quot;: &amp;quot;example&amp;quot;,
        &amp;quot;preferred&amp;quot;: true,
        &amp;quot;version&amp;quot;: &amp;quot;v1&amp;quot;
    }
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The root discovery document will have an &lt;code&gt;items&lt;/code&gt; member listing the available
APIs and a &lt;code&gt;discoveryLink&lt;/code&gt; for each API. The &lt;code&gt;discoveryLink&lt;/code&gt; provides the schema
for the API. We can download this schema and use it as input to the client
generator.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;http --download https://example.appspot.com/_ah/api/discovery/v1/apis/example/v1/rest

generate_library --input=rest.json --language=csharp --output_dir=tmp/csharp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your C# client library is now ready to use. As of this writing you can generate
client libraries in C++, C#, Dart, GWT, Java, PHP and Python.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Basic Authentication with Google Cloud Endpoints</title>
      <link>http://sookocheff.com/posts/2014-07-16-using-basic-authentication-with-google-cloud-endpoints</link>
      <pubDate>Wed, 16 Jul 2014 01:02:25 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-07-16-using-basic-authentication-with-google-cloud-endpoints</guid>
      <description>

&lt;p&gt;Cloud Endpoints provides strong integration with OAuth 2.0. If you can use this
integration &amp;ndash; do it. However, some legacy systems require supporting
alternative authentication mechanisms. This article will show you how to secure
an API endpoint using Basic Authentication. You can use this as a starting point
for whatever authentication method you choose.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;A basic endpoint&lt;/h2&gt;

&lt;p&gt;As a starting point let&amp;rsquo;s define a basic endpoint that will return a
hypothetical UserMessage defining a User resource.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(message_types.VoidMessage,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;user&#39;,
                      name=&#39;user.auth&#39;)
    def user_auth(self, request):
        ## Return a UserMessage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s build up a UserMessage based on the credentials set in the HTTP
Authorization header. We can access the HTTP headers of the request through the
&lt;a href=&#34;https://developers.google.com/appengine/docs/python/tools/protorpc/remote/httprequeststateclass&#34;&gt;HTTPRequestState&lt;/a&gt;
using the instance variable &lt;code&gt;request_state&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(message_types.VoidMessage,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;user&#39;,
                      name=&#39;user.auth&#39;)
    def user_auth(self, request):
        basic_auth = self.request_state.headers.get(&#39;authorization&#39;)
        print basic_auth
        ## Return a UserMessage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can test this endpoint using &lt;a href=&#34;https://github.com/jakubroztocil/httpie&#34;&gt;httpie&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http -a username:password GET :8888/_ah/api/users/v1/user
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Examing the logs will show that we receive the HTTP Authorization header in its
base64 encoded form.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;Basic dXNlcm5hbWU6cGFzc3dvcmQ=
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The header can be decoded with the &lt;code&gt;base64&lt;/code&gt; module.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;basic_auth = self.request_state.headers.get(&#39;authorization&#39;)
auth_type, credentials = basic_auth.split(&#39; &#39;)
print base64.b64decode(credentials)  # prints username:password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the username and password we can check the datastore for a User model with
the same credentials and return a &lt;code&gt;UserMessage&lt;/code&gt; based on the model .&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(message_types.VoidMessage,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;user&#39;,
                      name=&#39;user.auth&#39;)
    def user_auth(self, request):
        basic_auth = self.request_state.headers.get(&#39;authorization&#39;)
        auth_type, credentials = basic_auth.split(&#39; &#39;)
        username, password = base64.b64decode(credentials).split(&#39;:&#39;)
        user = User.get_by_username(username)
        if user and user.verify_password(password):
            return user.to_message()
        else:
            raise endpoints.UnauthorizedException
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should serve as a starting point for anyone wishing to use Basic
Authentication with Google Cloud Endpoints. If you&amp;rsquo;ve read this far, why not
subscribe to this blog through &lt;a href=&#34;http://kevinsookocheff.us3.list-manage2.com/subscribe?u=8b57d632b8677f07ca57dc9cb&amp;amp;id=ec7ddaa3ba&#34;&gt;email&lt;/a&gt; or &lt;a href=&#34;http://sookocheff.com/index.xml&#34;&gt;RSS&lt;/a&gt;?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unit Testing Cloud Endpoints</title>
      <link>http://sookocheff.com/posts/2014-07-10-unit-testing-cloud-endpoints</link>
      <pubDate>Thu, 10 Jul 2014 14:32:15 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-07-10-unit-testing-cloud-endpoints</guid>
      <description>

&lt;p&gt;Writing unit tests for App Engine Cloud Endpoints is a fairly straight forward
process. Unfortunately it is not well documented and a few gotchas exist. This
article provides a template you can use to unit test Cloud Endpoints including
full source code for a working example.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;The Model&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s use a simple User model as the resource being exposed by our API. This
model has two properties &amp;ndash; a username and an email address. The class also
provides &lt;code&gt;to_message&lt;/code&gt; function that converts the model to a ProtoRPC Message for
transmission by the Cloud Endpoints API.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;class User(ndb.Model):
    &amp;quot;&amp;quot;&amp;quot;
    A basic user model.
    &amp;quot;&amp;quot;&amp;quot;
    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def to_message(self):
        &amp;quot;&amp;quot;&amp;quot;
        Convert the model to a ProtoRPC messsage.
        &amp;quot;&amp;quot;&amp;quot;
        return UserMessage(id=self.key.id(),
                           username=self.username,
                           email=self.email)


class UserMessage(messages.Message):
    &amp;quot;&amp;quot;&amp;quot;
    A message representing a User model.
    &amp;quot;&amp;quot;&amp;quot;
    id = messages.IntegerField(1)
    username = messages.StringField(2)
    email = messages.StringField(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;The API&lt;/h2&gt;

&lt;p&gt;To keep things simple the API for this resource provides a single &lt;code&gt;GET&lt;/code&gt; endpoint
that returns a &lt;code&gt;UserMessage&lt;/code&gt; based on a &lt;code&gt;User&lt;/code&gt; in the datastore. We parameterize
our endpoint with an &lt;code&gt;ID_RESOURCE&lt;/code&gt; that takes an &lt;code&gt;IntegerField&lt;/code&gt; holding the id
of the User resource.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ID_RESOURCE = endpoints.ResourceContainer(message_types.VoidMessage,
                                          id=messages.IntegerField(1, 
                                                                   variant=messages.Variant.INT32, 
                                                                   required=True))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The API itself has one method, &lt;code&gt;users_get&lt;/code&gt;, that returns a user given an id or
&lt;code&gt;404&lt;/code&gt; if no user with the specified id exists.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(ID_RESOURCE,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;users/{id}&#39;,
                      name=&#39;users.get&#39;)
    def users_get(self, request):
        entity = User.get_by_id(request.id)
        if not entity:
            message = &#39;No user with the id &amp;quot;%s&amp;quot; exists.&#39; % request.id
            raise endpoints.NotFoundException(message)

        return entity.to_message()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;The Tests&lt;/h2&gt;

&lt;p&gt;The setup for our tests is similar to many App Engine test cases. We set our
environment and initialize any test stubs we may need.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;class GaeTestCase(unittest.TestCase):
    &amp;quot;&amp;quot;&amp;quot;
    API unit tests.
    &amp;quot;&amp;quot;&amp;quot;

    def setUp(self):
        super(GaeTestCase, self).setUp()
        tb = testbed.Testbed()
        tb.setup_env(current_version_id=&#39;testbed.version&#39;)  # Required for the endpoints API
        tb.activate()
        tb.init_all_stubs()
        self.api = UsersApi()  # Set our API under test
        self.testbed = tb

    def tearDown(self):
        self.testbed.deactivate()
        super(GaeTestCase, self).tearDown()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The actual tests call the endpoints method directly. Endpoint methods that
are set to receive a &lt;code&gt;ResourceContainer&lt;/code&gt; expect a &lt;code&gt;CombinedContainer&lt;/code&gt; as the
parameter to the function. The &lt;code&gt;ResourceContainer&lt;/code&gt; class has a property called
&lt;code&gt;combined_message_class&lt;/code&gt; that returns a &lt;code&gt;CombinedContainer&lt;/code&gt; class that can be
instantiated and passed to our endpoint. We instantiate our container with the
identifier we expect for our User resource.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;def test_get_returns_entity(self):
    user = User(username=&#39;soofaloofa&#39;, email=&#39;soofaloofa@example.com&#39;)
    user.put()

    container = ID_RESOURCE.combined_message_class(id=user.key.id())
    response = self.api.users_get(container)
    self.assertEquals(response.username, &#39;soofaloofa&#39;)
    self.assertEquals(response.email, &#39;soofaloofa@example.com&#39;)
    self.assertEquals(response.id, user.key.id())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also add a test for the &lt;code&gt;404&lt;/code&gt; condition by calling &lt;code&gt;assertRaises&lt;/code&gt; on our
endpoint with an identifier that does not correspond to a User resource.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;def test_get_returns_404_if_no_entity(self):
    container = ID_RESOURCE.combined_message_class(id=1)
    self.assertRaises(endpoints.NotFoundException, self.api.users_get, container)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Full source code follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;import unittest
import endpoints

from protorpc import remote
from protorpc import messages
from protorpc import message_types
from google.appengine.ext import testbed
from google.appengine.ext import ndb


class User(ndb.Model):
    &amp;quot;&amp;quot;&amp;quot;
    A basic user model.
    &amp;quot;&amp;quot;&amp;quot;
    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def to_message(self):
        &amp;quot;&amp;quot;&amp;quot;
        Convert the model to a ProtoRPC messsage.
        &amp;quot;&amp;quot;&amp;quot;
        return UserMessage(id=self.key.id(),
                           username=self.username,
                           email=self.email)


class UserMessage(messages.Message):
    &amp;quot;&amp;quot;&amp;quot;
    A message representing a User model.
    &amp;quot;&amp;quot;&amp;quot;
    id = messages.IntegerField(1)
    username = messages.StringField(2)
    email = messages.StringField(3)

ID_RESOURCE = endpoints.ResourceContainer(message_types.VoidMessage,
                                          id=messages.IntegerField(1, variant=messages.Variant.INT32, required=True))


@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(ID_RESOURCE,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;users/{id}&#39;,
                      name=&#39;users.get&#39;)
    def users_get(self, request):
        entity = User.get_by_id(request.id)
        if not entity:
            message = &#39;No user with the id &amp;quot;%s&amp;quot; exists.&#39; % request.id
            print message
            raise endpoints.NotFoundException(message)

        return entity.to_message()


class GaeTestCase(unittest.TestCase):
    &amp;quot;&amp;quot;&amp;quot;
    API unit tests.
    &amp;quot;&amp;quot;&amp;quot;

    def setUp(self):
        super(GaeTestCase, self).setUp()
        tb = testbed.Testbed()
        tb.setup_env(current_version_id=&#39;testbed.version&#39;)
        tb.activate()
        tb.init_all_stubs()
        self.api = UsersApi()
        self.testbed = tb

    def tearDown(self):
        self.testbed.deactivate()
        super(GaeTestCase, self).tearDown()

    def test_get_returns_entity(self):
        user = User(username=&#39;soofaloofa&#39;, email=&#39;soofaloofa@example.com&#39;)
        user.put()

        container = ID_RESOURCE.combined_message_class(id=user.key.id())
        response = self.api.users_get(container)
        self.assertEquals(response.username, &#39;soofaloofa&#39;)
        self.assertEquals(response.email, &#39;soofaloofa@example.com&#39;)
        self.assertEquals(response.id, user.key.id())

    def test_get_returns_404_if_no_entity(self):
        container = ID_RESOURCE.combined_message_class(id=1)
        self.assertRaises(endpoints.NotFoundException, self.api.users_get, container)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Creating RESTful APIs with App Engine Cloud Endpoints</title>
      <link>http://sookocheff.com/posts/2014-07-02-creating-restful-apis-with-cloud-endpoints</link>
      <pubDate>Wed, 02 Jul 2014 06:14:23 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-07-02-creating-restful-apis-with-cloud-endpoints</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://developers.google.com/appengine/docs/python/endpoints/&#34;&gt;App Engine Cloud
Endpoints&lt;/a&gt; is a
great way to quickly and easily create JSON API endpoints. What&amp;rsquo;s not clear is
how to structure your &lt;code&gt;Message&lt;/code&gt; code to support a RESTful
create-read-update-delete (CRUD) API. This article will show the basic CRUD
operations for one Resource. The results can easily be adapted to support a full
REST API.&lt;/p&gt;

&lt;p&gt;To support this discussion let&amp;rsquo;s use a concrete resource for our API &amp;ndash; a &lt;code&gt;User&lt;/code&gt;
resource. We can give our &lt;code&gt;User&lt;/code&gt; model a few simple attributes.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-07-02-creating-restful-apis-with-app-engine-cloud-endpoints/user-model.png&#34;&gt;
&lt;img src=&#34;/img/2014-07-02-creating-restful-apis-with-app-engine-cloud-endpoints/user-model.png&#34; alt=&#34;User Model&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A CRUD API for this resource would support a URL structure and HTTP verbs
for each operation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;# Create a new user
HTTP POST /users/

# Read a user by id
HTTP GET /users/{id}

# Update a user by id
HTTP PUT /users/{id}

# Delete a user by id
HTTP DELETE /users/{id}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given this model we can define a basic Cloud Endpoints message representing a &lt;code&gt;User&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;class UserMessage(messages.Message):
    id = messages.StringField(1)
    email = messages.StringField(2)
    username = messages.StringField(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can write the &lt;strong&gt;C&lt;/strong&gt; (create) portion of the CRUD API using HTTP POST
and a &lt;code&gt;ResourceContainer&lt;/code&gt; to hold the message we wish to submit to the API.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;POST_RESOURCE = endpoints.ResourceContainer(UserMessage)

...

@endpoints.method(POST_RESOURCE,
                  UserMessage,
                  path=&#39;/users&#39;,
                  http_method=&#39;POST&#39;,
                  name=&#39;users.create&#39;)
def create(self, request):
    user = User(username=request.username, email=request.email)
    user.put()
    return user.to_message()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Similarly we can define the &lt;strong&gt;R&lt;/strong&gt; (read) portion of the API using an HTTP GET
method. To parameterize our cloud endpoint we need to add the parameter to our
&lt;code&gt;ResourceContainer&lt;/code&gt;. I&amp;rsquo;ll call it &lt;code&gt;id&lt;/code&gt; here. The actual message type is
&lt;code&gt;VoidMessage&lt;/code&gt; because we are not passing any information in our request to the
API endpoint other than the &lt;code&gt;id&lt;/code&gt; parameter.&lt;/p&gt;

&lt;p&gt;The response retrieves the entity from the datastore and returns it as a
message.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;ID_RESOURCE = endpoints.ResourceContainer(message_types.VoidMessage,
                                          id=messages.StringField(1,
                                                                  variant=messages.Variant.STRING,
                                                                  required=True))

...

@endpoints.method(ID_RESOURCE,
                  UserMessage,
                  http_method=&#39;GET&#39;,
                  path=&#39;users/{id}&#39;,
                  name=&#39;users.read&#39;)
def read(self, request):
    entity = User.get_by_id(request.id)
    if not entity:
        message = &#39;No User with the id &amp;quot;%s&amp;quot; exists.&#39; % request.id
        raise endpoints.NotFoundException(message)

    return entity.to_message()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;strong&gt;U&lt;/strong&gt; (update) operation uses a similar parameterized &lt;code&gt;ResourceContainer&lt;/code&gt; to
access a User given an id. We augment this request with the &lt;code&gt;UserMessage&lt;/code&gt; which
defines the content of the body of the message. The endpoint takes the content
of the message and updates the entity with that content.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;PUT_RESOURCE = endpoints.ResourceContainer(UserMessage,
                                           id=messages.StringField(1,
                                                                   variant=messages.Variant.STRING,
                                                                   required=True))

...

@endpoints.method(PUT_RESOURCE,
                  UserMessage,
                  http_method=&#39;PUT&#39;,
                  path=&#39;users/{id}&#39;,
                  name=&#39;users.update&#39;)
def update(self, request):
    entity = User.update_from_message(request.id, request)
    if not entity:
        message = &#39;No User with the id &amp;quot;%s&amp;quot; exists.&#39; % request.id
        raise endpoints.NotFoundException(message)

    return entity.to_message()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Lastly, the &lt;strong&gt;D&lt;/strong&gt; (delete) endpoint takes an identifier which we have previously
defined as &lt;code&gt;ID_RESOURCE&lt;/code&gt;. The endpoint deletes the entity referred to by that
identifier and returns a &lt;code&gt;VoidMessage&lt;/code&gt; which is converted to an &lt;code&gt;HTTP 204 No
Content&lt;/code&gt; response by the cloud endpoints API.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;@endpoints.method(ID_RESOURCE,
                  message_types.VoidMessage,
                  http_method=&#39;DELETE&#39;,
                  path=&#39;users/{id}&#39;,
                  name=&#39;users.delete&#39;)
def delete(self, request):
    entity = User.get_by_id(request.id)
    if not entity:
        message = &#39;No User with the id &amp;quot;%s&amp;quot; exists.&#39; % request.id
        raise endpoints.NotFoundException(message)

    entity.key.delete()
    return message_types.VoidMessage()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This basic pattern can be used with any resource that your API wishes to
support and gives a basic pattern with which to build out your full API.&lt;/p&gt;

&lt;p&gt;If you have any questions please send me an email or let me know in the
comments!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Running Multiple App Engine Modules Locally with dev_appserver.py</title>
      <link>http://sookocheff.com/posts/2014-06-23-running-multiple-app-engine-modules-locally</link>
      <pubDate>Tue, 17 Jun 2014 13:09:42 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-06-23-running-multiple-app-engine-modules-locally</guid>
      <description>&lt;p&gt;The recently released &lt;a href=&#34;https://developers.google.com/appengine/docs/python/modules/&#34;&gt;App Engine Modules API&lt;/a&gt; allows developers to compartmentalize their applications into logical units that can share state using the datastore or memcache.&lt;/p&gt;

&lt;p&gt;The documentation for this API is fairly complete but one part is lacking â€” running multiple modules locally using dev_appserver.py. Thankfully, the solution is not too complicated.  Just pass the list of &lt;code&gt;.yaml&lt;/code&gt; files defining your modules to dev_appserver and it will run all of your modules locally.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;dev_appserver.py src/app.yaml src/backend.yaml src/dispatch.yaml
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Downloading directories of code from Github using the Github API</title>
      <link>http://sookocheff.com/posts/2014-06-17-downloading-directories-of-code-from-github-using-the-github-api</link>
      <pubDate>Tue, 17 Jun 2014 06:14:23 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-06-17-downloading-directories-of-code-from-github-using-the-github-api</guid>
      <description>

&lt;p&gt;At &lt;a href=&#34;http://www.vendAsta.com&#34;&gt;VendAsta&lt;/a&gt; we frequently share libraries of code
between projects. To make it easier to share this code I&amp;rsquo;ve developed a small
package manager that downloads code within a directory from Github to be copied
in to your current project. It&amp;rsquo;s a quick and dirty alternative to cloning an
entire repository, grabbing the set of files you want and placing them in your
project.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ll use the &lt;a href=&#34;https://github.com/jacquev6/PyGithub&#34;&gt;PyGithub&lt;/a&gt; Python library to
interact with the Github API.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Logging in to Github&lt;/h2&gt;

&lt;p&gt;The first step is to log in to Github using our credentials. To do this we
instantiate a new Github object given our username and password and access the
associated user by calling &lt;code&gt;get_user&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;from github import Github

github = Github(&#39;soofaloofa&#39;, &#39;password&#39;)
user = github.get_user()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is equivalent to making a &lt;a href=&#34;https://developer.github.com/v3/#authentication&#34;&gt;basic authentication
request&lt;/a&gt; to get the currently
&lt;a href=&#34;https://developer.github.com/v3/users/#get-the-authenticated-user&#34;&gt;authenticated
user&lt;/a&gt; and
storing the result in a local representation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;bash&#34;&gt;curl -u soofaloofa https://api.github.com/user
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Accessing a repository&lt;/h2&gt;

&lt;p&gt;Now that we have a user we can get a repository for that user by name. To get
the repository for this website we make a request to &lt;a href=&#34;https://developer.github.com/v3/repos/#get&#34;&gt;get a repo by
owner&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;repository = user.get_repo(&#39;soofaloofa.github.io&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Downloading a single file&lt;/h2&gt;

&lt;p&gt;To download a single file from a repository we make a call to &lt;a href=&#34;https://developer.github.com/v3/repos/contents/#get-contents&#34;&gt;get the contents
of a file&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;file_content = repository.get_contents(&#39;README.md&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Referencing commits&lt;/h2&gt;

&lt;p&gt;We have all the building blocks to download a resource from Github. The next
step is to download a resource referenced by a specific commit. The Github API
expects SHA values to reference a commit. To make this a bit more user friendly
we can write a function that will search for a SHA given a git tag or branch
name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;def get_sha_for_tag(repository, tag):
    &amp;quot;&amp;quot;&amp;quot;
    Returns a commit PyGithub object for the specified repository and tag.
    &amp;quot;&amp;quot;&amp;quot;
    branches = repository.get_branches()
    matched_branches = [match for match in branches if match.name == tag]
    if matched_branches:
        return matched_branches[0].commit.sha

    tags = repository.get_tags()
    matched_tags = [match for match in tags if match.name == tag]
    if not matched_tags:
        raise ValueError(&#39;No Tag or Branch exists with that name&#39;)
    return matched_tags[0].commit.sha
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can pass this SHA to the &lt;code&gt;get_contents&lt;/code&gt; function to get a file for that
specific commit.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;sha = get_sha_for_tag(repository, &#39;develop&#39;)
file_content = repository.get_contents(&#39;README.md&#39;, ref=sha)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Putting it all together&lt;/h2&gt;

&lt;p&gt;By putting a bit more polish on this we can easily download entire directories
of code that reference a single tag or branch and copy them to our local
environment. The basic workflow is:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Choose a repository.&lt;/li&gt;
&lt;li&gt;Choose a branch or tag.&lt;/li&gt;
&lt;li&gt;Choose a directory.&lt;/li&gt;
&lt;li&gt;Iteratively download all the files in that directory.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let&amp;rsquo;s make that happen.&lt;/p&gt;

&lt;p&gt;For this code I&amp;rsquo;ll assume that the Github user belongs to a single organization
and that this organization is sharing code between repositories.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;from github import Github
import getpass

username = raw_input(&amp;quot;Github username: &amp;quot;)
password = getpass.getpass(&amp;quot;Github password: &amp;quot;)

github = Github(username, password)
organization = github.get_user().get_orgs()[0]

repository_name = raw_input(&amp;quot;Github repository: &amp;quot;)
repository = organization.get_repo(repository_name)

branch_or_tag_to_download = raw_input(&amp;quot;Branch or tag to download: &amp;quot;)
sha = get_sha_for_tag(repository, branch_or_tag_to_download)

directory_to_download = raw_input(&amp;quot;Directory to download: &amp;quot;)
download_directory(repository, sha, directory_to_download)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This piece of code is fairly simple and relies on a couple of helper functions:
&lt;code&gt;get_sha_for_tag&lt;/code&gt; and &lt;code&gt;download_directory&lt;/code&gt;. &lt;code&gt;get_sha_for_tag&lt;/code&gt; will return the
SHA commit hash given a branch or tag and &lt;code&gt;download_directory&lt;/code&gt; will recursively
download the files in the given directory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;def get_sha_for_tag(repository, tag):
    &amp;quot;&amp;quot;&amp;quot;
    Returns a commit PyGithub object for the specified repository and tag.
    &amp;quot;&amp;quot;&amp;quot;
    branches = repository.get_branches()
    matched_branches = [match for match in branches if match.name == tag]
    if matched_branches:
        return matched_branches[0].commit.sha

    tags = repository.get_tags()
    matched_tags = [match for match in tags if match.name == tag]
    if not matched_tags:
        raise ValueError(&#39;No Tag or Branch exists with that name&#39;)
    return matched_tags[0].commit.sha


def download_directory(repository, sha, server_path):
    &amp;quot;&amp;quot;&amp;quot;
    Download all contents at server_path with commit tag sha in 
    the repository.
    &amp;quot;&amp;quot;&amp;quot;
    contents = repository.get_dir_contents(server_path, ref=sha)

    for content in contents:
        print &amp;quot;Processing %s&amp;quot; % content.path
        if content.type == &#39;dir&#39;:
            download_directory(repository, sha, content.path)
        else:
            try:
                path = content.path
                file_content = repository.get_contents(path, ref=sha)
                file_data = base64.b64decode(file_content.content)
                file_out = open(content.name, &amp;quot;w&amp;quot;)
                file_out.write(file_data)
                file_out.close()
            except (GithubException, IOError) as exc:
                logging.error(&#39;Error processing %s: %s&#39;, content.path, exc)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve been using a variation of this simple script to share code between Github
repositories and appreciate it&amp;rsquo;s flexibility and ease of use. Let me know if you
find it useful!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How to bypass the auto_now property option during an ndb put</title>
      <link>http://sookocheff.com/posts/2014-05-28-how-to-bypass-the-auto-now-property-option-during-an-ndb-put</link>
      <pubDate>Wed, 28 May 2014 05:55:48 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-05-28-how-to-bypass-the-auto-now-property-option-during-an-ndb-put</guid>
      <description>&lt;p&gt;In App Engine the &lt;code&gt;auto_now&lt;/code&gt; option sets a property to the current date/time
whenever the entity is created or updated. This is a great feature for tracking
the time when an entity was last updated. However, sometimes you may want to put
an entity without updating an &lt;code&gt;auto_now&lt;/code&gt; timestamp. This article will show you
how.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s start with a very basic ndb model with an &lt;code&gt;updated&lt;/code&gt; property having
the &lt;code&gt;auto_now&lt;/code&gt; option set to &lt;code&gt;True&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;from google.appengine.ext import ndb

class Article(ndb.model):
    title = ndb.model.StringProperty()
    updated = ndb.model.DateTimeProperty(auto_now=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, let&amp;rsquo;s put the entity to the datastore &lt;em&gt;without updating the timestamp&lt;/em&gt; and
&lt;em&gt;completely bypassing the &lt;code&gt;auto_now&lt;/code&gt; option&lt;/em&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;article = Article(title=&#39;Python versus Ruby&#39;)
article._properties[&#39;updated&#39;]._auto_now = False
article.put()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s pretty simple, but with caveats. Putting the entity using the code above
will store the updated entity in the instance cache (and memcache). If we get
the entity it will be retrieved from the instance cache with the &lt;code&gt;auto_now&lt;/code&gt;
property still set to &lt;code&gt;False&lt;/code&gt;. This can have unwanted side-effects because
subsequent updates to the entity will not trigger the &lt;code&gt;auto_now&lt;/code&gt; functionality.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;article = Article(title=&#39;Python versus Ruby&#39;)
article._properties[&#39;updated&#39;]._auto_now = False
key = article.put() # Put the entity with the auto_now option set to False

article = key.get() # Get the entity from instance cache
article.title = &#39;Python versus Go&#39;
article.put() # Put the entity with the auto_now option *still* set to False
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can set the &lt;code&gt;auto_now&lt;/code&gt; option to &lt;code&gt;True&lt;/code&gt; again to re-enable the functionality.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;python&#34;&gt;article = Article(title=&#39;Python versus Ruby&#39;)
article._properties[&#39;updated&#39;]._auto_now = False
key = article.put()

article._properties[&#39;updated&#39;]._auto_now = True
article = key.get() # Get the entity from instance cache
article.title = &#39;Python versus Go&#39;
article.put() # Puts the entity with the auto_now option set to True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For more information on ndb caching &lt;a href=&#34;https://developers.google.com/appengine/docs/python/ndb/cache&#34;&gt;refer to the
documentation&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine MapReduce API - Part 5: Using Combiners to Reduce Data Throughput</title>
      <link>http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput</link>
      <pubDate>Tue, 20 May 2014 08:54:12 UTC</pubDate>
      <author>Kevin Sookocheff</author>
      <guid>http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput</guid>
      <description>

&lt;h2 id=&#34;toc_0&#34;&gt;MapReduce API Series&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-15-app-engine-mapreduce-api-part-1-the-basics/&#34;&gt;Part 1: The Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-22-app-engine-mapreduce-api-part-2-running-a-mapreduce-job-using-mapreduceyaml/&#34;&gt;Part 2: Running a MapReduce Job Using mapreduce.yaml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;Part 3: Programmatic MapReduce using Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-13-app-engine-mapreduce-api-part-4-combining-sequential-mapreduce-jobs/&#34;&gt;Part 4: Combining Sequential MapReduce Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput/&#34;&gt;Part 5: Using Combiners to Reduce Data Throughput&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So far we&amp;rsquo;ve looked at using MapReduce pipelines to perform calculations over
large data sets and combined multiple pipelines in succession. In this article
we will look at how to reduce the amount of data transfer by using a combiner.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;What is a combiner?&lt;/h2&gt;

&lt;p&gt;A combiner is a function that takes the output of a series of map calls as input and outputs a value of the same format to be processed by the reducer. The combiner is run just before the output of the mapper is written to disk. In fact, the combiner may not be run at all if the data can reside completely in memory and so your algorithm must be able to complete with our without the combiner. By reducing the amount of data that needs to be written to disk you can increase performance of the reduce stage.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s look at an example that uses a combiner to reduce data throughput. To drive this discussion we will use an example that counts the number of occurrences of a character in a string. We originally looked at this example &lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;here&lt;/a&gt;. In this version we will only include the character or characters that occur the most. The operation will work like this: the mapper function will count the occurrence of each character in a string. The combiner will take these (key, value) pairs and output only the character or characters that appear the most. Finally, the reducer will sum those values to find our result. This contrived problem will provide a working example of a combiner.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with the MapReduce job from our previous example.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;quot;&amp;quot;&amp;quot;
app.pipelines
&amp;quot;&amp;quot;&amp;quot;
import collections

from mapreduce.lib import pipeline
from mapreduce import mapreduce_pipeline

###
### MapReduce Pipeline
###
def character_count_map(random_string):
    &amp;quot;&amp;quot;&amp;quot; yield the number of occurrences of each character in random_string. &amp;quot;&amp;quot;&amp;quot;
    counter = collections.Counter(random_string)
    for character in counter.elements():
        yield (character, counter[character])

def character_count_reduce(key, values):
    &amp;quot;&amp;quot;&amp;quot; sum the number of characters found for the key. &amp;quot;&amp;quot;&amp;quot;
    yield (key, sum([int(i) for i in values]))

class CountCharactersPipeline(pipeline.Pipeline):
    &amp;quot;&amp;quot;&amp;quot; Count the number of occurrences of a character in a set of strings. &amp;quot;&amp;quot;&amp;quot;

    def run(self, *args, **kwargs):
        &amp;quot;&amp;quot;&amp;quot; run &amp;quot;&amp;quot;&amp;quot;
        mapper_params = {
            &amp;quot;count&amp;quot;: 100,
            &amp;quot;string_length&amp;quot;: 20,
        }
        reducer_params = {
            &amp;quot;mime_type&amp;quot;: &amp;quot;text/plain&amp;quot;
        }
        output = yield mapreduce_pipeline.MapreducePipeline(
            &amp;quot;character_count&amp;quot;,
            mapper_spec=&amp;quot;app.pipelines.character_count_map&amp;quot;,
            mapper_params=mapper_params,
            reducer_spec=&amp;quot;app.pipelines.character_count_reduce&amp;quot;,
            reducer_params=reducer_params,
            input_reader_spec=&amp;quot;mapreduce.input_readers.RandomStringInputReader&amp;quot;,
            output_writer_spec=&amp;quot;mapreduce.output_writers.BlobstoreOutputWriter&amp;quot;,
            shards=16)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given this base we add a combiner step to the &lt;code&gt;MapreducePipeline&lt;/code&gt; by passing the &lt;code&gt;combiner_spec&lt;/code&gt; argument to the initialization.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       output = yield mapreduce_pipeline.MapreducePipeline(
            &amp;quot;character_count&amp;quot;,
            mapper_spec=&amp;quot;app.pipelines.character_count_map&amp;quot;,
            mapper_params=mapper_params,
            reducer_spec=&amp;quot;app.pipelines.character_count_reduce&amp;quot;,
            reducer_params=reducer_params,
            combiner_spec=&amp;quot;app.pipelines.character_count_combine&amp;quot;,
            input_reader_spec=&amp;quot;mapreduce.input_readers.RandomStringInputReader&amp;quot;,
            output_writer_spec=&amp;quot;mapreduce.output_writers.BlobstoreOutputWriter&amp;quot;,
            shards=16)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our combine function accepts a few parameters the key, a list of values for that key and a list of previously combined results. The combiner function yields combined values that might be processed by another combiner call and that will eventually end up in the reducer function.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s write our simple combiner function. We yield only a value instead of a &lt;code&gt;(key, value)&lt;/code&gt; tuple because the key is assumed to stay the same.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def character_count_combine(key, values, previously_combined_values):
    &amp;quot;&amp;quot;&amp;quot; emit the maximum value in values and previously_combined_values &amp;quot;&amp;quot;&amp;quot;
    yield max(values + previously_combined_values)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our combiner function is not guaranteed to run so we need to update our reduce function to take the maximum of the list of values as well.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def character_count_reduce(key, values):
    &amp;quot;&amp;quot;&amp;quot; sum the number of characters found for the key. &amp;quot;&amp;quot;&amp;quot;
    yield (key, max(values))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This gives us our final pipeline using map, reduce and combine.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;###
### MapReduce Pipeline
###
def character_count_map(random_string):
    &amp;quot;&amp;quot;&amp;quot; yield the number of occurrences of each character in random_string. &amp;quot;&amp;quot;&amp;quot;
    counter = collections.Counter(random_string)
    for character in counter.elements():
        yield (character, counter[character])

def character_count_reduce(key, values):
    &amp;quot;&amp;quot;&amp;quot; sum the number of characters found for the key. &amp;quot;&amp;quot;&amp;quot;
    yield (key, max(values))

def character_count_combine(key, values, previously_combined_values):
    &amp;quot;&amp;quot;&amp;quot; emit the maximum value in values and previously_combined_values &amp;quot;&amp;quot;&amp;quot;
    yield max(values + previously_combined_values)

class CountCharactersPipeline(pipeline.Pipeline):
    &amp;quot;&amp;quot;&amp;quot; Count the number of occurrences of a character in a set of strings. &amp;quot;&amp;quot;&amp;quot;

    def run(self, *args, **kwargs):
        &amp;quot;&amp;quot;&amp;quot; run &amp;quot;&amp;quot;&amp;quot;
        mapper_params = {
            &amp;quot;count&amp;quot;: 100,
            &amp;quot;string_length&amp;quot;: 20,
        }
        reducer_params = {
            &amp;quot;mime_type&amp;quot;: &amp;quot;text/plain&amp;quot;
        }
        output = yield mapreduce_pipeline.MapreducePipeline(
            &amp;quot;character_count&amp;quot;,
            mapper_spec=&amp;quot;app.pipelines.character_count_map&amp;quot;,
            mapper_params=mapper_params,
            reducer_spec=&amp;quot;app.pipelines.character_count_reduce&amp;quot;,
            reducer_params=reducer_params,
            combiner_spec=&amp;quot;app.pipelines.character_count_combine&amp;quot;,
            input_reader_spec=&amp;quot;mapreduce.input_readers.RandomStringInputReader&amp;quot;,
            output_writer_spec=&amp;quot;mapreduce.output_writers.BlobstoreOutputWriter&amp;quot;,
            shards=16)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
