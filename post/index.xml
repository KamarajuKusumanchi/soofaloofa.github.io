<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Sookocheff</title>
    <link>http://sookocheff.com/post/</link>
    <language>en-us</language>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <lastBuildDate>Mon, 14 Sep 2015 20:26:21 CST</lastBuildDate>
    
    <item>
      <title>Beginning Docker</title>
      <link>http://sookocheff.com/post/docker/beginning-docker/</link>
      <pubDate>Mon, 14 Sep 2015 20:26:21 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/docker/beginning-docker/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;m writing this article as a means of tracking commonly used docker commands in
a place where I won&amp;rsquo;t forget them. If you find it useful or have additional
suggestions let me know in the comments.&lt;/p&gt;

&lt;h2 id=&#34;docker-machine:c1a9119712c687d2f7cd6051cc704390&#34;&gt;Docker Machine&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# create a new virtual machine using virtualbox, name it default
&amp;gt; docker-machine create --driver virtualbox default

# list available VMs
&amp;gt; docker-machine ls 

# list available VMs
&amp;gt; docker-machine ls 

# list environment variables needed connect to default VM
&amp;gt; docker-machine env default

# source environment variables into current shell session
&amp;gt; eval $(docker-machine env default)

# SSH into the VM
&amp;gt; docker-machine ssh default

# check a machine&#39;s status
&amp;gt; docker-machine status default

# start a machine
&amp;gt; docker-machine start default

# stop a machine
&amp;gt; docker-machine stop default

# remove a machine
&amp;gt; docker-machine rm default
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;docker:c1a9119712c687d2f7cd6051cc704390&#34;&gt;Docker&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# create a container
&amp;gt; docker create redis

# start a container
&amp;gt; docker start d8c20e1b7e98

# create and start a container
&amp;gt; docker run d8c20e1b7e98

# pause a container (it won&#39;t be scheduled to execute tasks)
&amp;gt; docker pause d8c20e1b7e98

# unpause a container (resume scheduling)
&amp;gt; docker unpause d8c20e1b7e98

# set the port mappings between container and host
&amp;gt; docker run -p 6379:6379 redis

# auto-restart a container
&amp;gt; docker run --restart=always redis

# auto-restart up to three times on failure
&amp;gt; docker run --restart=on-failure:3 redis

# list all running containers
&amp;gt; docker ps

# list *all* containers
&amp;gt; docker ps -a

# run a command (ex: bash)
&amp;gt; docker run redis bash

# run an interactive container (t=&amp;gt;connect TTY, i=&amp;gt;interactive session)
&amp;gt; docker run -ti redis bash

# inspect a container
&amp;gt; docker inspect d8c20e1b7e98

# delete a container
&amp;gt; docker rm d8c20e1b7e98

# delete *all* containers
&amp;gt; docker rm $(docker ps -a -q)

# list all images
&amp;gt; docker images

# delete an image
&amp;gt; docker rmi redis:latest

# delete all images
&amp;gt; docker rmi $(docker images -q)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Review of the Coursera Data Science Specialization</title>
      <link>http://sookocheff.com/post/datascience/datasciencespecialization/</link>
      <pubDate>Thu, 10 Sep 2015 06:29:12 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/datascience/datasciencespecialization/</guid>
      <description>

&lt;h1 id=&#34;a-review-of-the-coursera-data-science-specialization:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;A Review of the Coursera Data Science Specialization&lt;/h1&gt;

&lt;p&gt;I recently completed the 10th and final course in the Data Science Specialization offered by Coursera in conjunction with Johns Hopkins University. My background is as a computer scientist and programmer looking to learn more about statistical analysis and machine learning — I have always had an interest in data analysis and machine learning but never actually studied it. I used the Data Science Specialization acted as a starting point to learn more about the field and become familiar with typical problems and solutions that data scientists encounter in the field. This article describes my experience with the specialization and answers the question of whether or not the it is worth the time.&lt;/p&gt;

&lt;h2 id=&#34;the-data-scientist-s-toolbox:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;The Data Scientist&amp;rsquo;s Toolbox&lt;/h2&gt;

&lt;p&gt;The specialization opens with The Data Scientist&amp;rsquo;s Toolbox, providing a broad overview of the specialization itself and of the tools and technologies that will be covered throughout the course. You will learn the basics of Git, GitHub, Markdown and R. With tutorials on how to install RStudio and R packages.&lt;/p&gt;

&lt;p&gt;The Data Scientist&amp;rsquo;s Toolbox is basic to the point of being remedial. A large portion of the course is spent reviewing what the upcoming courses will cover. It seems disingenuous to charge $50 for a course that explains what additional courses will cover. The concrete material in this course could easily be relegated to pre-requisites of other courses. If you complete a basic tutorial on Git and Markdown and know how to install software you will have completed all the requirements for this course.&lt;/p&gt;

&lt;h3 id=&#34;verdict:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Skip. If you know absolutely nothing about software development this course may be useful. Otherwise, skip it.&lt;/p&gt;

&lt;h2 id=&#34;r-programming:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;R Programming&lt;/h2&gt;

&lt;p&gt;The R Programming course provides an overview of R as a programming language. It covers basic R syntax and data structures and offers an opportunity to practice writing basic R functions.&lt;/p&gt;

&lt;p&gt;Given background in other programming languages the R Programming course will not provide you with much benefit. I would recommend following some R programming introductory tutorials at your own pace. If, however, you do not have much programming experience this course will explain the fundamentals and teach you to think programmatically. This is one of the few courses that comes with a recommended textbook. Unfortunately, the textbook covers advanced R material and would only be useful after becoming familiar with R programming. The pace of the book does not match the expectations of the course.&lt;/p&gt;

&lt;h3 id=&#34;verdict-1:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Take.  There is enough R specific material to make this course worth while. Just don&amp;rsquo;t expect revelations — you will not be an expert in R after this course. An additional textbook would be wonderful.&lt;/p&gt;

&lt;h2 id=&#34;getting-and-cleaning-data:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Getting and Cleaning Data&lt;/h2&gt;

&lt;p&gt;Getting and Cleaning Data covers reading files into R from a variety of sources: CSV, MySQL, HDFS, among others. You will learn how to reshape data using R and how to generate summary statistics of your data.&lt;/p&gt;

&lt;p&gt;While getting and cleaning data is a necessary process in data analysis, the material covered in this course is basic and disjointed. I would have vastly preferred covering how to load one or two different file types and spent more time on manipulating and preparing data with R.&lt;/p&gt;

&lt;h3 id=&#34;verdict-2:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Skip. This material can easily be learned when needed using specific Google searches on how to load a particular file with R. Consider taking this course if you need additional practice in R.&lt;/p&gt;

&lt;h2 id=&#34;exploratory-data-analysis:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Exploratory Data Analysis&lt;/h2&gt;

&lt;p&gt;The Exploratory Analysis course covers plotting in R using the base plotting system, the lattice package, and ggplot2. It then moves on to explain k-means, dimensionality reduction and principal components analysis.&lt;/p&gt;

&lt;p&gt;The material on plotting is great. I only wish that the coverage of the lattice package were removed and more time dedicated to ggplot. I&amp;rsquo;ve yet to see any lattice plots in the real world — it seems like ggplot is the de facto standard in this area and should be covered more thoroughly. The course provides a cursory overview of k-means and principal components analysis as a way to summarize data. This material could have used a lot more explanation and motivation.&lt;/p&gt;

&lt;h3 id=&#34;verdict-3:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Take. ggplot is a must and this course will get you started on the right path to plotting. The statistical material on k-means and principal components analysis deserved more in-depth treatment and could almost be a course in itself. I wished for more material in these areas.&lt;/p&gt;

&lt;h2 id=&#34;reproducible-research:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Reproducible Research&lt;/h2&gt;

&lt;p&gt;Reproducible research provides the motivation for making research reproducible. It then follows with coverage of R specific technologies that can aid in producing reproducible research: knitr, RPubs, and slidify.&lt;/p&gt;

&lt;p&gt;I have mixed feelings about this course. It&amp;rsquo;s great that coverage of reproducible research is included in the specialization but four weeks is too much for this particular topic. Most of the course seems like filler and following a quick tutorial on knitr would provide much the same benefit as taking this course. I also do not see the benefit in covering both RPubs and slidify when they do basically the same thing. I would much prefer seeing more in-depth coverage of one technology — anything learned in one technology could be easily applied to the other when it is needed.&lt;/p&gt;

&lt;h3 id=&#34;verdict-4:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Skip. The coverage of each technology is cursory at best. Reading the documentation of knitr would provide more detail for less time and effort than taking this course. Not recommended.&lt;/p&gt;

&lt;h2 id=&#34;statistical-inference:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Statistical Inference&lt;/h2&gt;

&lt;p&gt;The statistical inference course covers they key concepts of statistical inference: probability, confidence intervals, hypothesis testing and common statistical distributions. The course is math heavy, covering the theory behind each statistical method before providing R code for using the method in practice.&lt;/p&gt;

&lt;p&gt;The course is a jarring departure from the rest of the specialization so far. Previous courses have covered practical considerations and mostly focussed on becoming proficient with R and the RStudio environment. The mathematical treatment of statistics and statistical models can be overwhelming at times and the videos are unevenly paced with complex topics being covered with only one or two slides. The course highlights one of the key failings of the specialization: lack of supplementary material. I would have loved a companion text and problem set to help work though the details.&lt;/p&gt;

&lt;h3 id=&#34;verdict-5:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Skip. The material in this course is fundamental. Unfortunately you will need a better source to learn it from. It&amp;rsquo;s a shame that so much material is crammed into this four week course with no additional material to draw from. I would recommend picking up a basic statistics book rather than taking this course.&lt;/p&gt;

&lt;h2 id=&#34;regression-models:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Regression Models&lt;/h2&gt;

&lt;p&gt;Regression models continues the discussion of statistical methods, this time focussing on linear regression, log-linear regression and how to interpret the results of a regression model using residuals.&lt;/p&gt;

&lt;p&gt;The regression models course falls victim to the same shortcomings as the statistical inference course. Namely, the theoretical nature of the course without supplementary resources makes it difficult to fully dive into the technical details. One or two slides and a video as a format for learning complex material is unrealistic.&lt;/p&gt;

&lt;h3 id=&#34;verdict-6:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Skip. This course attempts to cover a very broad range of material in a short time frame. Unfortunately, not having a supplementary text to work from means that the coverage of the material is spotty and incomplete.&lt;/p&gt;

&lt;h2 id=&#34;practical-machine-learning:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Practical Machine Learning&lt;/h2&gt;

&lt;p&gt;The practical machine learning course eases off from the theoretical underpinnings of prediction to introduce the caret package in R and walks through some typical machine learning algorithms. It starts with a discussion of what prediction is and how to measure if a prediction model is accurate or not, then dives in to linear regression, prediction trees, and random forests.&lt;/p&gt;

&lt;p&gt;The caret package is becoming the de facto standard interface for machine learning in R. As such, the course has some great material focusing on the practical aspects of the caret package. The course also provides a discussion of how to evaluate a prediction model for accuracy and how to compare models against one another. I wish there was more of this material as it seems like such a fundamental problem in machine learning.&lt;/p&gt;

&lt;h3 id=&#34;verdict-7:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Take. This course provides a solid introduction to the caret package and should guide the student to further areas of study. You will not be an expert in machine learning after taking this course but it should give you a starting point to learn additional details whenever they are needed.&lt;/p&gt;

&lt;h2 id=&#34;developing-data-products:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Developing Data Products&lt;/h2&gt;

&lt;p&gt;This course focuses on using R to create presentations and web apps as a means of showcasing your models. This course covers Shiny, RStudio presenter and slidify, among other technologies. Each technology for presenting your work is introduced along with a minimal tutorial. Once that tutorial is complete, a new technology is introduced.&lt;/p&gt;

&lt;p&gt;I wish this course focussed more on one or two technologies and expanded more deeply on how they work and what can be accomplished with them. As it stands, it&amp;rsquo;s hard to recommend a course that simply re-implements existing documentation for each technology that could easily be gotten in other, more complete, forms from the respective technologies websites. The course instructor even admits that most of the material is the exact examples from the technologies documentation.&lt;/p&gt;

&lt;h3 id=&#34;verdict-8:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Skip. Just read the tutorial for the technologies that interest you.&lt;/p&gt;

&lt;h2 id=&#34;data-science-capstone:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Data Science Capstone&lt;/h2&gt;

&lt;p&gt;The capstone project involves creating a Shiny web application that, given a sequence of input words, predicts the next word in the sentence. This project requires some understanding of NLP, data management, Shiny, and RPubs.&lt;/p&gt;

&lt;p&gt;The project provided a good experience of what being a data scientist entails. It necessitated a lot of research, data mangling, and performance tuning that are common in the field. The end product is useful and the skills required to make it run well are novel and useful. My only issue with the project was that there was little use of statistical methods.&lt;/p&gt;

&lt;h3 id=&#34;verdict-9:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Verdict&lt;/h3&gt;

&lt;p&gt;Take. The capstone brings together a lot of elements of the course and ties it all together. I feel like I learned a lot about data science simply by taking the capstone project.&lt;/p&gt;

&lt;h2 id=&#34;summary:693c3509f98d11a9bb6c7b10600e6ef2&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The individual reviews of the courses follow a definite trend: uneven coverage of material and technologies. That trend continues for the specialization as a whole. I don&amp;rsquo;t think anyone doing actual data science would believe that a topic such as statistical inference should be given the same weight as a topic such as git. Although git is definitely something that is valuable for a data scientist, it is not essential to the field. As such, the course would be a great introduction to data science from the perspective of a statistician. It would teach the statistician basic programming techniques, version control, and how to deploy your code via a Shiny application.&lt;/p&gt;

&lt;p&gt;Another failing of the course is the lack of supplementary material. Not everyone learns best from watching videos and if you need to go back and review material at a later date then you need to either find the correct place in the video or look at only a few bullet points in a set of slides. The specialization begs for a text book for at least a few of the courses and could definitely use some supplementary material for almost every course. It seems that the instructors agree with this assessment as they are slowly publishing books through Leanpub that supplement the Coursera course. This is great news for new students going through the material.&lt;/p&gt;

&lt;p&gt;As it stands, the specialization does provide a broad overview of the field of data science and I do feel like I can now dive in to particular areas of interest with more confidence. Overall, I&amp;rsquo;m glad that I took the entire specialization but looking back I believe I could have skipped a few of the courses and focussed more on my areas of interest from the beginning. If you wish to maximize the value for the time spent in this specialization, consider cherry-picking the particular courses that interest you rather than taking the entire specialization.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Counting N-Grams with Cloud Dataflow</title>
      <link>http://sookocheff.com/post/nlp/counting-n-grams-with-cloud-dataflow/</link>
      <pubDate>Wed, 05 Aug 2015 05:41:06 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/nlp/counting-n-grams-with-cloud-dataflow/</guid>
      <description>

&lt;p&gt;Counting &lt;a href=&#34;http://sookocheff.com/post/nlp/n-gram-modeling/&#34;&gt;n-grams&lt;/a&gt; is a common
pre-processing step for computing sentence and word probabilities over a corpus.
Thankfully, this task is &lt;a href=&#34;https://en.wikipedia.org/wiki/Embarrassingly_parallel&#34;&gt;embarrassingly
parallel&lt;/a&gt; and is a
natural fit for distributed processing frameworks like &lt;a href=&#34;https://cloud.google.com/dataflow/&#34;&gt;Cloud
Dataflow&lt;/a&gt;. This article provides an
implementation of n-gram counting using Cloud Dataflow that is able to
efficiently compute n-grams in parallel over massive datasets.&lt;/p&gt;

&lt;h2 id=&#34;the-algorithm:373238c81a8bcfdf6cdd9d4aa8562076&#34;&gt;The Algorithm&lt;/h2&gt;

&lt;p&gt;Cloud Dataflow uses a programming abstraction called &lt;code&gt;PCollections&lt;/code&gt; which are
collections of data that can be operated on in parallel (Parallel Collections).
When programming for Cloud Dataflow you treat each operation as a transformation
of a parallel collection that returns another parallel collection for further
processing. This style of development is similar to the traditional Unix
philosophy of piping the output of one command to another for further
processing.&lt;/p&gt;

&lt;p&gt;An outline of the algorithm for counting n-grams is presented in the following
figure. The first stage of our dataflow pipeline is reading all lines of our
input. We then proceed to extract n-grams from each individual line, outputting
the results as a &lt;code&gt;PCollection&lt;/code&gt;. We then count the n-grams and take the top
n-grams in our dataset. Lastly, the results are output to a file.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/dataflow-graph.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/dataflow-graph.png&#34; alt=&#34;Dataflow Graph&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;As a concrete example, we can represent the same algorithm as transformations on
a text file. In this example we will count the occurrence of bigrams.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;I am Sam. I am Kevin.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, the file is read as input and bigrams are extracted.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(&#39;I&#39;, &#39;am&#39;)
(&#39;am&#39;, &#39;Sam.&#39;)
(&#39;Sam.&#39;, &#39;I&#39;)
(&#39;I&#39;, &#39;am&#39;)
(&#39;am&#39;, &#39;Kevin.&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next, for each element, we count the number of occurrences. This happens in two
stages. First, we group all elements by key. This has the effect of combining
all tuples with the same value to be on one line.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(&#39;I&#39;, &#39;am&#39;), (&#39;I&#39;, &#39;am&#39;)
(&#39;am&#39;, &#39;Sam.&#39;)
(&#39;Sam.&#39;, &#39;I&#39;)
(&#39;am&#39;, &#39;Kevin.&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;From here, we simply count the number of elements in each group.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;(&#39;I&#39;, &#39;am&#39;), 2
(&#39;am&#39;, &#39;Sam.&#39;), 1
(&#39;Sam.&#39;, &#39;I&#39;), 1
(&#39;am&#39;, &#39;Kevin.&#39;), 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In Cloud Dataflow, the previous operations are combined into the
&lt;code&gt;Count.PerElement&lt;/code&gt; operation that counts the number of times an element occurs.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/count-per-element.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/count-per-element.png&#34; alt=&#34;Count.PerElement&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Once all the elements are grouped and counted, we can extract the top &lt;code&gt;x&lt;/code&gt;
elements. To do this, we need to be able to combine elements across machines and
across files. Dataflow provides the &lt;code&gt;Combine.PerKey&lt;/code&gt; operation for this purpose.
This operation merges elements from multiple files into a single file. We can
then take the top &lt;code&gt;x&lt;/code&gt; results to view the top &lt;code&gt;x&lt;/code&gt; bigrams. Dataflow provides a
convenience function &lt;code&gt;Top.Globally&lt;/code&gt; to extract the top &lt;code&gt;x&lt;/code&gt; results from a
&lt;code&gt;PCollection&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/top-globally.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/counting-n-grams-with-cloud-dataflow/top-globally.png&#34; alt=&#34;Top.Globally&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;show-me-the-code:373238c81a8bcfdf6cdd9d4aa8562076&#34;&gt;Show Me The Code&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s go ahead and express our algorithm using Cloud Dataflow. The algorithm is
expressed in two parts. First, extracting n-grams from a block of text. This is
a simple transformation that takes a block of text as input and repeatedly
outputs individual n-grams. This list of n-grams serves as our initial
&lt;code&gt;PCollection&lt;/code&gt; for the rest of the algorithm.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/**
* This DoFn tokenizes lines of text into individual ngrams;
* we pass it to a ParDo in the pipeline.
*/
static class ExtractNGramsFn extends DoFn&amp;lt;String, String&amp;gt; {
  private static final long serialVersionUID = 0;
  
  private Integer n;
  
  public ExtractNGramsFn(Integer n) {
    this.n = n;
  }
  
  @Override
  public void processElement(ProcessContext c) {
    // Split the line into words.
    String[] words = c.element().split(&amp;quot;\\s+&amp;quot;);
  
    // Group into ngrams
    List&amp;lt;String&amp;gt; ngrams = new ArrayList&amp;lt;String&amp;gt;();
    for (int i = 0; i &amp;lt;= words.length-this.n; i++) {
      StringBuilder ngram = new StringBuilder();
      for (int j = 0; j &amp;lt; this.n; j++) {
        if (j &amp;gt; 0) {
          ngram.append(&amp;quot;\t&amp;quot;);
        }
        ngram.append(words[i+j]);
      }
      ngrams.add(ngram.toString());
    }
  
    // Output each ngram encountered into the output PCollection.
    for (String ngram : ngrams) {
      if (!ngram.isEmpty()) {
        c.output(ngram);
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Second, we use the &lt;code&gt;PCollection&lt;/code&gt; of all n-grams as input to a transform that outputs
the list of most frequently encountered n-grams in the corpus.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;/**
* A PTransform that converts a PCollection containing lines of text into a PCollection of
* word counts.
*/
public static class CountNGrams
  extends PTransform&amp;lt;PCollection&amp;lt;String&amp;gt;, PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt;&amp;gt; {
  
  private static final long serialVersionUID = 0;
  
  private Integer n;
  private Integer top;
  
  public CountNGrams(Integer n) {
    this.n = n;
    this.top = new Integer(100);
  }
  
  public CountNGrams(Integer n, Integer top) {
    this.n = n;
    this.top = top;
  }
  
  @Override
  public PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt; apply(PCollection&amp;lt;String&amp;gt; lines) {
  
    // Convert lines of text into individual ngrams.
    PCollection&amp;lt;String&amp;gt; ngrams = lines.apply(
        ParDo.of(new ExtractNGramsFn(this.n)));
  
    // Count the number of times each ngram occurs.
    PCollection&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt; ngramCounts =
        ngrams.apply(Count.&amp;lt;String&amp;gt;perElement());
  
    // Find the top ngrams in the corpus.
    PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt; topNgrams = 
        ngramCounts.apply(Top.of(this.top, new SerializableComparator&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;() {
                  private static final long serialVersionUID = 0;
  
                  @Override
                  public int compare(KV&amp;lt;String, Long&amp;gt; o1, KV&amp;lt;String, Long&amp;gt; o2) {
                    return Long.compare(o1.getValue(), o2.getValue());
                  }
                }).withoutDefaults());
    
    return topNgrams;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;full-source-code:373238c81a8bcfdf6cdd9d4aa8562076&#34;&gt;Full Source Code&lt;/h2&gt;

&lt;p&gt;The rest of the code is boilerplate to setup the pipeline and accept user input.
Feel free to use this code as a basis for your own pipelines.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;package com.sookocheff.cloud.dataflow.examples;

import com.google.cloud.dataflow.sdk.Pipeline;
import com.google.cloud.dataflow.sdk.io.TextIO;
import com.google.cloud.dataflow.sdk.options.DataflowPipelineOptions;
import com.google.cloud.dataflow.sdk.options.Default;
import com.google.cloud.dataflow.sdk.options.DefaultValueFactory;
import com.google.cloud.dataflow.sdk.options.Description;
import com.google.cloud.dataflow.sdk.options.PipelineOptions;
import com.google.cloud.dataflow.sdk.options.PipelineOptionsFactory;
import com.google.cloud.dataflow.sdk.transforms.Aggregator;
import com.google.cloud.dataflow.sdk.transforms.Count;
import com.google.cloud.dataflow.sdk.transforms.DoFn;
import com.google.cloud.dataflow.sdk.transforms.PTransform;
import com.google.cloud.dataflow.sdk.transforms.ParDo;
import com.google.cloud.dataflow.sdk.transforms.Sum;
import com.google.cloud.dataflow.sdk.transforms.Top;
import com.google.cloud.dataflow.sdk.transforms.SerializableComparator;
import com.google.cloud.dataflow.sdk.util.gcsfs.GcsPath;
import com.google.cloud.dataflow.sdk.values.KV;
import com.google.cloud.dataflow.sdk.values.PCollection;

import java.io.IOException;
import java.util.*;


/**
 * Count N-Grams.
 */
public class NGramCount {

  /**
   * This DoFn tokenizes lines of text into individual ngrams; we pass it to a ParDo in the
   * pipeline.
   */
  static class ExtractNGramsFn extends DoFn&amp;lt;String, String&amp;gt; {
    private static final long serialVersionUID = 0;

    private Integer n;

    public ExtractNGramsFn(Integer n) {
      this.n = n;
    }

    private final Aggregator&amp;lt;Long, Long&amp;gt; ngramCount =
        createAggregator(&amp;quot;ngramCount&amp;quot;, new Sum.SumLongFn());

    @Override
    public void processElement(ProcessContext c) {
      // Split the line into words (splits at any whitespace character, grouping
      // whitespace together).
      String[] words = c.element().split(&amp;quot;\\s+&amp;quot;);

      // Group into ngrams
      List&amp;lt;String&amp;gt; ngrams = new ArrayList&amp;lt;String&amp;gt;();
      for (int i = 0; i &amp;lt;= words.length-this.n; i++) {
        StringBuilder ngram = new StringBuilder();
        for (int j = 0; j &amp;lt; this.n; j++) {
          if (j &amp;gt; 0) {
            ngram.append(&amp;quot;\t&amp;quot;);
          }
          ngram.append(words[i+j]);
        }
        ngrams.add(ngram.toString());
      }

      // Output each ngram encountered into the output PCollection.
      for (String ngram : ngrams) {
        if (!ngram.isEmpty()) {
          ngramCount.addValue(1L);
          c.output(ngram);
        }
      }
    }
  }

  /** A DoFn that converts an NGram and Count into a printable string. */
  public static class FormatAsTextFn extends DoFn&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;, String&amp;gt; {
    private static final long serialVersionUID = 0;

    @Override
    public void processElement(ProcessContext c) {

      for (KV&amp;lt;String, Long&amp;gt; item : c.element()) {
        String ngram = item.getKey();
        long count = item.getValue();
        c.output(ngram + &amp;quot;\t&amp;quot; + count);
      }
    }
  }

  /**
   * A PTransform that converts a PCollection containing lines of text into a PCollection of
   * word counts.
   */
  public static class CountNGrams
    extends PTransform&amp;lt;PCollection&amp;lt;String&amp;gt;, PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt;&amp;gt; {

    private static final long serialVersionUID = 0;

    private Integer n;
    private Integer top;

    public CountNGrams(Integer n) {
      this.n = n;
      this.top = new Integer(100);
    }

    public CountNGrams(Integer n, Integer top) {
      this.n = n;
      this.top = top;
    }

    @Override
    public PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt; apply(PCollection&amp;lt;String&amp;gt; lines) {

      // Convert lines of text into individual ngrams.
      PCollection&amp;lt;String&amp;gt; ngrams = lines.apply(
          ParDo.of(new ExtractNGramsFn(this.n)));

      // Count the number of times each ngram occurs.
      PCollection&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt; ngramCounts =
          ngrams.apply(Count.&amp;lt;String&amp;gt;perElement());

      // Find the top ngrams in the corpus
      PCollection&amp;lt;List&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;&amp;gt; topNgrams = 
          ngramCounts.apply(Top.of(this.top, new SerializableComparator&amp;lt;KV&amp;lt;String, Long&amp;gt;&amp;gt;() {
                    private static final long serialVersionUID = 0;

                    @Override
                    public int compare(KV&amp;lt;String, Long&amp;gt; o1, KV&amp;lt;String, Long&amp;gt; o2) {
                      return Long.compare(o1.getValue(), o2.getValue());
                    }
                  }).withoutDefaults());
      
      return topNgrams;
    }
  }

  /**
   * Options supported by {@link NGramCount}.
   */
  public static interface NGramCountOptions extends PipelineOptions {
    @Description(&amp;quot;Number of n-grams to model.&amp;quot;)
    @Default.Integer(2)
    Integer getN();
    void setN(Integer value);

    @Description(&amp;quot;Number top n-gram counts to return.&amp;quot;)
    @Default.Integer(100)
    Integer getTop();
    void setTop(Integer value);

    @Description(&amp;quot;Path of the file to read from.&amp;quot;)
    @Default.String(&amp;quot;gs://dataflow-samples/shakespeare/kinglear.txt&amp;quot;)
    String getInputFile();
    void setInputFile(String value);

    @Description(&amp;quot;Path of the file to write to.&amp;quot;)
    @Default.InstanceFactory(OutputFactory.class)
    String getOutput();
    void setOutput(String value);

    /**
     * Returns gs://${STAGING_LOCATION}/&amp;quot;counts.txt&amp;quot; as the default destination.
     */
    public static class OutputFactory implements DefaultValueFactory&amp;lt;String&amp;gt; {
      @Override
      public String create(PipelineOptions options) {
        DataflowPipelineOptions dataflowOptions = options.as(DataflowPipelineOptions.class);
        if (dataflowOptions.getStagingLocation() != null) {
          return GcsPath.fromUri(dataflowOptions.getStagingLocation())
              .resolve(&amp;quot;counts.txt&amp;quot;).toString();
        } else {
          throw new IllegalArgumentException(&amp;quot;Must specify --output or --stagingLocation&amp;quot;);
        }
      }
    }

  }

  public static void main(String[] args) throws IOException {
    NGramCountOptions options = PipelineOptionsFactory.fromArgs(args).withValidation()
      .as(NGramCountOptions.class);
    Pipeline p = Pipeline.create(options);

    p.apply(TextIO.Read.named(&amp;quot;ReadLines&amp;quot;).from(options.getInputFile()))
     .apply(new CountNGrams(options.getN(), options.getTop()))
     .apply(ParDo.of(new FormatAsTextFn()))
     .apply(TextIO.Write.named(&amp;quot;WriteCounts&amp;quot;).to(options.getOutput()));

    p.run();
  }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>N-gram Modeling With Markov Chains</title>
      <link>http://sookocheff.com/post/nlp/ngram-modeling-with-markov-chains/</link>
      <pubDate>Fri, 31 Jul 2015 06:23:43 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/nlp/ngram-modeling-with-markov-chains/</guid>
      <description>

&lt;p&gt;A common method of reducing the complexity of n-gram modeling is using the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_property&#34;&gt;Markov Property&lt;/a&gt;. The Markov
Property states that the probability of future states depends only on the
present state, not on the sequence of events that preceded it. This concept can
be elegantly implemented using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_chain&#34;&gt;Markov
Chain&lt;/a&gt; storing the probabilities of
transitioning to a next state.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look at a simple example of a Markov Chain that models text using bigrams.
The following code creates a list of bigrams from a piece of text.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; s = &amp;quot;I am Sam. Sam I am. I do not like green eggs and ham.&amp;quot;
&amp;gt;&amp;gt;&amp;gt; tokens = s.split(&amp;quot; &amp;quot;)
&amp;gt;&amp;gt;&amp;gt; bigrams = [(tokens[i],tokens[i+1]) for i in range(0,len(tokens)-1)]
&amp;gt;&amp;gt;&amp;gt; bigrams
[(&#39;I&#39;, &#39;am&#39;), (&#39;am&#39;, &#39;Sam.&#39;), (&#39;Sam.&#39;, &#39;Sam&#39;), (&#39;Sam&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;am.&#39;), (&#39;am.&#39;, &#39;I&#39;), (&#39;I&#39;, &#39;do&#39;), (&#39;do&#39;, &#39;not&#39;), (&#39;not&#39;, &#39;like&#39;), (&#39;like&#39;, &#39;green&#39;), (&#39;green&#39;, &#39;eggs&#39;), (&#39;eggs&#39;, &#39;and&#39;), (&#39;and&#39;, &#39;ham.&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Listing the bigrams starting with the word &lt;code&gt;I&lt;/code&gt; results in:
&lt;code&gt;I am&lt;/code&gt;, &lt;code&gt;I am.&lt;/code&gt;, and &lt;code&gt;I do&lt;/code&gt;. If we were to use this data to predict a word that
follows the word &lt;code&gt;I&lt;/code&gt; we have three choices and each of them has the same
probability (&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt;) of being a valid choice. Modeling this using a Markov Chain
results in a state machine with an approximately 0.33 chance of transitioning to
any one of the next states.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/transitions-from-I.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/transitions-from-I.png&#34; alt=&#34;Transitions from I&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We can add additional transitions to our Chain by considering additional bigrams
starting with &lt;code&gt;am&lt;/code&gt;, &lt;code&gt;am.&lt;/code&gt;, and &lt;code&gt;do&lt;/code&gt;. In each case, there is only one possible
choice for the next state in our Markov Chain given the bigrams we know from our
input text. Each transition from one of these states therefore has a 1.0
probability.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/following-transitions-from-I.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/following-transitions-from-I.png&#34; alt=&#34;Following Transitions from I&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Now, given a starting point in our chain, say &lt;code&gt;I&lt;/code&gt;, we can follow the transitions
to predict a sequence of words. This sequence follows the probability
distribution of the bigrams we have learned. For example, we can randomly sample
from the possible transitions from &lt;code&gt;I&lt;/code&gt; to arrive at the next possible state in
the machine.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; import random
&amp;gt;&amp;gt;&amp;gt; random.sample([&#39;am&#39;, &#39;am.&#39;, &#39;do&#39;], 1)
[&#39;am.&#39;]
&amp;gt;&amp;gt;&amp;gt; random.sample([&#39;am&#39;, &#39;am.&#39;, &#39;do&#39;], 1)
[&#39;do&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Making the first transition, to &lt;code&gt;do&lt;/code&gt;, we can sample from the possible states
following &lt;code&gt;do&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; random.sample([&#39;am&#39;, &#39;am.&#39;, &#39;do&#39;], 1)
[&#39;do&#39;]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;writing-a-markov-chain:d97a8c10263ba1350fa7aeccc4b468c0&#34;&gt;Writing a Markov Chain&lt;/h2&gt;

&lt;p&gt;We have all the building blocks we need to write a complete Markov Chain
implementation. The implementation is a simple dictionary with each key being
the current state and the value being the list of possible next states. For
example, after learning the text &lt;code&gt;I am Sam.&lt;/code&gt; our dictionary would look like
this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
    &#39;I&#39;: [&#39;am&#39;],
    &#39;am&#39;: [&#39;Sam.&#39;],
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And after adding the text &lt;code&gt;Sam I am.&lt;/code&gt; our dictionary would look like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;{
    &#39;I&#39;: [&#39;am&#39;, &#39;am.&#39;],
    &#39;am&#39;: [&#39;Sam.&#39;],
    &#39;Sam&#39;: [&#39;I&#39;],
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can implement a basic Markov Chain that creates a bigram dictionary using the
following code.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class MarkovChain:

    def __init__(self):
        self.memory = {}

    def _learn_key(self, key, value):
        if key not in self.memory:
            self.memory[key] = []

        self.memory[key].append(value)

    def learn(self, text):
        tokens = text.split(&amp;quot; &amp;quot;)
        bigrams = [(tokens[i], tokens[i + 1]) for i in range(0, len(tokens) - 1)]
        for bigram in bigrams:
            self._learn_key(bigram[0], bigram[1])


if __name__ == &#39;__main__&#39;:
    m = MarkovChain()
    m.learn(&#39;I am Sam. Sam I am. I do not like green eggs and ham.&#39;)
    print(m.memory)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; python markov_chain.py
{&#39;I&#39;: [&#39;am&#39;, &#39;am.&#39;, &#39;do&#39;],
 &#39;Sam&#39;: [&#39;I&#39;],
 &#39;Sam.&#39;: [&#39;Sam&#39;],
 &#39;am&#39;: [&#39;Sam.&#39;],
 &#39;am.&#39;: [&#39;I&#39;],
 &#39;and&#39;: [&#39;ham.&#39;],
 &#39;do&#39;: [&#39;not&#39;],
 &#39;eggs&#39;: [&#39;and&#39;],
 &#39;green&#39;: [&#39;eggs&#39;],
 &#39;like&#39;: [&#39;green&#39;],
 &#39;not&#39;: [&#39;like&#39;]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can then transition to a new state in our Markov Chain by randomly
choosing a next state given the current state. If we do not have any information
on the current state we can randomly pick a state to start in.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _next(self, current_state):
    next_possible = self.memory.get(current_state)

    if not next_possible:
        next_possible = self.memory.keys()

    return random.sample(next_possible, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The transition probabilities between states naturally become weighted as we
learn more text.  For example, in the following sequence we learn a few
sentences with the same bigrams and in the final state we are twice as likely to
choose &lt;code&gt;am&lt;/code&gt; as the next word following &lt;code&gt;I&lt;/code&gt; by randomly sampling from the next
possible states.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from markov_chain import MarkovChain
&amp;gt;&amp;gt;&amp;gt; m = MarkovChain()
&amp;gt;&amp;gt;&amp;gt; m.learn(&#39;I am Sam.&#39;)
&amp;gt;&amp;gt;&amp;gt; m.memory
{&#39;I&#39;: [&#39;am&#39;], &#39;am&#39;: [&#39;Sam.&#39;]}
&amp;gt;&amp;gt;&amp;gt; m.learn(&#39;I am Kevin.&#39;)
&amp;gt;&amp;gt;&amp;gt; m.memory
{&#39;I&#39;: [&#39;am&#39;, &#39;am&#39;], &#39;am&#39;: [&#39;Sam.&#39;, &#39;Kevin.&#39;]}
&amp;gt;&amp;gt;&amp;gt; m.learn(&#39;I do.&#39;)
&amp;gt;&amp;gt;&amp;gt; m.memory  # Twice as likely to follow &#39;I&#39; with &#39;am&#39; than &#39;do&#39;.
{&#39;I&#39;: [&#39;am&#39;, &#39;am&#39;, &#39;do&#39;], &#39;am&#39;: [&#39;Sam.&#39;, &#39;Kevin.&#39;]}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The state machine produced by our code would have the probabilities in the
following figure.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/learned-probabilities.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/nlp/ngram-modeling-with-markov-chains/learned-probabilities.png&#34; alt=&#34;Learned Probabilities&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Finally, we can ask our chain to print out some text of an arbitrary length by
following the transitions between the text we have learned.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def babble(self, amount, state=&#39;&#39;):
    if not amount:
        return state

    next_word = self._next(state)

    if not next_word:
        return state

    return state + &#39; &#39; + self.babble(amount - 1, next_word)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting it all together we have a simple Markov Chain that can learn bigrams and
babble text given the probability of bigrams that it has learned. Markov Chain&amp;rsquo;s
are a simple way to store and query n-gram probabilities. Full source code for
this example follows.&lt;/p&gt;

&lt;h2 id=&#34;the-implementation:d97a8c10263ba1350fa7aeccc4b468c0&#34;&gt;The Implementation&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import random


class MarkovChain:

    def __init__(self):
        self.memory = {}

    def _learn_key(self, key, value):
        if key not in self.memory:
            self.memory[key] = []

        self.memory[key].append(value)

    def learn(self, text):
        tokens = text.split(&amp;quot; &amp;quot;)
        bigrams = [(tokens[i], tokens[i + 1]) for i in range(0, len(tokens) - 1)]
        for bigram in bigrams:
            self._learn_key(bigram[0], bigram[1])

    def _next(self, current_state):
        next_possible = self.memory.get(current_state)

        if not next_possible:
            next_possible = self.memory.keys()

        return random.sample(next_possible, 1)[0]

    def babble(self, amount, state=&#39;&#39;):
        if not amount:
            return state

        next_word = self._next(state)
        return state + &#39; &#39; + self.babble(amount - 1, next_word)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Modeling Natural Language with N-Gram Models</title>
      <link>http://sookocheff.com/post/nlp/n-gram-modeling/</link>
      <pubDate>Sat, 25 Jul 2015 06:41:06 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/nlp/n-gram-modeling/</guid>
      <description>

&lt;p&gt;One of the most widely used methods natural language is n-gram modeling. This
article explains what an n-gram model is, how it is computed, and what the
probabilities of an n-gram model tell us.&lt;/p&gt;

&lt;h2 id=&#34;what-is-an-n-gram:6aa7a6edb627f8b743e3120c4f84c63a&#34;&gt;What is an n-gram?&lt;/h2&gt;

&lt;p&gt;&lt;blockquote&gt;
  &lt;p&gt;An n-gram is a contiguous sequence of n items from a given sequence of text.&lt;/p&gt;
  &lt;footer&gt;Wikipedia &lt;cite title=&#34;https://en.wikipedia.org/wiki/N-gram&#34;&gt;https://en.wikipedia.org/wiki/N-gram&lt;/cite&gt;&lt;/footer&gt;
&lt;/blockquote&gt;
&lt;/p&gt;

&lt;p&gt;Given a sentence, &lt;code&gt;s&lt;/code&gt;, we can construct a list of n-grams from &lt;code&gt;s&lt;/code&gt; by finding
pairs of words that occur next to each other. For example, given the sentence &amp;ldquo;I
am Sam&amp;rdquo; you can construct bigrams (n-grams of length 2) by finding consecutive
pairs of words.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; s = &amp;quot;I am Sam.&amp;quot;
&amp;gt;&amp;gt;&amp;gt; tokens = s.split(&amp;quot; &amp;quot;)
&amp;gt;&amp;gt;&amp;gt; bigrams = [(tokens[i],tokens[i+1]) for i in range(0,len(tokens)-1)]
&amp;gt;&amp;gt;&amp;gt; bigrams
[(&#39;I&#39;, &#39;am&#39;), (&#39;am&#39;, &#39;Sam.&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;calculating-n-gram-probability:6aa7a6edb627f8b743e3120c4f84c63a&#34;&gt;Calculating n-gram Probability&lt;/h2&gt;

&lt;p&gt;Given a list of n-grams we can count the number of occurrences of each n-gram;
this count determines the frequency with which an n-gram occurs throughout our
document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; from collections import Counter
&amp;gt;&amp;gt;&amp;gt; count = Counter(bigrams)
&amp;gt;&amp;gt;&amp;gt; count
[((&#39;am&#39;, &#39;Sam.&#39;), 1), ((&#39;I&#39;, &#39;am&#39;), 1)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With this small corpus we only count one occurrence of each n-gram. By dividing
these counts by the size of all n-grams in our list we would get a probability
of 0.5 of each n-gram occurring.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s look a larger corpus of words and see what the probabilities can tell us.
The following sequence of bigrams was computed from data downloaded from &lt;a href=&#34;http://www.corpora.heliohost.org/&#34;&gt;HC
Corpora&lt;/a&gt;. It lists the 20 most frequently
encountered bigrams out of 97,810,566 bigrams in the entire corpus.&lt;/p&gt;

&lt;p&gt;This data represents the most frequently used pairs of words in the corpus along
with the number of times they occur.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;of	the	421560
in	the	380608
to	the	207571
for	the	190683
on	the	184430
to	be	153285
at	the	128980
and	the	114232
in	a	109527
with	the	99141
is	a	99053
for	a	90209
from	the	82223
with	a	78918
will	be	78049
of	a	78009
I	was	76788
I	have	76621
going	to	75088
is	the	70045
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By consulting our frequency table of bigrams, we can tell that the sentence
&lt;code&gt;There was heavy rain last night&lt;/code&gt; is much more likely to be grammatically
correct than the sentence &lt;code&gt;There was large rain last night&lt;/code&gt; by the fact that the
bigram &lt;code&gt;heavy rain&lt;/code&gt; occurs much more frequently than &lt;code&gt;large rain&lt;/code&gt; in our corpus.
Said another way, the probability of the bigram &lt;code&gt;heavy rain&lt;/code&gt; is larger than the
probability of the bigram &lt;code&gt;large rain&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;sentences-as-probability-models:6aa7a6edb627f8b743e3120c4f84c63a&#34;&gt;Sentences as probability models&lt;/h2&gt;

&lt;p&gt;More precisely, we can use n-gram models to derive a probability of the sentence
,&lt;code&gt;W&lt;/code&gt;, as the joint probability of each individual word in the sentence, &lt;code&gt;wi&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(W) = P(w1, w2, ..., wn)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This can be reduced to a sequence of n-grams using the Chain Rule of
conditional probability.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(x1, x2, ..., xn) = P(x1)P(x2|x1)...P(xn|x1,...xn-1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As a concrete example, let&amp;rsquo;s predict the probability of the sentence &lt;code&gt;There was
heavy rain&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(&#39;There was heavy rain&#39;) = P(&#39;There&#39;, &#39;was&#39;, &#39;heavy&#39;, &#39;rain&#39;)
P(&#39;There was heavy rain&#39;) = P(&#39;There&#39;)P(&#39;was&#39;|&#39;There&#39;)P(&#39;heavy&#39;|&#39;There was&#39;)P(&#39;rain&#39;|&#39;There was heavy&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Each of the terms on the right hand side of this equation are n-gram
probabilities that we can estimate using the counts of n-grams in our corpus. To
calculate the probability of the entire sentence, we just need to lookup the
probabilities of each component part in the conditional probability.&lt;/p&gt;

&lt;p&gt;Unfortunately, this formula does not scale since we cannot compute n-grams of
every length. For example, consider the case where we have solely bigrams in our
model; we have no way of knowing the probability `P(&amp;lsquo;rain&amp;rsquo;|&amp;lsquo;There was&amp;rsquo;) from
bigrams.&lt;/p&gt;

&lt;p&gt;By using the &lt;a href=&#34;https://en.wikipedia.org/wiki/Markov_property&#34;&gt;Markov Assumption&lt;/a&gt;,
we can simplify our equation by assuming that future states in our model only
depend upon the present state of our model. This assumption means that we can
reduce our conditional probabilities to be approximately equal so that&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(&#39;rain&#39;|&#39;There was heavy&#39;) ~ P(&#39;rain&#39;|&#39;heavy&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;More generally, we can estimate the probability of a sentence by the
probabilities of each component part. In the equation that follows, the
probability of the sentence is reduced to the probabilities of the sentence&amp;rsquo;s
individual bigrams.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-dns&#34;&gt;P(&#39;There was heavy rain&#39;) ~ P(&#39;There&#39;)P(&#39;was&#39;|&#39;There&#39;)P(&#39;heavy&#39;|&#39;was&#39;)P(&#39;rain&#39;|&#39;heavy&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;applications:6aa7a6edb627f8b743e3120c4f84c63a&#34;&gt;Applications&lt;/h2&gt;

&lt;p&gt;What can we use n-gram models for? Given the probabilities of a sentence we can
determine the likelihood of an automated machine translation being correct, we
could predict the next most likely word to occur in a sentence, we could
automatically generate text from speech, automate spelling correction, or
determine the relative sentiment of a piece of text.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Structuring an Application using Model View Controller</title>
      <link>http://sookocheff.com/post/architecture/structuring-with-mvc/</link>
      <pubDate>Thu, 09 Jul 2015 19:40:15 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/architecture/structuring-with-mvc/</guid>
      <description>

&lt;p&gt;Early pioneers in object-oriented programming paved the path towards using Model
View Controller (MVC) for graphical user interfaces as early as 1970 and web
applications have continued using the pattern to separate business logic from
display. This article attempts to clarify the use of Model View Controller
within web applications — giving consideration to the fact that most developers
will be building their application using an existing web framework.&lt;/p&gt;

&lt;h2 id=&#34;model-view-controller:9f1e744e0ce624b070c416549ce80507&#34;&gt;Model View Controller&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start our investigation of Model View Controller for web applications by examining a broad
overview of how the Model, View and Controller work together to handle a single
request to a web server. This diagram is adapted from the book &lt;a href=&#34;http://martinfowler.com/books/eaa.html&#34;&gt;Patterns of
Enterprise Application Architecture by Martin
Fowler&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/MVCBroadOverview.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/MVCBroadOverview.png&#34; alt=&#34;Model View Controller&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;A request comes in to the application and is handled by an input controller (1).
The controller parses any data that is on the request (e.g., query parameters,
cookies, form data) and chooses the appropriate model objects and
performs domain logic for this request (2). The controller chooses a view for
displaying the result of the domain logic and data or model objects are passed
into the view (3). The view uses this data to render the response (4). Finally,
the response is returned to the user via the controller (5).&lt;/p&gt;

&lt;p&gt;The most important reason for applying Model View Controller is to ensure that
models are completely separated from the presentation. This makes it easier to
modify the presentation independently of domain and business logic. Within this
request flow the model objects are responsible for integrating with the
persistent data source and potentially gathering information for display in the
view.&lt;/p&gt;

&lt;h2 id=&#34;web-frameworks:9f1e744e0ce624b070c416549ce80507&#34;&gt;Web Frameworks&lt;/h2&gt;

&lt;p&gt;Most web frameworks incorporate most of the functionality described by Model
View Controller in their design. The Model View Controller and associated
patterns were developed when web frameworks where in their infancy and largely
outline the correct process to create a web framework from scratch. A common
misconception for new web application developers learning Model View Controller
is to implement MVC within their web framework. This unfortunately leads to a
convoluted design with unnecessary layers of indirection during the processing
of the web request.&lt;/p&gt;

&lt;p&gt;My advice for new developers is to lean on your framework to implement Model
View Controller and focus your attention on your business logic. As a concrete
example, I will walk through a sample architecture for App Engine applications
that uses &lt;a href=&#34;https://webapp-improved.appspot.com/&#34;&gt;webapp2&lt;/a&gt;,
&lt;a href=&#34;https://cloud.google.com/appengine/docs/python/ndb/&#34;&gt;ndb&lt;/a&gt;, and
&lt;a href=&#34;http://jinja.pocoo.org/docs/dev/&#34;&gt;jinja&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this example file, App Engine acts as our web server implementing the
&lt;a href=&#34;http://wsgi.readthedocs.org/en/latest/&#34;&gt;WSGI&lt;/a&gt; specification. Once a request is
received by the server it is forwarded to the webapp2 framework which handles
routing to a &lt;a href=&#34;http://martinfowler.com/eaaCatalog/pageController.html&#34;&gt;Page Controller&lt;/a&gt;.  So far,
the WSGI application and webapp2 handle the creation of a controller to handle
the request.&lt;/p&gt;

&lt;h2 id=&#34;model-view-controller-for-app-engine:9f1e744e0ce624b070c416549ce80507&#34;&gt;Model View Controller for App Engine&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import webapp2


class HelloWorld(webapp2.RequestHandler):

    def get(self):
        self.response.out.write(&#39;Hello World&#39;)


ROUTES = [
    webapp2.Route(&#39;/&#39;, handler=HelloWorld)
]

APPLICATION = webapp2.WSGIApplication(ROUTES)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can start by implementing handling the request and customizing our response
based on any incoming request data (1).&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/InstantiateController.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/InstantiateController.png&#34; alt=&#34;Instantiating a Controller&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import webapp2


class HelloWorld(webapp2.RequestHandler):

    def get(self):
        name = self.request.params.get(&#39;name&#39;)
        if name:
            self.response.out.write(&#39;Hello %s&#39; % name)
        else:
            self.response.out.write(&#39;Hello World&#39;)


ROUTES = [
    webapp2.Route(&#39;/&#39;, handler=HelloWorld)
]

APPLICATION = webapp2.WSGIApplication(ROUTES)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s connect our application to a model (2).&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/PerformDomainLogic.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/PerformDomainLogic.png&#34; alt=&#34;Perform Domain Logic&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;The model in our diagram is connected to a data source and is responsible for
converting from the data source to an in memory representation. ndb follows an
&lt;a href=&#34;http://www.martinfowler.com/eaaCatalog/activeRecord.html&#34;&gt;Active Record&lt;/a&gt;
pattern, providing a one-to-one mapping from the data source to memory, and a
&lt;a href=&#34;http://www.martinfowler.com/eaaCatalog/tableModule.html&#34;&gt;Table Module&lt;/a&gt; for . You
could build up a rich &lt;a href=&#34;http://www.martinfowler.com/eaaCatalog/domainModel.html&#34;&gt;Domain
Model&lt;/a&gt; based on ndb. In
my opinion the overhead of a Domain Model outweighs its benefits and you end up
fighting against a pattern that is already available with ndb. For our simple
application, our model is a user object that is created or loaded based on the
query parameter passed in on the request.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import webapp2
from google.appengine.ext import ndb


class User(ndb.Model):

    name = ndb.StringProperty()


class HelloWorld(webapp2.RequestHandler):

    def get(self):
        name = self.request.params.get(&#39;name&#39;)
        if name:
            user = ndb.Key(&#39;User&#39;, name).get()
            if not user:
                user = User(name=name, id=name)
                user.put()
            self.response.out.write(&#39;Hello %s&#39; % user.name)
        else:
            self.response.out.write(&#39;Hello World&#39;)


ROUTES = [
    webapp2.Route(&#39;/&#39;, handler=HelloWorld)
]

APPLICATION = webapp2.WSGIApplication(ROUTES)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We&amp;rsquo;ve leveraged ndb to handle mapping from our data source to an in memory
representation and don&amp;rsquo;t need to define any special handling to interact with
the data source. This is typical when working with a pre-existing framework and
I would suggest caution when making any more complex data source transformations
&lt;em&gt;unless you are building your own framework&lt;/em&gt;. We also perform some simple domain
logic in our controller. Since getting or creating a model is a common operation
we may want to push this function to the model itself to provide reuse. The
Active Record pattern that ndb implements lends itself to this structure.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import webapp2
from google.appengine.ext import ndb


class User(ndb.Model):

    name = ndb.StringProperty()

    @classmethod
    def get_or_create(cls, name):
        if not name:
            return None

        user = ndb.Key(&#39;User&#39;, name).get()
        if not user:
            user = User(name=name, id=name)
            user.put()
        return user


class HelloWorld(webapp2.RequestHandler):

    def get(self):
        name = self.request.params.get(&#39;name&#39;)
        user = User.get_or_create(name)

        if user:
            self.response.out.write(&#39;Hello %s&#39; % user.name)
        else:
            self.response.out.write(&#39;Hello World&#39;)


ROUTES = [
    webapp2.Route(&#39;/&#39;, handler=HelloWorld)
]

APPLICATION = webapp2.WSGIApplication(ROUTES)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;ve separated common functionality to the model and used the controller to
coordinate the domain logic. As this application grows the controller can be
leveraged to perform more complex domain logic on multiple models. If we find
that we have two controllers performing similar logic we may want to move that
particular functionality out into a shared script.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s extend our example by rendering our page using a view.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/RenderView.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/RenderView.png&#34; alt=&#34;Rendering the View&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We leverage Jinja to act as our view and provide the
interpretation and rendering of our model following a
&lt;a href=&#34;http://www.martinfowler.com/eaaCatalog/templateView.html&#34;&gt;Template View&lt;/a&gt;
pattern. We&amp;rsquo;ve again kept our interpretation of Model View Controller simple by
working within our web application framework rather than against it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import webapp2
import jinja2
from google.appengine.ext import ndb


JINJA_ENVIRONMENT = jinja2.Environment(
    loader=jinja2.FileSystemLoader(os.path.dirname(__file__)),
    extensions=[&#39;jinja2.ext.autoescape&#39;])


class User(ndb.Model):

    name = ndb.StringProperty()

    @classmethod
    def get_or_create(cls, name):
        if not name:
            return None

        user = ndb.Key(&#39;User&#39;, name).get()
        if not user:
            user = User(name=name, id=name)
            user.put()
        return user


class HelloWorld(webapp2.RequestHandler):

    def get(self):
        name = self.request.params.get(&#39;name&#39;)
        user = User.get_or_create(name)

        name = user.name if user else &#39;World&#39;

        template = JINJA_ENVIRONMENT.get_template(&#39;index.html&#39;)
        self.response.out.write(template.render(name=name))


ROUTES = [
    webapp2.Route(&#39;/&#39;, handler=HelloWorld)
]

APPLICATION = webapp2.WSGIApplication(ROUTES)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, webapp2 and the WSGI application and server handle returning our
response to the user and completing the request-response cycle.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/ReturnResponse.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/architecture/structuring-with-mvc/ReturnResponse.png&#34; alt=&#34;Returning a Response&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;As our application becomes more complex, we may want to separate our code into
separate modules for Model, View and Controller. I recommend including an
additional module called Scripts that stores more complex interactions between
models that are used in multiple controllers. However, since each request to our
application is typically an independent transaction sharing logic between
different requests should be rare. The final directory structure should look
something like the following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;.
├── app.yaml
├── controller
│   ├── __init__.py
│   └── index.py
├── main.py
├── model
│   ├── __init__.py
│   └── user.py
├── scripts
│   └── __init__.py
└── view
    ├── __init__.py
    └── index.html
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The overarching theme of this article is to leverage our existing framework and
libraries to implement the Model View Controller pattern for us. Leave your
application simple and lean on your tools. Of course, design patterns are
subjective and my opinion may not apply to your use case — take what works for
you and leave the rest.&lt;/p&gt;

&lt;p&gt;Full source code for this example is available on &lt;a href=&#34;https://github.com/soofaloofa/ModelViewController&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managed VMs and the Future of App Engine</title>
      <link>http://sookocheff.com/post/appengine/managed-vms/managed-vms-and-the-future-of-appengine/</link>
      <pubDate>Tue, 23 Jun 2015 20:33:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/appengine/managed-vms/managed-vms-and-the-future-of-appengine/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been thinking about the transition of App Engine to Python 3 and have come
to the conclusion that it will never happen — App Engine will eventually be
deprecated in favour of Managed VMs. Let&amp;rsquo;s break this apart to see why this is.&lt;/p&gt;

&lt;p&gt;First, consider the effort required by Google to develop App Engine. The Python
runtime environment was modified to enforce the sandbox of the App
Engine environment. To provide a Python 3 environment for App Engine as we know
it, the Python 3 runtime would need to be modified with the same restrictions.
Even imagining that this would happen for Python 3.4, the effort to upgrade to
Python 3.5 would require additional effort by Google to modify the runtime.&lt;/p&gt;

&lt;p&gt;Considering the desire to run additional languages such as Javascript (via
NodeJS) or Ruby, Google is put in an untenable position if it expects to
support modified runtimes for multiple versions of different languages.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s consider the rise of Docker and the development of Managed VMs.
Managed VMs are built on top of Docker and provide a &amp;lsquo;dockerized&amp;rsquo; platform
hosting your language runtime. Managed VMs provide the auto-scaling,
health-check, colocation and server upgrades that are the hallmark of App Engine while
allowing the use of arbitrary runtimes within the Docker sandbox.&lt;/p&gt;

&lt;p&gt;This diagram from the &lt;a href=&#34;https://cloud.google.com/appengine/docs/managed-vms/&#34;&gt;Managed VMs documentation&lt;/a&gt;
shows the difference between the heavily modified App Engine sandbox and the
more traditional environment running within a Docker container as a Managed VM.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;https://cloud.google.com/appengine/docs/managed-vms/&#34;&gt;
  &lt;img src=&#34;https://cloud.google.com/appengine/images/vmhosting.png&#34; alt=&#34;Managed VM Sandbox&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;The great thing about Managed VMs is that they still allow access to the
traditional App Engine APIs such as Datastore, Memcached, Logging, Task Queues
and Search. This allows you to write applications in a fashion similar to your
existing App Engine projects in a Managed VM environment. You can also use the
Modules API to have a portion of your application be served by requests routed
to the Managed VM and other requests routed to your App Engine application.&lt;/p&gt;

&lt;p&gt;Managed VMs also allow better insight into product costs. The VMs are deployed
as Compute Engine instances, giving you the ability to monitor CPU and network
usage and upgrade and downgrade your instances as you see fit.&lt;/p&gt;

&lt;p&gt;My feeling is that as the APIs to access Google Cloud Platform are extended and
enhanced to work outside of the current App Engine sandbox, Managed VMs will
take over Google&amp;rsquo;s development efforts to the point where App Engine as we know
it is replaced with Managed VMs. The next project I develop will be using
Managed VMs from the start.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing CloudPyPI</title>
      <link>http://sookocheff.com/post/python/introducing-cloudpypi/</link>
      <pubDate>Tue, 16 Jun 2015 21:19:26 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/python/introducing-cloudpypi/</guid>
      <description>&lt;p&gt;A common problem with Python development for large-scale teams is sharing
internal libraries. At Vendasta we&amp;rsquo;ve been solving this problem using a private
PyPI installation running on Google App Engine with Python eggs and wheels being
served by Google Cloud Storage. Today, we are announcing the open source version
of this tool — CloudPyPI.&lt;/p&gt;

&lt;p&gt;CloudPyPI is a modification of
&lt;a href=&#34;https://pypi.python.org/pypi/pypiserver&#34;&gt;pypiserver&lt;/a&gt; for running on Google App
Engine. We&amp;rsquo;ve also introduced a simple user management system to allow
authenticated access to your Python packages. Together, we&amp;rsquo;ve found this to be a
robust tool for distributing private Python libraries internally. If this is a
problem you&amp;rsquo;ve been trying to solve, give CloudPyPI a try — contributions and
feature requests are always welcome.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;btn btn-default&#34; href=&#34;https://github.com/vendasta/cloudpypi&#34; role=&#34;button&#34;&gt;Check out CloudPyPI on Github&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 6: The Pipeline UI</title>
      <link>http://sookocheff.com/post/appengine/pipelines/pipeline-ui/</link>
      <pubDate>Tue, 09 Jun 2015 20:43:56 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/appengine/pipelines/pipeline-ui/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article will serve as a reminder of the Pipeline UI as much for the writer
as for the reader. The Pipeline UI requires the MapMeduce library to be
installed. If you are not familiar with MapReduce please refer to the &lt;a href=&#34;http://sookocheff.com/series/mapreduce-api/&#34;&gt;MapReduce
API Series of articles&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Once MapReduce is installed you will need to add a few indices to &lt;code&gt;index.yaml&lt;/code&gt;
to properly query for pipeline records for display in the UI.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;indexes:
- kind: _AE_Pipeline_Record
  properties: 
    - name: is_root_pipeline 
    - name: start_time 
      direction: desc

- kind: _AE_Pipeline_Record
  properties: 
    - name: class_path 
    - name: is_root_pipeline 
    - name: start_time
      direction: desc
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;list-root-pipelines:7ab75bf11afccfc89345fe106e22b8e1&#34;&gt;List Root Pipelines&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/mapreduce/pipeline/list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This URL will list all root pipelines in the system, ordered by their starting
time. You can filter pipelines by their class path and check on an individual
pipelines status using this UI.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-ui/root-pipelines.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-ui/root-pipelines.png&#34; alt=&#34;List Root Pipelines&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;pipeline-status:7ab75bf11afccfc89345fe106e22b8e1&#34;&gt;Pipeline Status&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/mapreduce/pipeline/status?root=7ba9b9b2b2e24787b3b4c11079178cb6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you know a pipeline&amp;rsquo;s root identifier you can jump directly to the status
page. This page presents you with a UI displaying the status of a pipeline and
any of the pipeline&amp;rsquo;s children.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-ui/pipeline-status.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-ui/pipeline-status.png&#34; alt=&#34;Pipeline Status&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;list-mapreduce-pipelines:7ab75bf11afccfc89345fe106e22b8e1&#34;&gt;List MapReduce Pipelines&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/mapreduce/status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If your pipeline is a MapReduce job, it will have an entry on the MapReduce
status page. You can navigate to individual jobs to check their sharding and
processing status or cleanup a job by removing datastore entries for the
pipeline.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-ui/mapreduce-list.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-ui/mapreduce-list.png&#34; alt=&#34;MapReduce List&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;mapreduce-status:7ab75bf11afccfc89345fe106e22b8e1&#34;&gt;MapReduce Status&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/mapreduce/detail?mapreduce_id=1574647046653BE202D4D
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you know a mapreduce job&amp;rsquo;s identifier you can jump directly to the status
page. This page will show any counters you have defined and show how the
processing of each shard of data.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-ui/mapreduce-status.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-ui/mapreduce-status.png&#34; alt=&#34;MapReduce Status&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 5: Asynchronous Pipelines</title>
      <link>http://sookocheff.com/post/appengine/pipelines/asynchronous-pipelines/</link>
      <pubDate>Tue, 02 Jun 2015 04:45:56 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/appengine/pipelines/asynchronous-pipelines/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article will cover fully asynchronous pipelines. The term &amp;lsquo;asynchronous&amp;rsquo; is
misleading here — all piplines are asynchronous in the sense that yielding a
pipeline is a non-blocking operation. An asynchronous refers to a
pipeline that remains in a RUN state until outside action is taken, for example,
a button is clicked or a task is executed.&lt;/p&gt;

&lt;p&gt;Marking a pipeline as an asynchronous pipeline is as simple as setting the
&lt;code&gt;async&lt;/code&gt; class property to True.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class AsyncPipeline(pipeline.Pipeline):
    async = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once this pipeline starts, it will remain in the RUN state until the pipeline is
transitioned to another state. You transition a pipeline to another state by
calling the &lt;code&gt;complete&lt;/code&gt; method, using a callback. &lt;code&gt;complete()&lt;/code&gt; is a
method only available to asynchronous pipelines. Calling complete will fill the
pipelines output slots and, if all slots have been filled, mark the pipeline
complete. Any barriers related to the slots being filled are notified as
described in &lt;a href=&#34;http://sookocheff.com/post/appengine/pipelines/pipeline-internals/&#34;&gt;the previous article&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class AsyncPipeline(pipeline.Pipeline):
    async = True

    def callback(self):
        self.complete()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;callback-urls:74bf5c045d8c3cbfb500a70528c1dea1&#34;&gt;Callback URLs&lt;/h2&gt;

&lt;p&gt;The pipeline API provides convenience methods for calling the callback method.
&lt;code&gt;get_callback_url&lt;/code&gt; returns a URL that, when accessed, passes any query
parameters to the callback method. For example, to generate a URL to our
pipeline with a &lt;code&gt;choice&lt;/code&gt; parameter we can call get_callback_url as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;url = get_callback_url(choice=&#39;approve&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will generate a URL of the form:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;/_ah/pipeline/callback?choice=approve&amp;amp;pipeline_id=fd789852183b4310b5f1353205a967fe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Accessing this URL will pass the &lt;code&gt;choice&lt;/code&gt; parameter to the callback function of
the pipeline with pipeline_id &lt;code&gt;fd789852183b4310b5f1353205a967fe&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class AsyncPipeline(pipeline.Pipeline):
    async = True
    public_callbacks = True

    def run(self):
        url = self.get_callback_url(choice=&#39;approve&#39;)
        logging.info(&#39;Callback URL: %s&#39; % url)

    def callback(self, choice):
        if choice == &#39;approve&#39;:
            logging.info(&#39;Pipeline Complete&#39;)
            self.complete()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the pipeline above will log the Callback URL to the console. By visiting
that URL, the &lt;code&gt;callback&lt;/code&gt; method will execute, completing your pipeline. You can
refer to the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines/blob/master/python/src/pipeline/common.py&#34;&gt;EmailToContinue&lt;/a&gt; Pipeline for a more robust example.&lt;/p&gt;

&lt;h2 id=&#34;callback-tasks:74bf5c045d8c3cbfb500a70528c1dea1&#34;&gt;Callback Tasks&lt;/h2&gt;

&lt;p&gt;The second way to execute a callback method is via a callback task. The
Pipelines API provides another convenience method to generate a callback task
that will execute our pipeline. In the following example, a task is created to
trigger in the future, adding an artificial delay to our pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DelayPipeline(pipeline.Pipeline):
    async = True

    def __init__(self, seconds):
        super(DelayPipeline, self).__init__(seconds=seconds)

    def run(self, seconds=None):
        task = self.get_callback_task(
            countdown=seconds,
            name=&#39;ae-pipeline-delay-&#39; + self.pipeline_id)
        try:
            task.add(self.queue_name)
        except (taskqueue.TombstonedTaskError, taskqueue.TaskAlreadyExistsError):
            pass

    def callback(self):
        self.complete(self.kwargs[&#39;seconds&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the task is queued using the pipeline_id in the task name. This helps
ensure our run method is idempotent. Full source code for an asynchronous
pipeline follows. This pipeline will delay for 10 seconds, and then log a
callback_url to the console. Visiting the callback URL will complete the
pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline
from google.appengine.api import taskqueue


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        pipeline = DelayPipeline(10)
        pipeline.start()


class DelayPipeline(pipeline.Pipeline):
    async = True

    def __init__(self, seconds):
        pipeline.Pipeline.__init__(self, seconds=seconds)

    def run(self, seconds=None):
        task = self.get_callback_task(
            countdown=seconds,
            name=&#39;ae-pipeline-delay-&#39; + self.pipeline_id)
        try:
            task.add(self.queue_name)
        except (taskqueue.TombstonedTaskError,
                taskqueue.TaskAlreadyExistsError):
            pass

    def callback(self):
        AsyncPipeline().start()


class AsyncPipeline(pipeline.Pipeline):
    async = True
    public_callbacks = True

    def run(self):
        url = self.get_callback_url(choice=&#39;approve&#39;)
        logging.info(&#39;Callback URL: %s&#39; % url)

    def callback(self, choice):
        if choice == &#39;approve&#39;:
            self.complete()


routes = [webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 4: Pipeline Internals</title>
      <link>http://sookocheff.com/post/appengine/pipelines/pipeline-internals/</link>
      <pubDate>Wed, 27 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/appengine/pipelines/pipeline-internals/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://sookocheff.com/post/appengine/pipelines/connecting-pipelines/&#34;&gt;We&amp;rsquo;ve learned how to execute and chain together pipelines&lt;/a&gt;,
now let&amp;rsquo;s take a look at how pipelines execute under the hood. If necessary,
you can refer to the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines&#34;&gt;source code of the pipelines
project&lt;/a&gt; to
clarify any details.&lt;/p&gt;

&lt;h2 id=&#34;the-pipeline-data-model:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;The Pipeline Data Model&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with the pipeline data model. Note that each Kind defined by the
pipelines API is prefixed by &lt;code&gt;_AE_Pipeline&lt;/code&gt;, making it easy to view individual
pipeline details by viewing the datastore entity.&lt;/p&gt;

&lt;h3 id=&#34;pipelinerecord:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;PipelineRecord&lt;/h3&gt;

&lt;p&gt;Every pipeline is represented by a &lt;em&gt;PipelineRecord&lt;/em&gt; in the datastore. The
PipelineRecord records the pipeline&amp;rsquo;s root identifier (if this pipeline is a
child), any child pipelines spawned by this pipeline, the current status
of the pipeline, and a few additional bookkeeping details.&lt;/p&gt;

&lt;p&gt;At any point in time a Pipeline may be in one of four states: WAITING, RUN,
DONE, and ABORTED.  WAITING implies that this pipeline has a barrier that
must be satisfied before the pipeline can be RUN. RUN means that the pipeline
has been started. DONE means that the pipeline is complete. ABORTED means
that the pipeline has been manually aborted.&lt;/p&gt;

&lt;h3 id=&#34;slotrecord:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;SlotRecord&lt;/h3&gt;

&lt;p&gt;The output of a pipeline is represented as a &lt;em&gt;Slot&lt;/em&gt; stored in the datastore as a
&lt;em&gt;SlotRecord&lt;/em&gt;. When a pipeline completes, it stores its output in the SlotRecord
to be made available to further pipelines.&lt;/p&gt;

&lt;h3 id=&#34;barrierrecord:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;BarrierRecord&lt;/h3&gt;

&lt;p&gt;A &lt;em&gt;BarrierRecord&lt;/em&gt; represents the slots that must be filled before a pipeline can
execute. The barrier tracks &lt;em&gt;blocking_slots&lt;/em&gt; that must be filled before the
barrier can be lifted. Once the barrier is lifted a &lt;em&gt;target&lt;/em&gt; pipeline is
notified and the target can transition to the RUN state.&lt;/p&gt;

&lt;p&gt;Barriers that depend on a slot being filled are stored in the &lt;em&gt;BarrierIndex&lt;/em&gt;,
which tracks barriers that are dependent on a slot. The purpose of the
BarrierIndex is to force &lt;a href=&#34;https://cloud.google.com/datastore/docs/articles/balancing-strong-and-eventual-consistency-with-google-cloud-datastore/&#34;&gt;strong consistency&lt;/a&gt; when querying for a SlotRecord&amp;rsquo;s Barriers.&lt;/p&gt;

&lt;h3 id=&#34;statusrecord:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;StatusRecord&lt;/h3&gt;

&lt;p&gt;The &lt;em&gt;StatusRecord&lt;/em&gt; tracks the current status of a pipeline and facilitates the
pipeline user interface. The StatusRecord is updated as the pipeline progresses
to give a view of the current pipeline state. Not much more than that.&lt;/p&gt;

&lt;h2 id=&#34;pipeline-execution:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;Pipeline Execution&lt;/h2&gt;

&lt;p&gt;Having an understanding of the pipeline data model gives a rough idea of how
pipelines are executed. Each stage of execution corresponds to a webapp2 handler
that services the request and advances the state of the pipeline. The following
diagram shows each of the pipeline stages during typical execution and the
description that follows provides more detail on each stage.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-internals/pipeline-states.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-internals/pipeline-states.png&#34; alt=&#34;Pipeline States&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h3 id=&#34;start:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;start()&lt;/h3&gt;

&lt;p&gt;A pipeline is started by calling its &lt;code&gt;start()&lt;/code&gt; method. When calling &lt;code&gt;start()&lt;/code&gt; a
&lt;em&gt;PipelineRecord&lt;/em&gt; is created and marked as a &lt;em&gt;RootPipeline&lt;/em&gt;, &lt;em&gt;SlotRecords&lt;/em&gt; are
created for each of the pipelines outputs and marked as children of the
pipeline, and &lt;em&gt;BarrierRecords&lt;/em&gt; are created corresponding to each of the output
slots of the pipeline. Finally, a task is queued to the &lt;code&gt;/run&lt;/code&gt; handler to
execute the pipeline.&lt;/p&gt;

&lt;h3 id=&#34;run-handler:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;/run handler&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;/run&lt;/code&gt; handler transitions the pipeline from the WAITING state to the RUN
state by setting a flag on the &lt;em&gt;PipelineRecord&lt;/em&gt;, the pipeline object instance is
then reconstructed given the data from the request and the pipeline&amp;rsquo;s &lt;code&gt;run()&lt;/code&gt;
method is called. When the &lt;code&gt;run()&lt;/code&gt; method is complete, any outputs are used to
fill &lt;em&gt;SlotRecords&lt;/em&gt; and yielded to the parent pipeline when necessary.  Finally,
any child pipelines and their dependent slots and barriers are created and
marked as children of the parent pipeline. Calls to the &lt;code&gt;/fanout&lt;/code&gt;
handler are made to queue tasks to start any child pipelines.&lt;/p&gt;

&lt;h3 id=&#34;fanout-handler:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;/fanout handler&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;/fanout&lt;/code&gt; handler loads all child pipelines given a parent pipeline and queues
a task to the &lt;code&gt;/run&lt;/code&gt; handler for each of them.&lt;/p&gt;

&lt;h3 id=&#34;output-handler:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;/output handler&lt;/h3&gt;

&lt;p&gt;Whenever a slot is filled a task is queued to the &lt;code&gt;/output&lt;/code&gt; handler to notify any
barriers to a pipeline&amp;rsquo;s execution that they can be removed. If a pipeline has
all its barriers to completing removed, a task is queued to the &lt;code&gt;/finalize&lt;/code&gt; handler
to mark our pipeline as complete. The &lt;code&gt;/output&lt;/code&gt; handler queues tasks to the
&lt;code&gt;/run&lt;/code&gt; method for any pipelines that have their barriers to starting removed.&lt;/p&gt;

&lt;h3 id=&#34;finalized-handler:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;/finalized handler&lt;/h3&gt;

&lt;p&gt;When the &lt;code&gt;/finalized&lt;/code&gt; handler is called, the pipeline is marked as complete and
our pipeline&amp;rsquo;s &lt;code&gt;finalized()&lt;/code&gt; method is called.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Understanding the Pipeline data model and run-time can help you to visualize and
debug any pipeline problems. Stay tuned for the next article covering
asynchronous pipelines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 3: Fan In, Fan Out, Sequencing</title>
      <link>http://sookocheff.com/post/appengine/pipelines/fan-in-fan-out/</link>
      <pubDate>Tue, 19 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/appengine/pipelines/fan-in-fan-out/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://sookocheff.com/post/appengine/pipelines/connecting-pipelines/&#34;&gt;Last time&lt;/a&gt;,
we studied how to connect two pipelines together. In this post, we expand on
this topic, exploring how to fan-out to do multiple tasks in parallel, fan-in
to combine multiple tasks into one, and how to do sequential work.&lt;/p&gt;

&lt;h2 id=&#34;fan-out:077d7e8a485e72c5c014b45abe98e05c&#34;&gt;Fan-Out&lt;/h2&gt;

&lt;p&gt;Fan-Out refers to spreading a task to multiple destinations in parallel. Using
the Pipelines API, fan-out can be achieved elegantly by yielding a new pipeline
for every task you wish to execute. Each of these pipelines is exeucted
immediately via a Task in the App Engine Task Queue. Fan-out parallelizes
implicitly when additional App Engine instances are started to handle the
increased number of requests arriving in the Task Queue. You can moderate the
amount of fan-out by changing the processing rate on the task queue that
executes your pipelines.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;Squaring: %s&#39;, number)
        return number * number


class FanOutPipeline(pipeline.Pipeline):

    def run(self, count):
        for i in xrange(0, count):
            yield SquarePipeline(i)
        # All children run immediately
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fan-in:077d7e8a485e72c5c014b45abe98e05c&#34;&gt;Fan-In&lt;/h2&gt;

&lt;p&gt;Fan-In implies waiting for a collection of related tasks to complete before
continuing processing. The example can be extended by summing the list of
squared values — when we call &lt;code&gt;yield Sum(*results)&lt;/code&gt; the pipeline run-time will
wait until all results are ready before executing Sum. Internally, a &lt;em&gt;barrier&lt;/em&gt;
record is created that blocks execution of Sum and tracks the dependencies
required to lift the barrier. Once all dependencies have been satisfied the
barrier is lifted and Sum can execute.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;Squaring: %s&#39; % number)
        return number * number


class Sum(pipeline.Pipeline):

    def run(self, *args):
        value = sum(list(args))
        logging.info(&#39;Sum: %s&#39;, value)
        return value


class FanInPipeline(pipeline.Pipeline):

    def run(self, count):
        results = []
        for i in xrange(0, count):
            result = yield SquarePipeline(i)
            results.append(result)

        # Waits until all SquarePipeline results are complete
        yield Sum(*results)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sequencing:077d7e8a485e72c5c014b45abe98e05c&#34;&gt;Sequencing&lt;/h2&gt;

&lt;p&gt;A common workflow is running pipelines in a predefined sequence. The Pipelines
API provides context managers that will force execution ordering using the
&lt;code&gt;with&lt;/code&gt; keyword. This is useful for Pipelines with no output that you wish to
execute in a specific order — we cannot wait for the output and so no barrier
must be satisfied, but we still want to enforce an execution order. In the
following example, we extend the FanOutFanInPipeline to update an HTML
dashboard with our results and, once that is complete, send out an e-mail to the
development team. This example is taken from the excellent &lt;a href=&#34;https://www.youtube.com/watch?v=Rsfy_TYA2ZY&#34;&gt;Pipelines API
introductory video&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class FanOutFanInPipeline(pipeline.Pipeline):

    def run(self, count):
        results = []
        for i in xrange(0, count):
            result = yield SquarePipeline(i)
            results.append(result)

        result = yield Sum(*results)
        with pipeline.InOrder():
            yield UpdateDashboard()
            yield EmailTeam()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion:077d7e8a485e72c5c014b45abe98e05c&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This article describes how to coordinate pipeline tasks using fan-in, fan-out
and sequencing. The next article we will discuss Pipeline API internals.&lt;/p&gt;

&lt;p&gt;Full source code of both Fan-In and Fan-Out follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        stage = FanOutFanInPipeline(10)
        stage.start()


class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;Squaring: %s&#39; % number)
        return number * number


class Sum(pipeline.Pipeline):

    def run(self, *args):
        value = sum(list(args))
        logging.info(&#39;Sum: %s&#39;, value)
        return value


class FanOutFanInPipeline(pipeline.Pipeline):

    def run(self, count):
        results = []
        for i in xrange(0, count):
            result = yield SquarePipeline(i)
            results.append(result)

        yield Sum(*results)


routes = [
    webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)
]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 2: Connecting Pipelines</title>
      <link>http://sookocheff.com/post/appengine/pipelines/connecting-pipelines/</link>
      <pubDate>Tue, 12 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/appengine/pipelines/connecting-pipelines/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://sookocheff.com/post/appengine/pipelines/the-basics/&#34;&gt;Last time&lt;/a&gt;,
we discussed basic pipeline instantiation and execution. This time, we will
cover sequential pipelines, answering the question &amp;ldquo;How do I connect the output
of one pipeline with the input of another pipeline&amp;rdquo;?&lt;/p&gt;

&lt;p&gt;To begin, let&amp;rsquo;s review a basic pipeline that squares its input. If any of this
does not make sense refer to the &lt;a href=&#34;http://sookocheff.com/post/appengine/pipelines/the-basics/&#34;&gt;first part of this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        stage = SquarePipeline(10)
        stage.start()


class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        return number * number

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.default.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first step in passing data between two pipelines is updating our pipeline to
use the generator interface. The generator interface uses the &lt;code&gt;yield&lt;/code&gt; keyword as
a means of connecting pipelines together. For this contrived example, let&amp;rsquo;s
create a &lt;em&gt;parent&lt;/em&gt; pipeline that executes &lt;code&gt;SquarePipeline&lt;/code&gt; twice in succession.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What now? We need a way to access the value stored in &lt;code&gt;second_square&lt;/code&gt;. When
execution hits a &lt;code&gt;yield&lt;/code&gt; statement a task is started to run the pipeline and a
&lt;code&gt;PipelineFuture&lt;/code&gt; is returned. The &lt;code&gt;PipelineFuture&lt;/code&gt; will have a value &lt;em&gt;after&lt;/em&gt; the
task has finished executing but not immediately. So how do we access the value?
With a &lt;em&gt;child&lt;/em&gt; pipeline that can read the result. In this example, we simply log
the value of the computation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
        yield LogResult(second_square)

class LogResult(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;All done! Value is %s&#39;, number)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The rule of thumb here is that &lt;em&gt;anything you instantiate your pipeline with (and
subsequently pass to the &lt;code&gt;run&lt;/code&gt; method) is accessible within your
pipeline&lt;/em&gt;. These are called &lt;em&gt;immediate values&lt;/em&gt; and you can treat them as regular
Python values. When this code is executed, each pipeline started by a &lt;code&gt;yield&lt;/code&gt;
call is a separate App Engine Task that executes in the Task Queue. The Pipeline
runtime coordinates running these tasks and shares the results of execution
between tasks, allowing you to safely connect pipelines together.&lt;/p&gt;

&lt;p&gt;Full source code for this example follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        stage = TwiceSquaredPipeline(10)
        stage.start()


class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        return number * number


class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
        yield LogResult(second_square)


class LogResult(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;All done! Value is %s&#39;, number)


routes = [
    webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)
]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 1: The Basics</title>
      <link>http://sookocheff.com/post/appengine/pipelines/the-basics/</link>
      <pubDate>Tue, 05 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/appengine/pipelines/the-basics/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines&#34;&gt;Pipelines API&lt;/a&gt;
is a general purpose workflow engine for App Engine applications. With the
Pipelines API we can connect together complex workflows into a coherent run time
backed by the Datastore. This article provides a basic overview of the Pipelines
API and how it can be used for abritrary computational workflows.&lt;/p&gt;

&lt;p&gt;In the most basic sense a Pipeline is an object that takes input, performs some
logic or computation on that input, and produces output. Pipelines can take two
general forms &amp;ndash; synchronous or asynchronous. Synchronous pipelines act as basic
functions that must complete during a single request. Asynchronous pipelines
spawn child pipelines and connect them together into a workflow by passing input
and output parameters around.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A word of warning.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pipelines must be idempotent and it is up to the developer to ensure that they
are &amp;ndash; this is not enforced by the run-time. A pipeline may fail and be retried
and it is important that running the same pipeline with the same set of inputs
will product the same results.&lt;/p&gt;

&lt;h2 id=&#34;getting-started:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;The first step is to grab the latest version of the Pipelines API (and its
        dependencies) using pip. The following assumes you install third party
App Engine dependencies in the lib directory relative to where pip is being run.
You can also grab the source code from
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install GoogleAppEnginePipeline -t lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pipeline requests need to be handled by the Pipeline application. We set that up
by adding a handler to &lt;code&gt;app.yaml&lt;/code&gt;. The pipeline library itself will enforce the
login access restrictions so we do not need to secure these handlers.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;handlers:
- url: /_ah/pipeline.*
  script: pipeline.handlers._APP
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;basic-synchronous-pipelines:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Basic Synchronous Pipelines&lt;/h2&gt;

&lt;p&gt;A synchronous pipeline runs within the bounds of a single App Engine request.
Once the request has been made the pipeline starts and pipeline processing
happens automatically. We can set up this pipeline by defining a handler
responsible for starting the pipeline. For now, create a default handler that
will receive a request at the URL of your choosing.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2

class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        logging.info(&#39;Launch pipeline&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A request processed by this handler will kick off our Pipeline. To define a
pipeline we inherit from the Pipeline object and the method &lt;code&gt;run&lt;/code&gt;. The pipeline
is launched via the &lt;code&gt;start&lt;/code&gt; method. The code below instantiates a custom
pipeline and launches it. Accessing the URL for the RunPipelineHandler will
print the message &amp;lsquo;Do something here&amp;rsquo; to the logs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline

class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        logging.info(&#39;Launch pipeline&#39;)
        pipeline = MyPipeline()
        pipeline.start()


class MyPipeline(pipeline.Pipeline):
    def run(self, *args, **kwargs):
        logging.info(&#39;Do something here.&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can update our pipeline to do a simple operation, like squaring a number.
You&amp;rsquo;ll notice in the code that follows that the arguments passed when
initializing the pipeline are accessible as parameters to the &lt;code&gt;run&lt;/code&gt; method
within the pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this pipeline will show that the pipeline executes correctly. But where
does our return value go? How can we access the output of &lt;code&gt;SquarePipeline&lt;/code&gt;?&lt;/p&gt;

&lt;h2 id=&#34;accessing-pipeline-output:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Accessing Pipeline Output&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll notice that in &lt;code&gt;SquarePipeline&lt;/code&gt; we are returning a value directly but
we never actually access it. Pipeline output can only ever be accessed after the
pipeline has finished executing. We can check for the end of pipeline execution
using the &lt;code&gt;has_finalized&lt;/code&gt; property. This property will be set to &lt;code&gt;True&lt;/code&gt; when all
stages of a pipeline have finished executing. At this point in time our output
will be available as a value on the Pipeline object. Let&amp;rsquo;s see what happens when
we try to check if our pipeline has finalized. To do this we need to store the
pipeline_id generated from our start method and check the &lt;code&gt;has_finalized&lt;/code&gt;
property.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()

        pipeline_id = square_stage.pipeline_id

        stage = SquarePipeline.from_id(pipeline_id)
        if stage.has_finalized:
            logging.info(&#39;Finalized&#39;)
        else:
            logging.info(&#39;Not finalized&#39;)


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the preceding code we see that our pipeline is not finalized. What
happened here? The pipeline is executed as an ayschronous task after it has been
started and may or may not complete by the time we check that it has finalized.
The pipeline itself is a future whose value has not materialized. Any output
from a pipeline is not actually available until all child pipeline tasks are
executed. So how do we get the final value of the SquarePipeline?&lt;/p&gt;

&lt;h2 id=&#34;finalized:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Finalized&lt;/h2&gt;

&lt;p&gt;The finalized method is called by the pipeline API once a Pipeline has completed
its work (by filling all of is slots &amp;ndash; to be described later). By overriding
the &lt;code&gt;finalized&lt;/code&gt; method we can see the result of our pipeline and do further
processing on that result if necessary. By default our output is set to
&lt;code&gt;self.outputs.default.value&lt;/code&gt;. As an example, executing the following code will
log the message &amp;ldquo;All done! Square is 100&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.default.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will see in a later article how to connect the output of one pipeline with
another.&lt;/p&gt;

&lt;h2 id=&#34;named-outputs:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Named outputs&lt;/h2&gt;

&lt;p&gt;Pipelines also allow you to explicitly name outputs, this is useful in the case
where you have more than one output to return or as a means of passing data
between one pipeline execution and the next. When using named outputs, instead
of returning a value from the &lt;code&gt;run&lt;/code&gt; method we fill a pipeline slot with our
value. To use named outputs we define an &lt;code&gt;output_names&lt;/code&gt; class variable listing
the names of our outputs. By calling &lt;code&gt;self.fill&lt;/code&gt; on our named output we store
the return value of our pipeline for later access in the &lt;code&gt;run&lt;/code&gt; method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):

    output_names = [&#39;square&#39;]

    def run(self, number):
        self.fill(self.outputs.square, number * number)

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.square.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-a-pipeline:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Testing a pipeline&lt;/h2&gt;

&lt;p&gt;Sometimes our pipelines call out over the wire or perform expensive data
operations. The Pipeline API provides a convenient way to test pipelines. By
calling &lt;code&gt;start_test&lt;/code&gt; instead of &lt;code&gt;start&lt;/code&gt;. In our example we verify the
expected output of our squaring pipeline by calling &lt;code&gt;start_test&lt;/code&gt;. The final
value of our pipeline is available immediately.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start_test()
        assert stage.outputs.square.value == 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we need to mock out any behaviour from our &lt;code&gt;run&lt;/code&gt; method, we can supply a
&lt;code&gt;run_test&lt;/code&gt; method that is executed whenever we run our pipeline with
&lt;code&gt;start_test&lt;/code&gt;. Within this method we can mock out or adjust the behaviour of the
pipeline to work under test.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This article gives a basic outline of how to start and execute pipelines. Full
source code for the final example is listed below. In the next article we will
see how to pass the output of one pipeline to another and understand how parent
and child pipelines interact.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):

    output_names = [&#39;square&#39;]

    def run(self, number):
        self.fill(self.outputs.square, number * number)

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.square.value)

routes = [
    webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)
]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Durabledict for App Engine</title>
      <link>http://sookocheff.com/post/appengine/durabledict-for-app-engine/</link>
      <pubDate>Wed, 29 Apr 2015 06:19:23 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/post/appengine/durabledict-for-app-engine/</guid>
      <description>

&lt;h2 id=&#34;tldr:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;tldr;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/soofaloofa/datastoredict&#34;&gt;DatastoreDict&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-s-a-durabledict:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;What&amp;rsquo;s a durabledict?&lt;/h2&gt;

&lt;p&gt;Good question. &lt;a href=&#34;https://github.com/disqus/durabledict&#34;&gt;Durabledict&lt;/a&gt; is a Python
implementation of a persistent dictionary. The dictionary values are cached
locally and sync with the datastore whenever a value in the datastore changes.&lt;/p&gt;

&lt;p&gt;Disqus provides concrete implementations for Redis, Django, ZooKeeper and in
memory. This blog post details an implementation using the App Engine datastore
and memcache.&lt;/p&gt;

&lt;h2 id=&#34;creating-your-own-durabledict:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;Creating your own durabledict&lt;/h2&gt;

&lt;p&gt;By following the &lt;a href=&#34;https://github.com/disqus/durabledict&#34;&gt;guide the durabledict
README&lt;/a&gt; we can create our own
implementation. We need to subclass &lt;code&gt;durabledict.base.DurableDict&lt;/code&gt; and implement
the following interface methods. Strictly speaking, &lt;code&gt;_pop&lt;/code&gt; and &lt;code&gt;_setdefault&lt;/code&gt; do
not have to be implemented but doing so makes your durabledict behave like a
base dict in all cases.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;persist(key, value)&lt;/code&gt; - Persist value at key to your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;depersist(key)&lt;/code&gt; - Delete the value at key from your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;durables()&lt;/code&gt; - Return a key=val dict of all keys in your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;last_updated()&lt;/code&gt; - A comparable value of when the data in your data store was last updated.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_pop(key, default=None)&lt;/code&gt; - If key is in the dictionary, remove it and return its value, else return default. If default is not given and key is not in the dictionary, a KeyError is raised.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_setdefault(key, default=None)&lt;/code&gt; - If key is in the dictionary, return its value. If not, insert key with a value of default and return default. default defaults to None.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s implement these one-by-one.&lt;/p&gt;

&lt;h3 id=&#34;persist-key-value:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;persist(key, value)&lt;/h3&gt;

&lt;p&gt;Persisting a value to the datastore is a relatively simple operation. If the key
already exists we update it&amp;rsquo;s value. If the key does not already exist we create
it. To aid with this operation we create a &lt;code&gt;get_or_create&lt;/code&gt; method that will
return an existing entity if one exists or create a new entity if one does not
exist.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def persist(self, key, val):
    instance, created = get_or_create(self.model, key, val)

    if not created and instance.value != val:
        instance.value = val
        instance.put()

    self.touch_last_updated()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last line of this function updates the last time this durabledict was
changed. This is used for caching. We create the &lt;code&gt;last_updated&lt;/code&gt; and
&lt;code&gt;touch_last_updated&lt;/code&gt; functions now.&lt;/p&gt;

&lt;h3 id=&#34;last-updated-key-value:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;last_updated(key, value)&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def last_updated(self):
    return self.cache.get(self.cache_key)

def touch_last_updated(self):
    self.cache.incr(self.cache_key, initial_value=self.last_synced + 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;init:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;&lt;strong&gt;init&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We now have the building blocks to create our initial durabledict. Within the
&lt;code&gt;__init__&lt;/code&gt; method we set a manager and cache instance. The manager is
responsible for ndb datastore operations to decouple the ndb interface from the
durabledict implementation. We decouple our caching method in a similar fashion.
We also set the initial value of the cache whenever we create a new instance of
the durabledict.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import memcache

from durabledict.base import DurableDict
from durabledict.encoding import NoOpEncoding


class DatastoreDict(DurableDict):

    def __init__(self,
                 model,
                 value_col=&#39;value&#39;,
                 cache=memcache,
                 cache_key=&#39;__DatastoreDict:LastUpdated__&#39;):

        self.model = model
        self.value_col = value_col
        self.cache = cache
        self.cache_key = cache_key

        self.cache.add(self.cache_key, 1)

        super(DatastoreDict, self).__init__(encoding=NoOpEncoding)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;depersist-key:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;depersist(key)&lt;/h3&gt;

&lt;p&gt;Depersist implies deleting a key from the dictionary (and datastore). Here we
assume a helper method &lt;code&gt;delete&lt;/code&gt; that, given an ndb model and a string
representing it&amp;rsquo;s key deletes the model. Since the data has changed we also
update the last touched value to force a cache invalidation and data refresh.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def depersist(self, key):
    delete(self.model, key)
    self.touch_last_updated()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;durables:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;durables()&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;durables()&lt;/code&gt; returns the entire dictionary. Since we are all matching entities
from the datastore it is important to keep your dictionary relatively small &amp;ndash;
as the dictionary grows in size, resyncing it&amp;rsquo;s state with the datastore will
get more and more expensive. This function assumes a &lt;code&gt;get_all&lt;/code&gt; method that will
return all instances of a model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def durables(self):
    encoded_models = get_all(self.model)
    return dict((model.key.id(), getattr(model, self.value_col)) for model in encoded_models)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;setdefault-key-default-none:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;setdefault(key, default=None)&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;_setdefault()&lt;/code&gt; overrides the dictionary built-in &lt;code&gt;setdefault&lt;/code&gt; which allows you
to insert a key into the dictionary, creating the key with the default value if
it does not exist and returning the existing value if it does exist.&lt;/p&gt;

&lt;p&gt;For example, the following sequence of code creates a key for &lt;code&gt;y&lt;/code&gt;, which does not
exist, and returns the existing value for &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; d = {&#39;x&#39;: 1}
&amp;gt;&amp;gt;&amp;gt; d.setdefault(&#39;y&#39;, 2)
2
&amp;gt;&amp;gt;&amp;gt; d
{&#39;y&#39;: 2, &#39;x&#39;: 1}
&amp;gt;&amp;gt;&amp;gt; d.setdefault(&#39;x&#39;, 3)
1
&amp;gt;&amp;gt;&amp;gt; d
{&#39;y&#39;: 2, &#39;x&#39;: 1}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can implement &lt;code&gt;_setdefault&lt;/code&gt; using the &lt;code&gt;get_or_create&lt;/code&gt; helper method, updating
the cache if we have changed the dictionary.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _setdefault(self, key, default=None):
    instance, created = get_or_create(self.model, key, default)

    if created:
        self.touch_last_updated()

    return getattr(instance, self.value_col)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pop-key-default-none:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;pop(key, default=None)&lt;/h3&gt;

&lt;p&gt;pop returns the value for a key and deletes the key. This is fairly straight
forward given a &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; helper method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _pop(self, key, default=None):
    instance = get(self.model, key)
    if instance:
        value = getattr(instance, self.value_col)
        delete(self.model, key)
        self.touch_last_updated()
        return value
    else:
        if default is not None:
            return default
        else:
            raise KeyError
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-help:3359893ce98fc496a5f20da3cde1b0c4&#34;&gt;The Help&lt;/h3&gt;

&lt;p&gt;The previous discussion uses a few helper methods that we haven&amp;rsquo;t defined yet.
Each of these methods takes an arbitrary ndb model and performs an operation on
it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_key(cls, key):
    return ndb.Key(DatastoreDictAncestorModel,
                   DatastoreDictAncestorModel.generate_key(cls).string_id(),
                   cls, key.lower(),
                   namespace=&#39;&#39;)


@ndb.transactional
def get_all(cls):
    return cls.query(
        ancestor=DatastoreDictAncestorModel.generate_key(cls)).fetch()


@ndb.transactional
def get(cls, key):
    return build_key(cls, key).get()


@ndb.transactional
def get_or_create(cls, key, value=None):
    key = build_key(cls, key)

    instance = key.get()
    if instance:
        return instance, False

    instance = cls(key=key, value=value)
    instance.put()

    return instance, True


@ndb.transactional
def delete(cls, key):
    key = build_key(cls, key)
    return key.delete()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last item of note is the use of a parent for each DatastoreDict. This common
ancestor forces strong read consistency for the &lt;code&gt;get_all&lt;/code&gt; method, allowing us to
update a dictionary and have a consistent view of the data on subsequent reads.
We use an additional model to provide the strong read consistency.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DatastoreDictAncestorModel(ndb.Model):

    @classmethod
    def generate_key(cls, child_cls):
        key_name = &#39;__%s-%s__&#39; % (&#39;ancestor&#39;, child_cls.__name__)
        return ndb.Key(cls, key_name, namespace=&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
