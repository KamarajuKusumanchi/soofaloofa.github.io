<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Sookocheff</title>
    <link>http://sookocheff.com/posts/index.xml/</link>
    <language>en-us</language>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <lastBuildDate>Mon, 23 Mar 2015 15:32:37 CST</lastBuildDate>
    
    <item>
      <title>Deploying R Studio on Compute Engine</title>
      <link>http://sookocheff.com/posts/2015-03-30-deploying-r-studio-to-compute-engine/</link>
      <pubDate>Mon, 23 Mar 2015 15:32:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-03-30-deploying-r-studio-to-compute-engine/</guid>
      <description>

&lt;p&gt;Sometimes you have a data analysis problem that is just too big for your desktop
or laptop. The limiting factor here is generally RAM. Thankfully, services like
Google Compute Engine allow you to lease servers with up to 208GB of RAM, large
enough for a wide variety of intensive tasks. An ancillary benefit of using a
service like Compute Engine is that it allows you to easily load your data from
a Cloud Storage Bucket, meaning you don&amp;rsquo;t need to keep a copy of the large
dataset locally at all times.&lt;/p&gt;

&lt;p&gt;R Studio has a remote mode allowing you to install it on a server with access
through a remote interface. This tutorial details how to start a Compute Engine
instance, install R Studio on it and access R Studio from the remote interface.&lt;/p&gt;

&lt;p&gt;The rest of this tutorial assumes that you have a Google Cloud Platform account
with billing enabled and have installed the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud
SDK&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Deploying a Compute Engine Instance&lt;/h2&gt;

&lt;p&gt;The first step is to deploy your Compute Engine instance. The &lt;code&gt;gcloud compute&lt;/code&gt;
command allows you to create instances. The only required parameter to create an
instance is the instance name. We will call our instance &lt;code&gt;r-studio&lt;/code&gt; but you can
choose any name you like. R Studio Server is typically built on Ubuntu so it is
safest to use the Ubuntu distribution for your server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute instances create r-studio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will be prompted to choose a
&lt;a href=&#34;https://cloud.google.com/compute/docs/zones&#34;&gt;Zone&lt;/a&gt;. Just choose a zone close to
you. You can also specify the zone when creating the instance using the &lt;code&gt;--zone&lt;/code&gt;
parameter. For example.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute instances create r-studio --zone us-central1-a
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Installing R Studio&lt;/h2&gt;

&lt;p&gt;Once we have our Compute Engine instance set up, we log in to the machine using ssh.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute ssh r-studio --zone us-central1-a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we are logged in to the Compute Engine instance, it&amp;rsquo;s time to install
R by first updating the Debian apt-get repository and then installing R.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update
sudo apt-get install r-base r-base-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R Studio currently requries OpenSSL version 0.9.8. We need to install this
separately and then install install R Studio&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget http://ftp.us.debian.org/debian/pool/main/o/openssl/libssl0.9.8_0.9.8o-4squeeze14_amd64.deb
sudo dpkg -i libssl0.9.8_0.9.8o-4squeeze14_amd64.deb
sudo apt-get install gdebi-core
wget http://download2.rstudio.org/rstudio-server-0.98.1103-amd64.deb
sudo gdebi rstudio-server-0.98.1103-amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should be up and running with R Studio on your compute engine instance. To
verify, navigate to the IP address of your Compute Engine instance on port 8787
(the default R Studio port).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;http://&amp;lt;ipaddress&amp;gt;:8787
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R Studio only permits access to users of the system, we can add a user with
standard Linux tools like adduser. For example, to create a new user named
rstudio and specify the password you could execute the following commands.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo adduser rstudio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will be prompted to enter a password for the user and confirm the users name
and phone number.&lt;/p&gt;

&lt;p&gt;Afterwards, logging in with the user you created will present a web UI of the
familiar R Studio. You can now perform analysis on those larger data sets using
the R Studio that just weren&amp;rsquo;t possible on a laptop.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a BigQuery Table using the Java Client Library</title>
      <link>http://sookocheff.com/posts/2015-03-23-creating-a-big-query-table-java-api/</link>
      <pubDate>Mon, 23 Mar 2015 15:32:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-03-23-creating-a-big-query-table-java-api/</guid>
      <description>&lt;p&gt;I haven&amp;rsquo;t been able to find great documentation on creating a BigQuery
TableSchema using the Java Client Library. This blog post hopes to rectify that
:).&lt;/p&gt;

&lt;p&gt;You can use the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/bigquery-samples-java&#34;&gt;BigQuery sample
code&lt;/a&gt; for an idea
of how to create a client connection to BigQuery. Assuming you have the
connection set up you can start by creating a new &lt;code&gt;TableSchema&lt;/code&gt;. The
&lt;code&gt;TableSchema&lt;/code&gt; provides a method for setting the list of fields that make up the
columns of your BigQuery Table. Those columns are defined as an Array of
&lt;code&gt;TableFieldSchema&lt;/code&gt; objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;ArrayList&amp;lt;TableFieldSchema&amp;gt; fieldSchema = new ArrayList&amp;lt;TableFieldSchema&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For simple types you can populate your columns with the correct type and mode
according to the &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/v2/tables#resource&#34;&gt;BigQuery API
documentation&lt;/a&gt;.
For example, to create a STRING field that is NULLABLE you can use the
following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(new TableFieldSchema().setName(&amp;quot;username&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;NULLABLE&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And for repeated fields you can use the REPEATED mode.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(new TableFieldSchema().setName(&amp;quot;email&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;REPEATED&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create nested records you specify the parent as a RECORD mode and then call
&lt;code&gt;setFields&lt;/code&gt; for each column of nested data you want to insert. The columns of a
nested type are the same format as for the parent &amp;ndash; a list of TableFieldSchema
objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(
  new TableFieldSchema().setName(&amp;quot;location&amp;quot;).setType(&amp;quot;RECORD&amp;quot;).setFields(
    new ArrayList&amp;lt;TableFieldSchema&amp;gt;() {
      {
        add(new TableFieldSchema().setName(&amp;quot;city&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        add(new TableFieldSchema().setName(&amp;quot;address&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        add(new TableFieldSchema().setName(&amp;quot;zipcode&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
      }
    }
  )
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last step is to set the entire schema as the fields of our table schema.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;TableSchema schema = new TableSchema();
schema.setFields(fieldSchema);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we set a &lt;code&gt;TableReference&lt;/code&gt; that holds the current project id, dataset id and
table id. We use this &lt;code&gt;TableReference&lt;/code&gt; to create our &lt;code&gt;Table&lt;/code&gt; using the &lt;code&gt;TableSchema&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;TableReference ref = new TableReference();
ref.setProjectId(PROJECT_ID);
ref.setDatasetId(&amp;quot;pubsub&amp;quot;);
ref.setTableId(&amp;quot;review_test&amp;quot;);

Table content = new Table();
content.setTableReference(ref);
content.setSchema(schema);

client.tables().insert(ref.getProjectId(), ref.getDatasetId(), content).execute();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together gives you a working sample of creating a BigQuery Table using the Java Client Library.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) throws IOException, InterruptedException {
  Bigquery client = createAuthorizedClient(); // As per the BQ sample code
  
  ArrayList&amp;lt;TableFieldSchema&amp;gt; fieldSchema = new ArrayList&amp;lt;TableFieldSchema&amp;gt;();
  
  fieldSchema.add(new TableFieldSchema().setName(&amp;quot;username&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;NULLABLE&amp;quot;));
  fieldSchema.add(new TableFieldSchema().setName(&amp;quot;email&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;REPEATED&amp;quot;));
  fieldSchema.add(
    new TableFieldSchema().setName(&amp;quot;location&amp;quot;).setType(&amp;quot;RECORD&amp;quot;).setFields(
      new ArrayList&amp;lt;TableFieldSchema&amp;gt;() {
        {
          add(new TableFieldSchema().setName(&amp;quot;city&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
          add(new TableFieldSchema().setName(&amp;quot;address&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
          add(new TableFieldSchema().setName(&amp;quot;zipcode&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        }
  }));
  
  TableSchema schema = new TableSchema();
  schema.setFields(fieldSchema);
  
  TableReference ref = new TableReference();
  ref.setProjectId(&amp;quot;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;quot;);
  ref.setDatasetId(&amp;quot;&amp;lt;YOUR_DATASET_ID&amp;gt;&amp;quot;);
  ref.setTableId(&amp;quot;&amp;lt;YOUR_TABLE_ID&amp;gt;&amp;quot;);
  
  Table content = new Table();
  content.setTableReference(ref);
  content.setSchema(schema);
  
  client.tables().insert(ref.getProjectId(), ref.getDatasetId(), content).execute();
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Keeping App Engine Search Documents and Datastore Entities In Sync</title>
      <link>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</link>
      <pubDate>Mon, 23 Feb 2015 08:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</guid>
      <description>

&lt;p&gt;At Vendasta the App Engine Datastore serves as the single point of truth for
most operational data and the majority of interactions are against this single
point of truth. However, a piece of required functionality in many of our
products is to provide a searchable view of the data in the App Engine
Datastore. Search is difficult using the Datastore and so we have moved to using
the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/search/&#34;&gt;Search API&lt;/a&gt; as a
managed solution for searching datastore entities. In this use case, every edit
to an entity in the Datastore is reflected as a change to a Search Document.
This article details an architecture for keeping Datastore entities and Search
Documents in sync throughout failure and race conditions.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Updating the Search Document Using a _post_put_hook&lt;/h2&gt;

&lt;p&gt;To ensure that every put of an entity to the Datastore results in an update to
the associated search document, we update the search document in the
_post_put_hook of the entity. The _post_put_hook is executed every time
the entity is put so each time the entity has changed we will put a new and
updated search document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def _post_put_hook(self, future):
        document = search.Document(
            doc_id = self.username,
            fields=[
               search.TextField(name=&#39;username&#39;, value=self.username),
               search.TextField(name=&#39;email&#39;, value=self.email),
               ])
        try:
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)
        except search.Error:
            logging.exception(&#39;Put failed&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Updating the search document during every put as part of the post put hook is a
light weight way to keep the search document up-to-date with changes to the
entity. However, this design does not account for the potential error conditions
where putting the search document or the Datastore entity fails. We will need
some additional functionality to handle these cases.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Handling Search Document Put Failures&lt;/h2&gt;

&lt;p&gt;The first obstacle to overcome is handling failures when putting the search
document. One method for handling failures is retrying. We can add retrying to
our workflow by separating updating the search document into its own task and
deferring that task using the deferred library. This accomplishes two things.
First, moving the search document functionality into its own function makes our
code more modular. Second, the App Engine task queue mechanism allows us to
specify our retry semantics, handling backoff and failure conditions gracefully.
In this example, we allow infinite retries of failed tasks, allowing DevOps to find
search documents that may have become out of sync with their Datastore entities
and correct any problems that may arise. We assume in the example below that the
username acts as the ndb Key.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document, self.username)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Handling Datastore Put Failures&lt;/h2&gt;

&lt;p&gt;The second obstacle to overcome is safely handling Datastore put failures. In
this architecture, each change to a Datastore entity is required to run within a
transaction. We update the _post_put_hook to queue a transactional task &amp;ndash; which
forces the task to only be queued if the current transaction has successfully
completed. This guarantees that failed Datastore puts will not result in search
documents being updated and becoming out of sync with the Datastore.&lt;/p&gt;

&lt;p&gt;We specify that the task should be run as a transaction by passing the result of
the &lt;code&gt;in_transaction&lt;/code&gt; function to the &lt;code&gt;_transactional&lt;/code&gt; parameter of &lt;code&gt;defer&lt;/code&gt;.
&lt;code&gt;in_transaction&lt;/code&gt; returns &lt;code&gt;True&lt;/code&gt; if the currently executing code is running in a
transaction and &lt;code&gt;False&lt;/code&gt; otherwise.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have sastisfied the case where either the search document or the
Datastore put has failed. If the search document put has failed we retry, if the
Datastore put has failed we do not put the search document. We still have one
remaining problem: Dirty Reads.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Handling Dirty Reads&lt;/h2&gt;

&lt;p&gt;The last obstacle to overcome is dealing with race conditions that could lead to
reading stale data and writing that data to the search document. Consider the
case where two subsequent puts to the Datastore occur back-to-back within a
short time frame. Each of these puts will write new data to the Datastore and
queue a task to put the updated search document to the Datastore. The dirty
read problem arises when the second task to update the search document reads old
data from the Datastore that may not have been fully replicated throughout the
Datastore.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34;&gt;
  &lt;img src=&#34;/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34; alt=&#34;Syncing Search Documents&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We can overcome this problem by versioning our tasks to coincide with the
version of our Datastore entity. We add a version number to the entity and
update the version number during a _pre_put_hook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    def _pre_put_hook(self):
        self.version = self.version + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now during the _post_put_hook we queue a task corresponding to the version number
of the Datastore entity we are putting. This ties the task to the point in time
when the Datastore entity was put.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    @classmethod
    def put_search_document(cls, username, version):
        model = ndb.Key(cls, username).get()
        if model:
            if version &amp;lt; model.version:
                logging.warning(&#39;Attempting to write stale data. Ignore&#39;)
                return

            if version &amp;gt; model.version:
                msg = &#39;Attempting to write future data. Retry to await consistency.&#39;
                logging.warning(msg)
                raise Exception(msg)

            # Versions match. Update the search document
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=model.username),
                   search.TextField(name=&#39;email&#39;, value=model.email),
                   search.TextField(name=&#39;version&#39;, value=model.version),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _pre_put_hook(self):
        self.version = self.version + 1

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       self.version,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the version number of the task being executed is less than the version number
written to the Datastore, we are attempting to write stale data and do not need
to process this request. If the version number is greater than the task being
executed, we are attempting to write data to the search document that has not
been fully replicated throughout the Datastore. In this case, we raise an
exception to retry putting the search document. In subsequent retries the data
will have propagated and our put will succeed. Note that if another task is
executed while the current task is retrying, the version number of our retrying
task will become stale and when the task is next executed we do not write the
now stale data to the search document.&lt;/p&gt;

&lt;p&gt;This still handles the case when a search document put fails &amp;ndash; whenever our
version number becomes out of sync due to the failed put, we do not write the
data to the search document. Furthermore, if our Datastore put fails then our
task to put the search document will not be queued &lt;em&gt;as long as the Datastore put
is run within a transaction&lt;/em&gt;. The version number will not be incremented in this
case because the value set during the _pre_put_hook will not be persisted during
a failed transaction.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Putting this all together, we&amp;rsquo;ve developed a solution for keeping search
documents in sync with Datastore entities that is robust to failure and race
conditions. This same technique can be used for syncing the state of any number
of datasets that are dependent on the Datastore being the single point of truth
in your system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Halting Python unittest Execution on First Error</title>
      <link>http://sookocheff.com/posts/2015-02-12-halting-unittest-execution-at-first-error/</link>
      <pubDate>Thu, 12 Feb 2015 06:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-12-halting-unittest-execution-at-first-error/</guid>
      <description>&lt;p&gt;We all know the importance of unit tests. Especially in a dynamic language like
Python. Occasionally you have a set of unit tests that are failing in a
cascading fashion where the first error case causes subsequent tests to fail
(these tests are likely no longer unit tests, but that&amp;rsquo;s a different
 discussion). To help isolate the offending test case in a see of failures you
can set the &lt;code&gt;unittest.TestCase&lt;/code&gt; class to halt after the first error by
overriding the &lt;code&gt;run&lt;/code&gt; method as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class HaltingTestCase(unittest.TestCase):

    def run(self, result=None):
        &amp;quot;&amp;quot;&amp;quot; Stop after first error &amp;quot;&amp;quot;&amp;quot;
        if not result.errors:
            super(HaltingTestCase, self).run(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this block of code, if we do not have any errors we call the super class to
continue running tests. If we have an error execution stops after this method
call. This allows you to pinpoint the first error case, fix it, and continue on
fixing subsequent tests.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Create a Google Cloud Dataflow Project with Gradle</title>
      <link>http://sookocheff.com/posts/2015-02-11-cloud-dataflow-with-gradle/</link>
      <pubDate>Wed, 11 Feb 2015 06:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-11-cloud-dataflow-with-gradle/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been experimenting with the &lt;a href=&#34;https://cloud.google.com/dataflow/&#34;&gt;Google Cloud
Dataflow&lt;/a&gt; &lt;a href=&#34;https://github.com/GoogleCloudPlatform/DataflowJavaSDK&#34;&gt;Java
SDK&lt;/a&gt; for running managed
data processing pipelines. One of the first tasks is getting a build environment
up and running. For this I chose Gradle.&lt;/p&gt;

&lt;p&gt;We start by declaring this a java application and listing the configuration
variables that declare the source compatibility level (which for now must be
1.7) and the main class to be executed by the &lt;code&gt;run&lt;/code&gt; task to be defined later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apply plugin: &#39;java&#39;
apply plugin: &#39;application&#39;

sourceCompatibility = &#39;1.7&#39;

mainClassName = &#39;com.sookocheff.dataflow.Main&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then declare the mavenCentral repository where the dependencies are located
and the basic dependencies for a Cloud Dataflow application.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;repositories {
    mavenCentral()
}

dependencies {
    compile &#39;com.google.guava:guava:18.0&#39;
    compile &#39;com.google.cloud.dataflow:google-cloud-dataflow-java-sdk-all:0.3.150109&#39;
    
    testCompile &#39;junit:junit:4.11&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last, we create our run task that will launch the Cloud Dataflow application.
The Cloud Dataflow runtime expects the folder &lt;code&gt;resources/main&lt;/code&gt; to exist in your
build. If you are not actually shipping any resources with your application you
will need to tell Gradle to create the correct directory. We also pass any
parameters to our main class using the -P flag.  These two steps are
encapsulated below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;task resources {
    def resourcesDir = new File(&#39;build/resources/main&#39;)
    resourcesDir.mkdirs()
}

run {
    if (project.hasProperty(&#39;args&#39;)) {
        args project.args.split(&#39;\\s&#39;)
    }
}

run.mustRunAfter &#39;resources&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now be able to launch your Cloud Dataflow application using the
&lt;code&gt;gradle run&lt;/code&gt; task, passing your project identifiers as parameters. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gradle run -Pargs=&amp;quot;--project=&amp;lt;your-project&amp;gt; --runner=BlockingDataflowPipelineRunner --stagingLocation=gs://&amp;lt;staging-location&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A pypiserver Deployment Script</title>
      <link>http://sookocheff.com/posts/2015-02-01-a-pypiserver-deployment-script/</link>
      <pubDate>Sun, 01 Feb 2015 14:53:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-01-a-pypiserver-deployment-script/</guid>
      <description>&lt;p&gt;At Vendasta we&amp;rsquo;ve been slowly adopting pypi and pip for our internal code
libraries and the time has come to deploy our own private pypi server. After
evaluating a few options I settled on the simplistic
&lt;a href=&#34;https://pypi.python.org/pypi/pypiserver&#34;&gt;pypiserver&lt;/a&gt; &amp;ndash; a barebones
implementation of the &lt;a href=&#34;https://pypi.python.org/simple/&#34;&gt;simple HTTP API&lt;/a&gt; to
pypi.&lt;/p&gt;

&lt;p&gt;The deployment uses nginx as a front-end to pypiserver. pypiserver itself is ran
and monitored using supervisord. I created a bash script that creates a user and
group to run pypiserver and installs and runs nginx, supervisord and pypiserver.
I&amp;rsquo;ve been running this bash script through Vagrant to deploy a custom pypiserver
for private use. I wanted to save this code for posterity and hopefully help
someone else working on the same task.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

STARTUP_VERSION=1
STARTUP_MARK=/var/startup.script.$STARTUP_VERSION

# Exit if this script has already ran
if [[ -f $STARTUP_MARK ]]; then
  exit 0  
fi

set -o nounset
set -o pipefail
set -o errexit

# Install prerequesites
sudo apt-get update
sudo apt-get install -y vim
sudo apt-get install -y apache2-utils
sudo apt-get install -y nginx
sudo apt-get install -y supervisor

# Install pip
wget &amp;quot;https://bootstrap.pypa.io/get-pip.py&amp;quot;
sudo python get-pip.py

# Install pypiserver with passlib for upload support
sudo pip install passlib
sudo pip install pypiserver

# Set the port configuration
proxy_port=8080
pypi_port=7201

# Create a user and group to run pypiserver
user=pypiusername
password=pypipasswrd
group=$user

sudo groupadd &amp;quot;$group&amp;quot;
sudo useradd $user -m -g $group -G $group
sudo -u $user -H -s eval &#39;htpasswd -scb $HOME/.htaccess&#39; &amp;quot;$user $password&amp;quot;
sudo -u $user -H -s eval &#39;mkdir -p $HOME/packages&#39;

##############
# nginx config
##############
echo &amp;quot;$user:$(openssl passwd -crypt $password)&amp;quot; &amp;gt; /etc/nginx/user.pwd

# nginx can&#39;t run as a daemon to work with supervisord
echo &amp;quot;daemon off;&amp;quot; &amp;gt;&amp;gt; /etc/nginx/nginx.conf

cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/nginx/sites-enabled/pypi-server.conf
server {
  listen $proxy_port;
  location / {
    proxy_pass http://localhost:$pypi_port;
    auth_basic &amp;quot;PyPi Authentication&amp;quot;;
    auth_basic_user_file /etc/nginx/user.pwd;
  }
}
EOF

rm /etc/nginx/sites-enabled/default

###################
# supervisor config
###################
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/supervisor/conf.d/pypi-server.conf
[program:pypi-server]
command=pypi-server -p $pypi_port -P /home/$user/.htaccess /home/$user/packages
directory=/home/$user
user=$user
autostart=true
autorestart=true
stderr_logfile=/var/log/pypi-server.err.log
stdout_logfile=/var/log/pypi-server.out.log
EOF

cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/supervisor/conf.d/nginx.conf
[program:nginx]
command=/usr/sbin/nginx
autostart=true
autorestart=true
stdout_events_enabled=true
stderr_events_enabled=true
EOF

sudo supervisorctl reread
sudo supervisorctl update

touch $STARTUP_MARK
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Downloading files from Google Cloud Storage with webapp2</title>
      <link>http://sookocheff.com/posts/2015-01-27-webapp2-download-handler/</link>
      <pubDate>Tue, 27 Jan 2015 06:07:12 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-27-webapp2-download-handler/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been working on a simple App Engine application that offers upload and
download functionality to and from Google Cloud Storage. When it came time to
actually download the content I needed to write a webapp2 &lt;code&gt;RequestHandler&lt;/code&gt; that
will retrieve the file from Cloud Storage and return it to the client.&lt;/p&gt;

&lt;p&gt;The trick to this is to set the proper content type in your response header. In
the example below I used the &lt;a href=&#34;(https://cloud.google.com/appengine/docs/python/googlecloudstorageclient/)&#34;&gt;Cloud Storage Client Library&lt;/a&gt; to open and
read the file, then set the response appropriately.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import webapp2
import cloudstorage

class FileDownloadHandler(webapp2.RequestHandler):

  def get(self, filename):
    self.response.headers[&#39;Content-Type&#39;] = &#39;application/x-gzip&#39;
    self.response.headers[&#39;Content-Disposition&#39;] = &#39;attachment; filename=%s&#39; % filename

    filename = &#39;/bucket/&#39; + filename
    gcs_file = cloudstorage.open(filename)
    data = gcs_file.read()
    gcs_file.close()

    self.response.write(data)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Querying App Engine Logs with Elasticsearch</title>
      <link>http://sookocheff.com/posts/2015-01-23-querying-app-engine-logs-with-elasticsearch/</link>
      <pubDate>Fri, 23 Jan 2015 06:15:07 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-23-querying-app-engine-logs-with-elasticsearch/</guid>
      <description>

&lt;p&gt;From a DevOps perspective having a historical record of application logs can aid
immensely in tracking down bugs, responding to customer questions, or finding
out when and why that critical piece of data was updated to the wrong value. One
of the biggest grievances with the built-in log handling of Google App Engine is
that historical logs are only available for the previous three days. We wanted
to do a little bit better and have logs available for a 30 day time period. This
article outlines a method we&amp;rsquo;ve developed for pushing App Engine logs to an
elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;A side benefit of this approach is that if you have multiple App Engine
projects, all of their logs can be searched at the same time. This provides an
immediate benefit when tracking down systems integration problems or parsing API
traffic between applications.&lt;/p&gt;

&lt;p&gt;The solution we chose for this problem revolves around the MapReduce API. If you
need a refresher on this API please check out my &lt;a href=&#34;http://sookocheff.com/series/mapreduce-api/&#34;&gt;MapReduce tutorial
series&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The gist of this solution is to run a MapReduce job that reads data from the
&lt;a href=&#34;https://cloud.google.com/appengine/docs/python/logs/&#34;&gt;App Engine Logs API&lt;/a&gt; using the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/master/python/src/mapreduce/input_readers.py#L1952&#34;&gt;LogInputReader&lt;/a&gt;,
converts the data to a JSON format for ingestion into elasticsearch, and finally
write the parsed data to the elasticsearch cluster using a &lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;custom MapReduce
OutputWriter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We execute this MapReduce job on a timer using cron to push logs to
elasticsearch on a specific schedule. In our case, we run this job every 15
minutes to provide a relatively recent view of current operational data.&lt;/p&gt;

&lt;p&gt;The following diagram presents the architecture of our solution.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2015-01-23-querying-app-engine-logs-with-elasticsearch/elasticsearch-sequence-diagram.png&#34;&gt;
  &lt;img src=&#34;/img/2015-01-23-querying-app-engine-logs-with-elasticsearch/elasticsearch-sequence-diagram.png&#34; alt=&#34;Architecture for Logging to elasticsearch&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;The majority of the solution is contained in a MapperPipeline. The following
code illustrates how to setup the MapperPipeline. What&amp;rsquo;s remaining is to write a
&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;custom MapReduce OutputWriter&lt;/a&gt; that pushes data to
elasticsearch and a function that converts a RequestLog object to JSON suitable
for elasticsearch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class CronHandler(webapp2.RequestHandler):

    def get(self):
        run()


def run():
    start_time, end_time = get_time_range()
    logging.debug(&#39;Dumping logs for date range (%s, %s).&#39;, start_time, end_time)

    start_time = float(start_time.strftime(&#39;%s.%f&#39;))
    end_time = float(end_time.strftime(&#39;%s.%f&#39;))

    p = Log2ElasticSearch(start_time, end_time)
    p.start()


class Log2Elasticsearch(pipeline.Pipeline):

    def run(self, start_time, end_time, module_name, module_versions):
        &amp;quot;&amp;quot;&amp;quot;
        Args:
            module_versions: A list of tuples of the form (module, version), that
                indicate that the logs for the given module/version combination should be
                fetched.  Duplicate tuples will be ignored.
        &amp;quot;&amp;quot;&amp;quot;
        yield mapreduce_pipeline.MapperPipeline(
            &amp;quot;vlogs-elasticsearch-injestion&amp;quot;,
            handler_spec=&amp;quot;log2json&amp;quot;,
            input_reader_spec=&amp;quot;mapreduce.input_readers.LogInputReader&amp;quot;,
            output_writer_spec=&amp;quot;mapreduce.output_writers.ElasticSearchOutputWriter&amp;quot;,
            params={
                &amp;quot;input_reader&amp;quot;: {
                    &amp;quot;start_time&amp;quot;: start_time,
                    &amp;quot;end_time&amp;quot;: end_time,
                    &amp;quot;include_app_logs&amp;quot;: True,
                },
            },
            shards=16
        )
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Parsing bash script options with getopts</title>
      <link>http://sookocheff.com/posts/2015-01-04-parsing-bash-script-arguments-with-shopts/</link>
      <pubDate>Sun, 04 Jan 2015 12:31:51 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-04-parsing-bash-script-arguments-with-shopts/</guid>
      <description>

&lt;p&gt;A common task in shell scripting is to parse command line arguments to your
script. Bash provides the &lt;code&gt;getopts&lt;/code&gt; built-in function to do just that. This
tutorial explains how to use the &lt;code&gt;getopts&lt;/code&gt; built-in function to parse arguments and options to a bash script.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;getopts&lt;/code&gt; function takes three parameters. The first is a specification of
which options are valid, listed as a sequence of letters. For example, the
string &lt;code&gt;&#39;ht&#39;&lt;/code&gt; signifies that the options &lt;code&gt;-h&lt;/code&gt; and &lt;code&gt;-t&lt;/code&gt; are valid.&lt;/p&gt;

&lt;p&gt;The second argument to &lt;code&gt;getopts&lt;/code&gt; is a variable that will be populated with the
option or argument to be processed next. In the following loop, &lt;code&gt;opt&lt;/code&gt; will hold
the value of the current option that has been parsed by &lt;code&gt;getopts&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:ht&amp;quot; opt; do
  case ${opt} in
    h ) # process option a
      ;;
    t ) # process option l
      ;;
    \? ) echo &amp;quot;Usage: cmd [-h] [-t]
      ;;
  esac
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example shows a few additional features of &lt;code&gt;getopts&lt;/code&gt;. First, if an invalid
option is provided, the option variable is assigned the value &lt;code&gt;?&lt;/code&gt;. You can catch
this case and provide an appropriate usage message to the user. Second, this
behaviour is only true when you prepend the list of valid options with &lt;code&gt;:&lt;/code&gt; to
disable the default error handling of invalid options. It is recommended to
always disable the default error handling in your scripts.&lt;/p&gt;

&lt;p&gt;The third argument to &lt;code&gt;getopts&lt;/code&gt; is the list of arguments and options to be
processed. When not provided, this defaults to the arguments and options
provided to the application (&lt;code&gt;$@&lt;/code&gt;). You can provide this third argument to use
&lt;code&gt;getopts&lt;/code&gt; to parse any list of arguments and options you provide.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Shifting processed options&lt;/h2&gt;

&lt;p&gt;The variable &lt;code&gt;OPTIND&lt;/code&gt; holds the number of options parsed by the last call to
&lt;code&gt;getopts&lt;/code&gt;. It is common practice to call the &lt;code&gt;shift&lt;/code&gt; command at the end of your
processing loop to remove options that have already been handled from &lt;code&gt;$@&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Parsing options with arguments&lt;/h2&gt;

&lt;p&gt;Options that themselves have arguments are signified with a &lt;code&gt;:&lt;/code&gt;. The argument to
an option is placed in the variable &lt;code&gt;OPTARG&lt;/code&gt;. In the following example, the
option &lt;code&gt;t&lt;/code&gt; takes an argument. When the argument is provided, we copy its value
to the variable &lt;code&gt;target&lt;/code&gt;. If no argument is provided &lt;code&gt;getopts&lt;/code&gt; will set &lt;code&gt;opt&lt;/code&gt; to
&lt;code&gt;:&lt;/code&gt;. We can recognize this error condition by catching the &lt;code&gt;:&lt;/code&gt; case and printing
an appropriate error message.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:t:&amp;quot; opt; do
  case ${opt} in 
    t )
      target=$OPTARG
      ;;
    \? )
      echo &amp;quot;Invalid option: $OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
      ;;
    : )
      echo &amp;quot;Invalid option: $OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
      ;;
  esac
done
shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;An extended example &amp;ndash; parsing nested arguments and options&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s walk through an extended example of processing a command that takes
options, has a sub-command, and whose sub-command takes an additional option
that has an argument. This is a mouthful so let&amp;rsquo;s break it down using an
example. Let&amp;rsquo;s say we are writing our own version of the &lt;a href=&#34;https://pip.pypa.io/en/latest/&#34;&gt;&lt;code&gt;pip&lt;/code&gt;
command&lt;/a&gt;. In this version you can call &lt;code&gt;pip&lt;/code&gt;
with the &lt;code&gt;-h&lt;/code&gt; option to display a help message.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip -h
Usage: 
    pip -h                      Display this help message.
    pip install                 Install a Python package.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use &lt;code&gt;getopts&lt;/code&gt; to parse the &lt;code&gt;-h&lt;/code&gt; option with the following &lt;code&gt;while&lt;/code&gt; loop.
In it we catch invalid options with &lt;code&gt;\?&lt;/code&gt; and &lt;code&gt;shift&lt;/code&gt; all arguments that have
been processed with &lt;code&gt;shift $((OPTIND -1))&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:h&amp;quot; opt; do
  case ${opt} in
    h )
      echo &amp;quot;Usage:&amp;quot;
      echo &amp;quot;    pip -h                      Display this help message.&amp;quot;
      echo &amp;quot;    pip install                 Install a Python package.&amp;quot;
      exit 0
      ;;
    \? )
      echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
      exit 1
      ;;
  esac
done
shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s add the sub-command &lt;code&gt;install&lt;/code&gt; to our script.  &lt;code&gt;install&lt;/code&gt; takes as an
argument the Python package to install.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip install urllib3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;install&lt;/code&gt; also takes an option, &lt;code&gt;-t&lt;/code&gt;. &lt;code&gt;-t&lt;/code&gt; takes as an argument the location to
install the package to relative to the current directory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip install urllib3 -t ./src/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To process this line we must find the sub-command to execute. This value is the
first argument to our script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;subcommand=$1
shift # Remove `pip` from the argument list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can process the sub-command &lt;code&gt;install&lt;/code&gt;. In our example, the option &lt;code&gt;-t&lt;/code&gt; is
actually an option that follows the package argument so we begin by removing
&lt;code&gt;install&lt;/code&gt; from the argument list and processing the remainder of the line.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;case &amp;quot;$subcommand&amp;quot; in
  install)
    package=$1
    shift # Remove `install` from the argument list
    ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After shifting the argument list we can process the remaining arguments as if
they are of the form &lt;code&gt;package -t src/lib&lt;/code&gt;. The &lt;code&gt;-t&lt;/code&gt; option takes an argument
itself. This argument will be stored in the variable &lt;code&gt;OPTARG&lt;/code&gt; and we save it to
the variable &lt;code&gt;target&lt;/code&gt; for further work.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;case &amp;quot;$subcommand&amp;quot; in
  install)
    package=$1
    shift # Remove `install` from the argument list

  while getopts &amp;quot;:t:&amp;quot; opt; do
    case ${opt} in
      t )
        target=$OPTARG
        ;;
      \? )
        echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
        exit 1
        ;;
      : )
        echo &amp;quot;Invalid Option: -$OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
        exit 1
        ;;
    esac
  done
  shift $((OPTIND -1))
  ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together, we end up with the following script that parses
arguments to our version of &lt;code&gt;pip&lt;/code&gt; and its sub-command &lt;code&gt;install&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;package=&amp;quot;&amp;quot;  # Default to empty package
target=&amp;quot;&amp;quot;  # Default to empty target

# Parse options to the `pip` command
while getopts &amp;quot;:h&amp;quot; opt; do
  case ${opt} in
    h )
      echo &amp;quot;Usage:&amp;quot;
      echo &amp;quot;    pip -h                      Display this help message.&amp;quot;
      echo &amp;quot;    pip install &amp;lt;package&amp;gt;       Install &amp;lt;package&amp;gt;.&amp;quot;
      exit 0
      ;;
   \? )
     echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
     exit 1
     ;;
  esac
done
shift $((OPTIND -1))

subcommand=$1; shift  # Remove &#39;pip&#39; from the argument list
case &amp;quot;$subcommand&amp;quot; in
  # Parse options to the install sub command
  install)
    package=$1; shift  # Remove &#39;install&#39; from the argument list

    # Process package options
    while getopts &amp;quot;:t:&amp;quot; opt; do
      case ${opt} in
        t )
          target=$OPTARG
          ;;
        \? )
          echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
          exit 1
          ;;
        : )
          echo &amp;quot;Invalid Option: -$OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
          exit 1
          ;;
      esac
    done
    shift $((OPTIND -1))
    ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After processing the above sequence of commands, the variable &lt;code&gt;package&lt;/code&gt; will
hold the package to install and the variable &lt;code&gt;target&lt;/code&gt; will hold the target to
install the package to. You can use this as a template for processing any set of
arguments and options to your scripts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing App Engine Dependencies Using pip</title>
      <link>http://sookocheff.com/posts/2014-12-40-managing-app-engine-dependencies-with-pip/</link>
      <pubDate>Tue, 30 Dec 2014 20:35:48 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-40-managing-app-engine-dependencies-with-pip/</guid>
      <description>&lt;p&gt;One unfortunate difficulty when working with App Engine is managing your local
dependencies. You don&amp;rsquo;t have access to your Python environment so all libraries
you wish to use must be &lt;em&gt;vendored&lt;/em&gt; with your installation. That is, you need to
copy all of your library code into a local folder to ship along with your app.&lt;/p&gt;

&lt;p&gt;This usually doesn&amp;rsquo;t cause any problems but difficulties start to crop up when
you manage multiple dependencies that rely on each other. For example, the
official &lt;a href=&#34;https://github.com/elasticsearch/elasticsearch-py&#34;&gt;elasticsearch
client&lt;/a&gt; requires
&lt;a href=&#34;https://github.com/shazow/urllib3&#34;&gt;urllib3&lt;/a&gt; between version &lt;code&gt;1.8&lt;/code&gt; and &lt;code&gt;2.0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Traditionally, &lt;a href=&#34;https://pip.pypa.io/en/latest/&#34;&gt;pip&lt;/a&gt; is used to install these
dependencies on your behalf. The command &lt;code&gt;pip install elasticsearch&lt;/code&gt; will
automatically fetch the urllib3 dependency for you and install it to your local
Python environment. By adding the &lt;code&gt;-t&lt;/code&gt; flag you can provide a destination folder
to install your libraries. As an example, we can install the elasticsearch
and urllib3 libraries to the folder &lt;code&gt;src/lib&lt;/code&gt; with the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install elasticsearch -t src/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works great for App Engine which requires the source of your libraries to
be shipped with your application. Unfortunately, it starts to break down when
you need to upgrade your dependencies. Installing with the &lt;code&gt;-t&lt;/code&gt; flag does not
overwrite the contents of the existing folder so running the same command again
results in an error.&lt;/p&gt;

&lt;p&gt;A solution to this can be found with some basic shell scripting. The first portion of our script installs the requested package and it&amp;rsquo;s
dependencies to a temporary directory and removes any extra files that we don&amp;rsquo;t
need.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install elasticsearch -t $TEMP_DIRECTORY
rm -r $TEMP_DIRECTORY/*.egg-info &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
rm -r $TEMP_DIRECTORY/*.dist-info &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to remove the specific libraries being installed from our App
Engine library directory and copy the contents of our temporary directory in their place.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TARGET=src/lib
for i in $(ls $TEMP_DIRECTORY); do
  rm -r $TARGET/$i &amp;gt;/dev/null 2&amp;gt;&amp;amp;1  # remove existing module
  cp -R $TEMP_DIRECTORY/$i $TARGET  # copy the replacement in
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code can be used as a starting point to write a more user friendly and
robust script. Although this does not truly solve the problem of dependency
management with App Engine it does provide a way to seamlessly vendor Python
libraries and all of their dependencies with your application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine MapReduce API - Part 7: Writing a Custom Output Writer</title>
      <link>http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/</link>
      <pubDate>Mon, 22 Dec 2014 07:07:35 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/</guid>
      <description>

&lt;h2 id=&#34;toc_0&#34;&gt;MapReduce API Series&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-15-app-engine-mapreduce-api-part-1-the-basics/&#34;&gt;Part 1: The Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-22-app-engine-mapreduce-api-part-2-running-a-mapreduce-job-using-mapreduceyaml/&#34;&gt;Part 2: Running a MapReduce Job Using mapreduce.yaml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;Part 3: Programmatic MapReduce using Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-13-app-engine-mapreduce-api-part-4-combining-sequential-mapreduce-jobs/&#34;&gt;Part 4: Combining Sequential MapReduce Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput/&#34;&gt;Part 5: Using Combiners to Reduce Data Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/&#34;&gt;Part 6: Writing a Custom Input Reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;Part 7: Writing a Custom Output Writer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The MapReduce library supports a number of default output writers. You can also
write your own that implements the output writer interface. This article
examines how to write a custom output writer that pushes data from the App
Engine datastore to an elasticsearch cluster. A similar pattern can be followed
to push the output from your MapReduce job to any number of places.&lt;/p&gt;

&lt;p&gt;An output writer must implement the abstract interface defined by the MapReduce
library. You can find the interface
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/a1844a2652d51c3bef4448c9265c7c5790c9e476/python/src/mapreduce/output_writers.py#L95&#34;&gt;here&lt;/a&gt;.
It may be a good idea to keep a reference to that interface available while
reading this article.&lt;/p&gt;

&lt;p&gt;The most important methods of the interface are &lt;code&gt;create&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.  &lt;code&gt;create&lt;/code&gt;
is used to create a new OutputWriter that will handle writing for a single
shard. Our elasiticsearch OutputWriter takes parameters specifying the
elasticsearch index to write to and the document type. We take advantage of a
helper function provided by the library (&lt;code&gt;_get_params&lt;/code&gt;) to get the parameters of
a MapReduce job given the MapReduce specification.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mapreduce.output_writers import OutputWriter, _get_params

class ElasticSearchOutputWriter(OutputWriter):

    def __init__(self, default_index_name=None, default_doc_type=None):
        super(ElasticSearchOutputWriter, self).__init__()
        self.default_index_name = default_index_name
        self.default_doc_type = default_doc_type
        
    @classmethod
    def create(cls, mr_spec, shard_number, shard_attempt, _writer_state=None):
        params = _get_params(mr_spec)
        return cls(default_index_name=params.get(&#39;default_index_name&#39;,
                   default_doc_type=params.get(&#39;default_doc_type&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we can create an instance of our OutputWriter we can implement the
&lt;code&gt;write&lt;/code&gt; method to write data to elasticsearch. We use a MutationPool for this
(the MutationPool itself will be discussed shortly). The MutationPool is
attached to the current execution context of this MapReduce job. Every MapReduce
job has it&amp;rsquo;s own persistent context that can store information required for the
current execution of the job. This allows multiple OutputWriter shards to write
into the MutationPool and have the MutationPool write data out to its final
destination.&lt;/p&gt;

&lt;p&gt;In this piece of code we check if we have a MutationPool associated with our
context and create a new MutationPool if we don&amp;rsquo;t.  Once we&amp;rsquo;ve retrieved or
created the MutationPool we add the output operation to the pool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mapreduce import context

def write(self, data):
   ctx = context.get()
   es_pool = ctx.get_pool(&#39;elasticsearch_pool&#39;)
   if not es_pool:
       es_pool = _ElasticSearchPool(ctx=ctx,
                                    default_index_name=default_index_name,
                                    default_doc_type=default_doc_type)
       ctx.register_pool(&#39;elasticsearch_pool&#39;, es_pool)

   es_pool.append(data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two methods provide the basis of our OutputWriter, implementing the
&lt;code&gt;to_json&lt;/code&gt;, &lt;code&gt;from_json&lt;/code&gt; and &lt;code&gt;finalize&lt;/code&gt; methods is left up to the reader.
&lt;code&gt;finalize&lt;/code&gt; does not need any functionality but you may want to log a message
upon completion.&lt;/p&gt;

&lt;p&gt;Now on to the MutationPool. The MutationPool acts as a buffered writer of data
changes. It acts as an abstraction that collects any sequence of operations that
are to be performed together. After &lt;code&gt;x&lt;/code&gt; number of operations have been collected
we operate on them all at once.  Mutation pools are strictly a performance
improvement but they can quickly become essential when processing large amounts
of data. For example, rather than writing to the datastore after each map
operation with &lt;code&gt;ndb.put&lt;/code&gt; we can collect a sequence of writes and put them all at
once with &lt;code&gt;ndb.put_multi&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For an &lt;code&gt;elasticsearch&lt;/code&gt; OutputWriter our mutation pool will collect and buffer
indexing tasks and perform them all during a single &lt;a href=&#34;http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/bulk.html&#34;&gt;streaming
bulk&lt;/a&gt;
operation. Within our OutputWriter we collect our sequence of operations in a
private list variable &lt;code&gt;_actions&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class _ElasticSearchPool(context.Pool):
    def __init__(self, ctx=None, default_index_name=None, default_doc_type=None):
        self._actions = []
        self._size = 0
        self._ctx = ctx
        self.default_index_name = default_index_name
        self.default_doc_type = default_doc_type
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then implement the &lt;code&gt;append&lt;/code&gt; method to add an action to the current
MutationPool. In this example we simply add the action to our list. If our list
is greater than &lt;code&gt;200&lt;/code&gt; elements we flush our MutationPool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def append(self, action):
    self._actions.append(action)
    self._size += 1
    if self._size &amp;gt; 200:
        self.flush()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, to flush the MutationPool we write all the data collected so far to
elasticsearch and clear our list of actions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def flush(self):
   es_client = elasticsearch(hosts=[&amp;quot;127.0.0.1&amp;quot;])  # instantiate elasticsearch client
   if self._actions:
       results = helpers.streaming_bulk(es_client,
                                                                   self._actions,
                                                                   chunk_size=200)
    self._actions = []
    self._size = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, as long as the map function of our MapReduce job outputs operations in a
format recognizeable by elasticsearch the OutputWriter will collect those
operations into a MutationPool and periodically flush the results to our
elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;You can use this code as the basis for writing OutputWriters for almost any
custom destination.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Bash String Operators</title>
      <link>http://sookocheff.com/posts/2014-12-11-bash-string-operators/</link>
      <pubDate>Thu, 11 Dec 2014 19:54:12 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-11-bash-string-operators/</guid>
      <description>

&lt;p&gt;A common task in &lt;em&gt;bash&lt;/em&gt; programming is to manipulate portions of a string and
return the result. &lt;em&gt;bash&lt;/em&gt; provides rich support for these manipulations via
string operators. The syntax is not always intuitive so I wanted to use this
blog post to serve as a permanent reminder of the operators.&lt;/p&gt;

&lt;p&gt;The string operators are signified with the &lt;code&gt;${}&lt;/code&gt; notation. The operations can be
grouped in to a few classes. Each heading in this article describes a class of
operation.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Substring Extraction&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Extract from a position&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${string:position}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Extraction returns a substring of &lt;code&gt;string&lt;/code&gt; starting at &lt;code&gt;position&lt;/code&gt; and ending at the end of &lt;code&gt;string&lt;/code&gt;. &lt;code&gt;string&lt;/code&gt; is treated as an array of characters starting at 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world&amp;quot;
&amp;gt; echo ${string:1}
ello world
&amp;gt; echo ${string:6}
world
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Extract from a position with a length&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${string:position:length}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Adding a length returns a substring only as long as the &lt;code&gt;length&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world&amp;quot;
&amp;gt; echo ${string:1:2}
el
&amp;gt; echo ${string:6:3}
wor
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Substring Removal&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Remove shortest starting match&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable#pattern}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;variable&lt;/code&gt; &lt;em&gt;starts&lt;/em&gt; with &lt;code&gt;pattern&lt;/code&gt;, delete the &lt;em&gt;shortest&lt;/em&gt; part that matches the pattern.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string#*hello}
world, hello jim
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;Remove longest starting match&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable##pattern}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;variable&lt;/code&gt; &lt;em&gt;starts&lt;/em&gt; with &lt;code&gt;pattern&lt;/code&gt;, delete the &lt;em&gt;longest&lt;/em&gt; match from &lt;code&gt;variable&lt;/code&gt; and return the rest.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string##*hello}
jim
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;Remove shortest ending match&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable%pattern}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;variable&lt;/code&gt; ends with &lt;code&gt;pattern&lt;/code&gt;, delete the shortest match from the end of &lt;code&gt;variable&lt;/code&gt; and return the rest.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string%hello*}
hello world,
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;Remove longest ending match&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable%%pattern}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;variable&lt;/code&gt; ends with &lt;code&gt;pattern&lt;/code&gt;, delete the longest match from the end of &lt;code&gt;variable&lt;/code&gt; and return the rest.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string%%hello*}

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;Substring Replacement&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_9&#34;&gt;Replace first occurrence of word&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable/pattern/string}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Find the first occurrence of &lt;code&gt;pattern&lt;/code&gt; in &lt;code&gt;variable&lt;/code&gt; and replace it with &lt;code&gt;string&lt;/code&gt;. If &lt;code&gt;string&lt;/code&gt; is null, &lt;code&gt;pattern&lt;/code&gt; is deleted from &lt;code&gt;variable&lt;/code&gt;. If &lt;code&gt;pattern&lt;/code&gt; starts with &lt;code&gt;#&lt;/code&gt;, the match must occur at the beginning of &lt;code&gt;variable&lt;/code&gt;. If &lt;code&gt;pattern&lt;/code&gt; starts with &lt;code&gt;%&lt;/code&gt;, the match must occur at the end of the &lt;code&gt;variable&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string/hello/goodbye}
goodbye world, hello jim
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_10&#34;&gt;Replace all occurrences of word&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable//pattern/string}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Same as above but finds &lt;strong&gt;all&lt;/strong&gt; occurrences of &lt;code&gt;pattern&lt;/code&gt; in &lt;code&gt;variable&lt;/code&gt; and replace them with &lt;code&gt;string&lt;/code&gt;. If &lt;code&gt;string&lt;/code&gt; is null, &lt;code&gt;pattern&lt;/code&gt; is deleted from &lt;code&gt;variable&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string//hello/goodbye}
goodbye world, goodbye jim
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine MapReduce API - Part 6: Writing a Custom Input Reader</title>
      <link>http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/</link>
      <pubDate>Thu, 04 Dec 2014 22:54:12 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/</guid>
      <description>

&lt;h2 id=&#34;toc_0&#34;&gt;MapReduce API Series&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-15-app-engine-mapreduce-api-part-1-the-basics/&#34;&gt;Part 1: The Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-22-app-engine-mapreduce-api-part-2-running-a-mapreduce-job-using-mapreduceyaml/&#34;&gt;Part 2: Running a MapReduce Job Using mapreduce.yaml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;Part 3: Programmatic MapReduce using Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-13-app-engine-mapreduce-api-part-4-combining-sequential-mapreduce-jobs/&#34;&gt;Part 4: Combining Sequential MapReduce Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput/&#34;&gt;Part 5: Using Combiners to Reduce Data Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/&#34;&gt;Part 6: Writing a Custom Input Reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;Part 7: Writing a Custom Output Writer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the great things about the MapReduce library is the abilitiy to write a
cutom InputReader to process data from any data source. In this post we will
explore how to write an InputReader the leases tasks from an AppEngine pull
queue by implementing the &lt;code&gt;InputReader&lt;/code&gt; interface.&lt;/p&gt;

&lt;p&gt;The interface we need to implement is available at
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/master/python/src/mapreduce/input_readers.py#L119&#34;&gt;&lt;code&gt;mapreduce.input_readers.InputReader&lt;/code&gt;&lt;/a&gt;.
Take a minute to examine the abstract methods that need to be implmemented.
Relevant portions of the source are copied below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class InputReader(json_util.JsonMixin):
  &amp;quot;&amp;quot;&amp;quot;Abstract base class for input readers.
  InputReaders have the following properties:
   * They are created by using the split_input method to generate a set of
     InputReaders from a MapperSpec.
   * They generate inputs to the mapper via the iterator interface.
   * After creation, they can be serialized and resumed using the JsonMixin
     interface.
  &amp;quot;&amp;quot;&amp;quot;

  def next(self):
    &amp;quot;&amp;quot;&amp;quot;Returns the next input from this input reader as a key, value pair.
    Returns:
      The next input from this input reader.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;next() not implemented in %s&amp;quot; % self.__class__)

  @classmethod
  def from_json(cls, input_shard_state):
    &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.
    Args:
      input_shard_state: The InputReader state as a dict-like object.
    Returns:
      An instance of the InputReader configured using the values of json.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;from_json() not implemented in %s&amp;quot; % cls)

  def to_json(self):
    &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.
    Returns:
      A json-izable version of the remaining InputReader.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;to_json() not implemented in %s&amp;quot; %
                              self.__class__)

  @classmethod
  def split_input(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;Returns a list of input readers.
    This method creates a list of input readers, each for one shard.
    It attempts to split inputs among readers evenly.
    Args:
      mapper_spec: model.MapperSpec specifies the inputs and additional
        parameters to define the behavior of input readers.
    Returns:
      A list of InputReaders. None or [] when no input data can be found.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;split_input() not implemented in %s&amp;quot; % cls)

  @classmethod
  def validate(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;Validates mapper spec and all mapper parameters.
    Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
    subdictionary in mapper_spec.params.
    Pre 1.6.4 API mixes input reader parameters with all other parameters. Thus
    to be compatible, input reader check mapper_spec.params as well and
    issue a warning if &amp;quot;input_reader&amp;quot; subdicationary is not present.
    Args:
      mapper_spec: The MapperSpec for this InputReader.
    Raises:
      BadReaderParamsError: required parameters are missing or invalid.
    &amp;quot;&amp;quot;&amp;quot;
    if mapper_spec.input_reader_class() != cls:
      raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s fill out this interface with our InputReader that leases tasks from an
AppEngine pull queue. To start, we implement the &lt;code&gt;split_input&lt;/code&gt; method that
instantiates a list of InputReaders, splitting the work among each reader. One
of the standard parameters for a MapReduce job is the number of shards you want
to use. For leasing tasks we will create one InputReader for shard
parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def split_input(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;
    Returns a list of input readers
    &amp;quot;&amp;quot;&amp;quot;
    shard_count = mapper_spec.shard_count

    return [cls()] * shard_count
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;split_input&lt;/code&gt; is called to start our InputReader and returns a list of readers.
Each of these reader instances must implement a the &lt;code&gt;next&lt;/code&gt; method which returns
a single value from our Reader. This method is part of the generator interface
and will be called during MapReduce operation. We can use &lt;code&gt;next&lt;/code&gt; to attempt to lease
a single task from our queue, returning the task as a key-value tuple.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def next(self):
    &amp;quot;&amp;quot;&amp;quot;
    Returns the queue, and a task leased from it as a tuple
    Returns:
      The next input from this input reader.
    &amp;quot;&amp;quot;&amp;quot;
    ctx = context.get()
    input_reader_params = ctx.mapreduce_spec.mapper.params.get(&#39;input_reader&#39;, {})
    queue_name = input_reader_params.get(self.QUEUE_PARAM)
    tag = input_reader_params.get(self.TAG_PARAM)
    lease_seconds = input_reader_params.get(self.LEASE_SECONDS_PARAM, 60)

    # Attempt to lease a task
    queue = taskqueue.Queue(queue_name)
    if tag:
        tasks = queue.lease_tasks_by_tag(lease_seconds, 1, tag=tag)
    else:
        tasks = queue.lease_tasks(lease_seconds, 1)

    if tasks:
        operation.counters.Increment(self.TASKS_LEASED_COUNTER)(ctx)
        return (queue, tasks[0])
    raise StopIteration()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We begin this function by reading in our parameters, using the context helper to
find the current parameters for this InputReder. We then attempt to lease a
task. If tasks are available to lease we return the task, otherwise we raise
&lt;code&gt;StopIteration&lt;/code&gt; to halt the generator.&lt;/p&gt;

&lt;p&gt;This basic implementation is all that&amp;rsquo;s needed to write an InputReader &amp;ndash; split
our source into multiple shards and return a single &lt;code&gt;next&lt;/code&gt; value from within
each shard. The MapReduce library will use this skeleton to call your &lt;code&gt;map&lt;/code&gt;
function for each &lt;code&gt;next&lt;/code&gt; value that is returned by the input reader.&lt;/p&gt;

&lt;p&gt;To finish this up, we add some boilerplate required for serialization of reader
state and parameter validation.&lt;/p&gt;

&lt;p&gt;If your InputReader needs to hold any state between execution of the &lt;code&gt;next&lt;/code&gt;
method you must serialize that state using the &lt;code&gt;to_json&lt;/code&gt; and &lt;code&gt;from_json&lt;/code&gt;
methods. &lt;code&gt;to_json&lt;/code&gt; returns the current state of the reader in JSON format.
&lt;code&gt;from_json&lt;/code&gt; creates an instance of an InputReader given a JSON format. Typically
we use this to save the constructor values used to create our InputReader. We&amp;rsquo;ll
also need to formally define our constructor here.&lt;/p&gt;

&lt;p&gt;The constructor takes only a few parameters. A queue name, a tag to lease tasks
with and the number of seconds to hold the lease.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __init__(self, queue_name=&#39;default&#39;, tag=None, lease_seconds=60):
    super(TaskInputReader, self).__init__()
    self.queue_name = queue_name
    self.tag = tag
    self.lease_seconds = lease_seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can define how to serialize and deserialize the state of our reader.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def from_json(cls, input_shard_state):
    &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.
    Args:
      input_shard_state: The InputReader state as a dict-like object.
    Returns:
      An instance of the InputReader configured using the values of json.
    &amp;quot;&amp;quot;&amp;quot;
    return cls(input_shard_state.get(&#39;queue_name&#39;),
               input_shard_state.get(&#39;tag&#39;),
               input_shard_state.get(&#39;lease_seconds&#39;)))

def to_json(self):
    &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.
    Returns:
      A json-izable version of the remaining InputReader.
    &amp;quot;&amp;quot;&amp;quot;
    return {
        &#39;queue_name&#39;: self.queue_name,
        &#39;tag&#39;: self.tag,
        &#39;lease_seconds&#39;: self.lease_seconds,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last method to implement is &lt;code&gt;validate&lt;/code&gt;. This method parses the parameters
used to start your InputReader to make sure they are valid. In our example we
validate that the &lt;code&gt;queue_name&lt;/code&gt; we are attempting to lease tasks from is valid
and that the number of seconds we wish to lease is an integer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def validate(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;
    Validates mapper spec and all mapper parameters.
    Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
    subdictionary in mapper_spec.params.
    Args:
      mapper_spec: The MapperSpec for this InputReader.
    Raises:
      BadReaderParamsError: required parameters are missing or invalid.
    &amp;quot;&amp;quot;&amp;quot;
    if mapper_spec.input_reader_class() != cls:
        raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)

    # Check that a valid queue is specified
    input_reader_params = mapper_spec.params.get(&#39;input_reader&#39;, {})
    queue_name = input_reader_params.get(&#39;queue_name&#39;)
    lease_seconds = input_reader_params.get(&#39;lease_seconds&#39;, 60)
    if not queue_name:
        raise BadReaderParamsError(&#39;queue_name is required&#39;)
    if not isinstance(lease_seconds, int):
        raise BadReaderParamsError(&#39;lease_seconds must be an integer&#39;)
    try:
        queue = taskqueue.Queue(name=queue_name)
        queue.fetch_statistics()
    except Exception as e:
        raise BadReaderParamsError(&#39;queue_name is invalid&#39;, e.message)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together we get our final InputReader. We can use this as a
basis to make more complex readers for additional data sources.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot;
TaskInputReader
&amp;quot;&amp;quot;&amp;quot;
from google.appengine.api import taskqueue

from mapreduce.input_readers import InputReader
from mapreduce.errors import BadReaderParamsError
from mapreduce import context
from mapreduce import operation


class TaskInputReader(InputReader):
    &amp;quot;&amp;quot;&amp;quot;
    Input reader for Pull-queue tasks
    &amp;quot;&amp;quot;&amp;quot;

    QUEUE_PARAM = &#39;queue&#39;
    TAG_PARAM = &#39;tag&#39;
    LEASE_SECONDS_PARAM = &#39;lease-seconds&#39;

    TASKS_LEASED_COUNTER = &#39;tasks leased&#39;

    def next(self):
        &amp;quot;&amp;quot;&amp;quot;
        Returns the queue, and a task leased from it as a tuple

        Returns:
          The next input from this input reader.
        &amp;quot;&amp;quot;&amp;quot;
        ctx = context.get()
        input_reader_params = ctx.mapreduce_spec.mapper.params.get(&#39;input_reader&#39;, {})
        queue_name = input_reader_params.get(self.QUEUE_PARAM)
        tag = input_reader_params.get(self.TAG_PARAM)
        lease_seconds = input_reader_params.get(self.LEASE_SECONDS_PARAM, 60)

        # Attempt to lease a task
        queue = taskqueue.Queue(queue_name)
        if tag:
            tasks = queue.lease_tasks_by_tag(lease_seconds, 1, tag=tag)
        else:
            tasks = queue.lease_tasks(lease_seconds, 1)

        if tasks:
            operation.counters.Increment(self.TASKS_LEASED_COUNTER)(ctx)
            return (queue, tasks[0])
        raise StopIteration()

    @classmethod
    def from_json(cls, input_shard_state):
        &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.

        Args:
          input_shard_state: The InputReader state as a dict-like object.

        Returns:
          An instance of the InputReader configured using the values of json.
        &amp;quot;&amp;quot;&amp;quot;
        return cls(input_shard_state.get(cls.QUEUE_NAME),
               input_shard_state.get(cls.TAG),
               input_shard_state.get(cls.LEASE_SECONDS)))

    def to_json(self):
        &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.

        Returns:
          A json-izable version of the remaining InputReader.
        &amp;quot;&amp;quot;&amp;quot;
        return {
            &#39;queue_name&#39;: self.queue_name,
            &#39;tag&#39;: self.tag,
            &#39;lease_seconds&#39;: self.lease_seconds,
        }

    @classmethod
    def split_input(cls, mapper_spec):
        &amp;quot;&amp;quot;&amp;quot;
        Returns a list of input readers
        &amp;quot;&amp;quot;&amp;quot;
        shard_count = mapper_spec.shard_count

        return [cls()] * shard_count

    @classmethod
    def validate(cls, mapper_spec):
        &amp;quot;&amp;quot;&amp;quot;
        Validates mapper spec and all mapper parameters.

        Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
        subdictionary in mapper_spec.params.

        Args:
          mapper_spec: The MapperSpec for this InputReader.

        Raises:
          BadReaderParamsError: required parameters are missing or invalid.
        &amp;quot;&amp;quot;&amp;quot;
        if mapper_spec.input_reader_class() != cls:
            raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)

        # Check that a valid queue is specified
        input_reader_params = mapper_spec.params.get(&#39;input_reader&#39;, {})
        queue_name = input_reader_params.get(cls.QUEUE_NAME)
        lease_seconds = input_reader_params.get(cls.LEASE_SECONDS, 60)
        if not queue_name:
            raise BadReaderParamsError(&#39;%s is required&#39; % cls.QUEUE_NAME)
        if not isinstance(lease_seconds, int):
            raise BadReaderParamsError(&#39;%s must be an integer&#39; % cls.LEASE_SECONDS)
        try:
            queue = taskqueue.Queue(name=queue_name)
            queue.fetch_statistics()
        except Exception as e:
            raise BadReaderParamsError(&#39;%s is invalid&#39; % cls.QUEUE_NAME, e.message)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Installing MySQL-Python on OS X Yosemite</title>
      <link>http://sookocheff.com/posts/2014-11-18-installing-mysql-python-on-yosemite/</link>
      <pubDate>Tue, 18 Nov 2014 11:15:23 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-11-18-installing-mysql-python-on-yosemite/</guid>
      <description>&lt;p&gt;Installing the MySQL-Python package requires a few steps. In an effort to aid
future Internet travellers, this post will document how to install the
MySQL-Python package on OS X Yosemite.&lt;/p&gt;

&lt;p&gt;First, install MariaDB, the drop-in replacement for MySQL. I chose MacPorts for
this task, though Homebrew would work just fine. Second, update your PATH to
include the mariadb executables. Third, install the Python MySQL connector.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo port install mariadb
PATH=/opt/local/lib/mariadb/bin:$PATH
pip install MySQL-Python
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it! You should be able to &lt;code&gt;import MySQLdb&lt;/code&gt; in your Python code and
interact with your MariaDB database.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatically Resizing a Compute Engine Disk</title>
      <link>http://sookocheff.com/posts/2014-11-11-automatically-resizing-a-compute-engine-disk/</link>
      <pubDate>Tue, 11 Nov 2014 19:21:45 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-11-11-automatically-resizing-a-compute-engine-disk/</guid>
      <description>&lt;p&gt;A recurring issue when working with &lt;a href=&#34;https://cloud.google.com/compute/&#34;&gt;Compute
Engine&lt;/a&gt; is that newly created Instances have
only 10GB of free space available. To take advantage of the full disk size you
need to manually partition and resize it. This article shows one method of
accomplishing this task.&lt;/p&gt;

&lt;p&gt;To correctly partition the disk we need to find the start sector. We can find this using &lt;code&gt;fdisk&lt;/code&gt; with the &lt;code&gt;-l&lt;/code&gt;
option to list the full disk output.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; sudo fdisk -l

Disk /dev/sda: 10.7 GB, 10737418240 bytes
4 heads, 32 sectors/track, 163840 cylinders, total 20971520 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disk identifier: 0x0009c3f5

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        4096    20971519    10483712   83  Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only one disk exists and its size is 10.7 GB. We need to resize this disk
ourselves to make it fully useable. The last line of the &lt;code&gt;fdisk&lt;/code&gt; output lists the start
sector. We can extract it using a combination of &lt;code&gt;grep&lt;/code&gt; and &lt;code&gt;awk&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# fisk -l: get the full disk output
# grep ^/dev/sda1: filter the line for the boot disk
# awk -F&amp;quot; &amp;quot; &#39;{ print $3 }&#39;: get the third token where the separator is space
start_sector=$(sudo fdisk -l | grep ^/dev/sda1 |  awk -F&amp;quot; &amp;quot;  &#39;{ print $3 }&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have the start sector we can run through the sequence of commands
required for &lt;code&gt;fdisk&lt;/code&gt; to create a new partition of the full disk size.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt;EOF | sudo fdisk -c -u /dev/sda
d
n
p
1
$start_sector

w
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-c&lt;/code&gt; and &lt;code&gt;-u&lt;/code&gt; option make &lt;code&gt;fdisk&lt;/code&gt; behave the same on both CentOS and Debian.
&lt;code&gt;d&lt;/code&gt; deletes the first (default) partition. &lt;code&gt;n&lt;/code&gt; creates a new partition. &lt;code&gt;p&lt;/code&gt;
selects the new partition as a primary. &lt;code&gt;1&lt;/code&gt; is the partition number.
&lt;code&gt;$start_sector&lt;/code&gt; is the value we extracted from &lt;code&gt;fdisk -l&lt;/code&gt;. The blank line
specifies the default end sector. Finally, &lt;code&gt;w&lt;/code&gt; writes our changes.&lt;/p&gt;

&lt;p&gt;We need to reboot our machine for these changes to take effect.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once our machine comes back to life we need to resize our disk to the full
partition size. This is done withe the &lt;code&gt;resize2fs&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;resize2fs /dev/sda1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together, we can write a script that will automatically resize
a Compute Engine disk. Using this as a startup script will make any Compute
Engine instance you create have a fully sized disk available for use.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;STARTUP_VERSION=1
PARTITION_MARK=/var/startup.partition.$STARTUP_VERSION
RESIZE_MARK=/var/startup.resize.$STARTUP_VERSION

# Repartition the disk to full size
function partition() {
  start_sector=$(sudo fdisk -l | grep ^/dev/sda1 |  awk -F&amp;quot; &amp;quot;  &#39;{ print $3 }&#39;)

  cat &amp;lt;&amp;lt;EOF | sudo fdisk -c -u /dev/sda
d
n
p
1
$start_sector

w
EOF

  # We&#39;ve made the changes to the partition table, they haven&#39;t taken effect; we need to reboot.
  touch $PARTITION_MARK

  sudo reboot
  exit
}

# Resize the filesystem
function resize() {
  resize2fs /dev/sda1
  touch $RESIZE_MARK
}

if [ ! -f $PARTITION_MARK ]; then
  partition
fi

if [ ! -f $RESIZE_MARK ]; then
  resize
fi
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
