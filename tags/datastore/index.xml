<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Sookocheff</title>
    <link>http://sookocheff.com/tags/datastore/</link>
    <language>en-us</language>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <lastBuildDate>Mon, 23 Feb 2015 08:20:37 CST</lastBuildDate>
    
    <item>
      <title>Keeping App Engine Search Documents and Datastore Entities In Sync</title>
      <link>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</link>
      <pubDate>Mon, 23 Feb 2015 08:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</guid>
      <description>

&lt;p&gt;At Vendasta the App Engine Datastore serves as the single point of truth for
most operational data and the majority of interactions are against this single
point of truth. However, a piece of required functionality in many of our
products is to provide a searchable view of the data in the App Engine
Datastore. Search is difficult using the Datastore and so we have moved to using
the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/search/&#34;&gt;Search API&lt;/a&gt; as a
managed solution for searching datastore entities. In this use case, every edit
to an entity in the Datastore is reflected as a change to a Search Document.
This article details an architecture for keeping Datastore entities and Search
Documents in sync throughout failure and race conditions.&lt;/p&gt;

&lt;h2 id=&#34;updating-the-search-document-using-a-post-put-hook:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Updating the Search Document Using a _post_put_hook&lt;/h2&gt;

&lt;p&gt;To ensure that every put of an entity to the Datastore results in an update to
the associated search document, we update the search document in the
_post_put_hook of the entity. The _post_put_hook is executed every time
the entity is put so each time the entity has changed we will put a new and
updated search document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def _post_put_hook(self, future):
        document = search.Document(
            doc_id = self.username,
            fields=[
               search.TextField(name=&#39;username&#39;, value=self.username),
               search.TextField(name=&#39;email&#39;, value=self.email),
               ])
        try:
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)
        except search.Error:
            logging.exception(&#39;Put failed&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Updating the search document during every put as part of the post put hook is a
light weight way to keep the search document up-to-date with changes to the
entity. However, this design does not account for the potential error conditions
where putting the search document or the Datastore entity fails. We will need
some additional functionality to handle these cases.&lt;/p&gt;

&lt;h2 id=&#34;handling-search-document-put-failures:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Search Document Put Failures&lt;/h2&gt;

&lt;p&gt;The first obstacle to overcome is handling failures when putting the search
document. One method for handling failures is retrying. We can add retrying to
our workflow by separating updating the search document into its own task and
deferring that task using the deferred library. This accomplishes two things.
First, moving the search document functionality into its own function makes our
code more modular. Second, the App Engine task queue mechanism allows us to
specify our retry semantics, handling backoff and failure conditions gracefully.
In this example, we allow infinite retries of failed tasks, allowing DevOps to find
search documents that may have become out of sync with their Datastore entities
and correct any problems that may arise. We assume in the example below that the
username acts as the ndb Key.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document, self.username)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;handling-datastore-put-failures:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Datastore Put Failures&lt;/h2&gt;

&lt;p&gt;The second obstacle to overcome is safely handling Datastore put failures. In
this architecture, each change to a Datastore entity is required to run within a
transaction. We update the _post_put_hook to queue a transactional task &amp;ndash; which
forces the task to only be queued if the current transaction has successfully
completed. This guarantees that failed Datastore puts will not result in search
documents being updated and becoming out of sync with the Datastore.&lt;/p&gt;

&lt;p&gt;We specify that the task should be run as a transaction by passing the result of
the &lt;code&gt;in_transaction&lt;/code&gt; function to the &lt;code&gt;_transactional&lt;/code&gt; parameter of &lt;code&gt;defer&lt;/code&gt;.
&lt;code&gt;in_transaction&lt;/code&gt; returns &lt;code&gt;True&lt;/code&gt; if the currently executing code is running in a
transaction and &lt;code&gt;False&lt;/code&gt; otherwise.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have sastisfied the case where either the search document or the
Datastore put has failed. If the search document put has failed we retry, if the
Datastore put has failed we do not put the search document. We still have one
remaining problem: Dirty Reads.&lt;/p&gt;

&lt;h2 id=&#34;handling-dirty-reads:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Dirty Reads&lt;/h2&gt;

&lt;p&gt;The last obstacle to overcome is dealing with race conditions that could lead to
reading stale data and writing that data to the search document. Consider the
case where two subsequent puts to the Datastore occur back-to-back within a
short time frame. Each of these puts will write new data to the Datastore and
queue a task to put the updated search document to the Datastore. The dirty
read problem arises when the second task to update the search document reads old
data from the Datastore that may not have been fully replicated throughout the
Datastore.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34; alt=&#34;Syncing Search Documents&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We can overcome this problem by versioning our tasks to coincide with the
version of our Datastore entity. We add a version number to the entity and
update the version number during a _pre_put_hook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    def _pre_put_hook(self):
        self.version = self.version + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now during the _post_put_hook we queue a task corresponding to the version number
of the Datastore entity we are putting. This ties the task to the point in time
when the Datastore entity was put.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    @classmethod
    def put_search_document(cls, username, version):
        model = ndb.Key(cls, username).get()
        if model:
            if version &amp;lt; model.version:
                logging.warning(&#39;Attempting to write stale data. Ignore&#39;)
                return

            if version &amp;gt; model.version:
                msg = &#39;Attempting to write future data. Retry to await consistency.&#39;
                logging.warning(msg)
                raise Exception(msg)

            # Versions match. Update the search document
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=model.username),
                   search.TextField(name=&#39;email&#39;, value=model.email),
                   search.TextField(name=&#39;version&#39;, value=model.version),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _pre_put_hook(self):
        self.version = self.version + 1

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       self.version,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the version number of the task being executed is less than the version number
written to the Datastore, we are attempting to write stale data and do not need
to process this request. If the version number is greater than the task being
executed, we are attempting to write data to the search document that has not
been fully replicated throughout the Datastore. In this case, we raise an
exception to retry putting the search document. In subsequent retries the data
will have propagated and our put will succeed. Note that if another task is
executed while the current task is retrying, the version number of our retrying
task will become stale and when the task is next executed we do not write the
now stale data to the search document.&lt;/p&gt;

&lt;p&gt;This still handles the case when a search document put fails &amp;ndash; whenever our
version number becomes out of sync due to the failed put, we do not write the
data to the search document. Furthermore, if our Datastore put fails then our
task to put the search document will not be queued &lt;em&gt;as long as the Datastore put
is run within a transaction&lt;/em&gt;. The version number will not be incremented in this
case because the value set during the _pre_put_hook will not be persisted during
a failed transaction.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Putting this all together, we&amp;rsquo;ve developed a solution for keeping search
documents in sync with Datastore entities that is robust to failure and race
conditions. This same technique can be used for syncing the state of any number
of datasets that are dependent on the Datastore being the single point of truth
in your system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Restoring an App Engine backup into a Big Query table</title>
      <link>http://sookocheff.com/posts/2014-08-04-restoring-an-app-engine-backup/</link>
      <pubDate>Mon, 04 Aug 2014 21:18:13 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-08-04-restoring-an-app-engine-backup/</guid>
      <description>

&lt;p&gt;An unfortunate DevOps task for any team running App Engine is restoring data
from backups. One way to do this is by accessing the Google Cloud Storage URL
for a given App Engine backup and importing that backup into BigQuery. This
article will show you to get the Cloud Storage URL for an App Engine backup and
manually perform that import.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-cloud-storage-url:fa0cf78d240508d5fee9738fef21ecf6&#34;&gt;Getting the Cloud Storage URL&lt;/h2&gt;

&lt;p&gt;The first thing you need to do is access the cloud storage URL for a given App
Engine backup. First, log in to the Google Developer Console and navigate to
your backup. The filename of the backup will be a long sequence of characters
followed by the name of your model. The file extension will be &lt;code&gt;.backup_info&lt;/code&gt;.
As an example, this is the filename of backup for an Account model used in one
of our projects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right click on your backup and copy the URL to your clipboard. The URL will be
of the form below. The name of your cloud storage bucket and the identifier for
you app have been highlighted below. Replace these with appropriate values for
your project.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://console.developers.google.com/m/cloudstorage/b/**bucket**/o/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To get the cloud storage URL in the format expected by a BigQuery import remove
everything up to the bucket name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;**bucket**/o/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now remove the &lt;code&gt;o&lt;/code&gt; between the bucket name and your app identifier.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;**bucket**/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, append &lt;code&gt;gs://&lt;/code&gt; to the file to arrive at your final Google Cloud Storage
URL.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gs://**bucket**/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to import the backup into BigQuery. To do this, navigate to
your project and create a new table in your desired dataset.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/create-new-table.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/create-new-table.png&#34; alt=&#34;Create new table.&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;In the &lt;code&gt;Choose destination&lt;/code&gt; tab pick a name for your new table. In my case I&amp;rsquo;ll
name the table with the date of my backup for reference.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/choose-destination.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/choose-destination.png&#34; alt=&#34;Choose destination&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Next, choose App Engine Datastore Backup as the source format and paste the
Cloud Storage URL you arrived at above in the appropriate field.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/select-data.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/select-data.png&#34; alt=&#34;Select Data Source&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;You can choose the defaults for the next tabs and, finally, import your App
Engine backup into BigQuery and watch it being fully restored.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bypassing ndb hooks with the RawDatastoreInputReader</title>
      <link>http://sookocheff.com/posts/2014-07-29-bypassing-ndb-hooks-with-the-raw-datastore-input-reader/</link>
      <pubDate>Tue, 29 Jul 2014 20:32:42 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-07-29-bypassing-ndb-hooks-with-the-raw-datastore-input-reader/</guid>
      <description>

&lt;p&gt;When doing a MapReduce operation there are times when you want to edit a set of
entities without triggering the post or pre put hooks associated with those
entities. On such ocassions using the raw datastore entity allows you to process
the data without unwanted side effects. This article will show how to use the
RawDatastoreInputReader to process datastore entities.&lt;/p&gt;

&lt;p&gt;When doing a MapReduce operation there are times when you want to edit a set of entities without triggering the post or pre put hooks associated with those entities. On such ocassions using the raw datastore entity allows you to process the data without unwanted side effects.&lt;/p&gt;

&lt;p&gt;For the sake of this discussion let&amp;rsquo;s assume we want to move a &lt;code&gt;phone_number&lt;/code&gt; field to a &lt;code&gt;work_number&lt;/code&gt; field for all entities of a certain Kind in the datastore.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-raw-datastore-entity:f509f31b4c10397cd1059cd7982b595f&#34;&gt;Getting the raw datastore entity&lt;/h2&gt;

&lt;p&gt;The MapReduce library provides a &lt;code&gt;RawDatastoreInputReader&lt;/code&gt; that will feed raw datastore entities to your mapping function. We can set our MapReduce operation to use the &lt;code&gt;RawDatastoreInputReader&lt;/code&gt; using a &lt;code&gt;mapreduce.yaml&lt;/code&gt; declaration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- name: move_phone_numbers
  mapper:
    input_reader: mapreduce.input_readers.RawDatastoreInputReader
    handler: app.pipelines.move_phone_numbers_map
    params:
    - name: entity_kind
      default: MyModel
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;manipulating-a-raw-datastore-entity:f509f31b4c10397cd1059cd7982b595f&#34;&gt;Manipulating a raw datastore entity&lt;/h2&gt;

&lt;p&gt;Our &lt;code&gt;raw_datastore_map&lt;/code&gt; function to use the datastore entity in its raw form. The raw form of the datastore entity provides a dictionary like interface that we can use to manipulate the entity. With this interface we can move the phone number to the correct field.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def move_phone_numbers_map(entity):
    phone_number = entity.get(&#39;phone_number&#39;)
    if phone_number:
        entity[&#39;work_number&#39;] = phone_number
    del entity[&#39;phone_number&#39;]
    
    yield op.db.Put(entity)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using &lt;code&gt;op.db.Put&lt;/code&gt; will put the entity to the datastore using the raw datastore
API, thereby bypassing any ndb hooks that are in place.  For more information on
the raw datastore API the best resource is the source code itself, available
from the &lt;a href=&#34;https://code.google.com/p/googleappengine/source/browse/trunk/python/google/appengine/api/datastore.py&#34;&gt;App Engine SDK
repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
