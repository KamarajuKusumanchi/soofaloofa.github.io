<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Sookocheff</title>
    <link>http://sookocheff.com/posts/</link>
    <language>en-us</language>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <lastBuildDate>Tue, 02 Jun 2015 04:45:56 CST</lastBuildDate>
    
    <item>
      <title>App Engine Pipelines API - Part 5: Asynchronous Pipelines</title>
      <link>http://sookocheff.com/posts/appengine/pipelines/asynchronous-pipelines/</link>
      <pubDate>Tue, 02 Jun 2015 04:45:56 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/appengine/pipelines/asynchronous-pipelines/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This article will cover fully asynchronous pipelines. The term &amp;lsquo;asynchronous&amp;rsquo; is
misleading here â€” all piplines are asynchronous in the sense that yielding a
pipeline is a non-blocking operation. An asynchronous refers to a
pipeline that remains in a RUN state until outside action is taken, for example,
a button is clicked or a task is executed.&lt;/p&gt;

&lt;p&gt;Marking a pipeline as an asynchronous pipeline is as simple as setting the
&lt;code&gt;async&lt;/code&gt; class property to True.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class AsyncPipeline(pipeline.Pipeline):
    async = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once this pipeline starts, it will remain in the RUN state until the pipeline is
transitioned to another state. You transition a pipeline to another state by
calling the &lt;code&gt;complete&lt;/code&gt; method, using a callback. &lt;code&gt;complete()&lt;/code&gt; is a
method only available to asynchronous pipelines. Calling complete will fill the
pipelines output slots and, if all slots have been filled, mark the pipeline
complete. Any barriers related to the slots being filled are notified as
described in &lt;a href=&#34;http://sookocheff.com/posts/appengine/pipelines/pipeline-internals/&#34;&gt;the previous article&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class AsyncPipeline(pipeline.Pipeline):
    async = True

    def callback(self):
        self.complete()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;callback-urls:74bf5c045d8c3cbfb500a70528c1dea1&#34;&gt;Callback URLs&lt;/h2&gt;

&lt;p&gt;The pipeline API provides convenience methods for calling the callback method.
&lt;code&gt;get_callback_url&lt;/code&gt; returns a URL that, when accessed, passes any query
parameters to the callback method. For example, to generate a URL to our
pipeline with a &lt;code&gt;choice&lt;/code&gt; parameter we can call get_callback_url as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;url = get_callback_url(choice=&#39;approve&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will generate a URL of the form:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;/_ah/pipeline/callback?choice=approve&amp;amp;pipeline_id=fd789852183b4310b5f1353205a967fe
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Accessing this URL will pass the &lt;code&gt;choice&lt;/code&gt; parameter to the callback function of
the pipeline with pipeline_id &lt;code&gt;fd789852183b4310b5f1353205a967fe&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class AsyncPipeline(pipeline.Pipeline):
    async = True
    public_callbacks = True

    def run(self):
        url = self.get_callback_url(choice=&#39;approve&#39;)
        logging.info(&#39;Callback URL: %s&#39; % url)

    def callback(self, choice):
        if choice == &#39;approve&#39;:
            logging.info(&#39;Pipeline Complete&#39;)
            self.complete()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the pipeline above will log the Callback URL to the console. By visiting
that URL, the &lt;code&gt;callback&lt;/code&gt; method will execute, completing your pipeline. You can
refer to the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines/blob/master/python/src/pipeline/common.py&#34;&gt;EmailToContinue&lt;/a&gt; Pipeline for a more robust example.&lt;/p&gt;

&lt;h2 id=&#34;callback-tasks:74bf5c045d8c3cbfb500a70528c1dea1&#34;&gt;Callback Tasks&lt;/h2&gt;

&lt;p&gt;The second way to execute a callback method is via a callback task. The
Pipelines API provides another convenience method to generate a callback task
that will execute our pipeline. In the following example, a task is created to
trigger in the future, adding an artificial delay to our pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DelayPipeline(pipeline.Pipeline):
    async = True

    def __init__(self, seconds):
        super(DelayPipeline, self).__init__(seconds=seconds)

    def run(self, seconds=None):
        task = self.get_callback_task(
            countdown=seconds,
            name=&#39;ae-pipeline-delay-&#39; + self.pipeline_id)
        try:
            task.add(self.queue_name)
        except (taskqueue.TombstonedTaskError, taskqueue.TaskAlreadyExistsError):
            pass

    def callback(self):
        self.complete(self.kwargs[&#39;seconds&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the task is queued using the pipeline_id in the task name. This helps
ensure our run method is idempotent. Full source code for an asynchronous
pipeline follows. This pipeline will delay for 10 seconds, and then log a
callback_url to the console. Visiting the callback URL will complete the
pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline
from google.appengine.api import taskqueue


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        pipeline = DelayPipeline(10)
        pipeline.start()


class DelayPipeline(pipeline.Pipeline):
    async = True

    def __init__(self, seconds):
        pipeline.Pipeline.__init__(self, seconds=seconds)

    def run(self, seconds=None):
        task = self.get_callback_task(
            countdown=seconds,
            name=&#39;ae-pipeline-delay-&#39; + self.pipeline_id)
        try:
            task.add(self.queue_name)
        except (taskqueue.TombstonedTaskError,
                taskqueue.TaskAlreadyExistsError):
            pass

    def callback(self):
        AsyncPipeline().start()


class AsyncPipeline(pipeline.Pipeline):
    async = True
    public_callbacks = True

    def run(self):
        url = self.get_callback_url(choice=&#39;approve&#39;)
        logging.info(&#39;Callback URL: %s&#39; % url)

    def callback(self, choice):
        if choice == &#39;approve&#39;:
            self.complete()


routes = [webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 4: Pipeline Internals</title>
      <link>http://sookocheff.com/posts/appengine/pipelines/pipeline-internals/</link>
      <pubDate>Wed, 27 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/appengine/pipelines/pipeline-internals/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://sookocheff.com/posts/appengine/pipelines/connecting-pipelines/&#34;&gt;We&amp;rsquo;ve learned how to execute and chain together pipelines&lt;/a&gt;,
now let&amp;rsquo;s take a look at how pipelines execute under the hood. If necessary,
you can refer to the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines&#34;&gt;source code of the pipelines
project&lt;/a&gt; to
clarify any details.&lt;/p&gt;

&lt;h2 id=&#34;the-pipeline-data-model:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;The Pipeline Data Model&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with the pipeline data model. Note that each Kind defined by the
pipelines API is prefixed by &lt;code&gt;_AE_Pipeline&lt;/code&gt;, making it easy to view individual
pipeline details by viewing the datastore entity.&lt;/p&gt;

&lt;h3 id=&#34;pipelinerecord:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;PipelineRecord&lt;/h3&gt;

&lt;p&gt;Every pipeline is represented by a &lt;em&gt;PipelineRecord&lt;/em&gt; in the datastore. The
PipelineRecord records the pipeline&amp;rsquo;s root identifier (if this pipeline is a
child), any child pipelines spawned by this pipeline, the current status
of the pipeline, and a few additional bookkeeping details.&lt;/p&gt;

&lt;p&gt;At any point in time a Pipeline may be in one of four states: WAITING, RUN,
DONE, and ABORTED.  WAITING implies that this pipeline has a barrier that
must be satisfied before the pipeline can be RUN. RUN means that the pipeline
has been started. DONE means that the pipeline is complete. ABORTED means
that the pipeline has been manually aborted.&lt;/p&gt;

&lt;h3 id=&#34;slotrecord:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;SlotRecord&lt;/h3&gt;

&lt;p&gt;The output of a pipeline is represented as a &lt;em&gt;Slot&lt;/em&gt; stored in the datastore as a
&lt;em&gt;SlotRecord&lt;/em&gt;. When a pipeline completes, it stores its output in the SlotRecord
to be made available to further pipelines.&lt;/p&gt;

&lt;h3 id=&#34;barrierrecord:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;BarrierRecord&lt;/h3&gt;

&lt;p&gt;A &lt;em&gt;BarrierRecord&lt;/em&gt; represents the slots that must be filled before a pipeline can
execute. The barrier tracks &lt;em&gt;blocking_slots&lt;/em&gt; that must be filled before the
barrier can be lifted. Once the barrier is lifted a &lt;em&gt;target&lt;/em&gt; pipeline is
notified and the target can transition to the RUN state.&lt;/p&gt;

&lt;p&gt;Barriers that depend on a slot being filled are stored in the &lt;em&gt;BarrierIndex&lt;/em&gt;,
which tracks barriers that are dependent on a slot. The purpose of the
BarrierIndex is to force &lt;a href=&#34;https://cloud.google.com/datastore/docs/articles/balancing-strong-and-eventual-consistency-with-google-cloud-datastore/&#34;&gt;strong consistency&lt;/a&gt; when querying for a SlotRecord&amp;rsquo;s Barriers.&lt;/p&gt;

&lt;h3 id=&#34;statusrecord:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;StatusRecord&lt;/h3&gt;

&lt;p&gt;The &lt;em&gt;StatusRecord&lt;/em&gt; tracks the current status of a pipeline and facilitates the
pipeline user interface. The StatusRecord is updated as the pipeline progresses
to give a view of the current pipeline state. Not much more than that.&lt;/p&gt;

&lt;h2 id=&#34;pipeline-execution:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;Pipeline Execution&lt;/h2&gt;

&lt;p&gt;Having an understanding of the pipeline data model gives a rough idea of how
pipelines are executed. Each stage of execution corresponds to a webapp2 handler
that services the request and advances the state of the pipeline. The following
diagram shows each of the pipeline stages during typical execution and the
description that follows provides more detail on each stage.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-internals/pipeline-states.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/appengine/pipelines/pipeline-internals/pipeline-states.png&#34; alt=&#34;Pipeline States&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h3 id=&#34;start:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;start()&lt;/h3&gt;

&lt;p&gt;A pipeline is started by calling its &lt;code&gt;start()&lt;/code&gt; method. When calling &lt;code&gt;start()&lt;/code&gt; a
&lt;em&gt;PipelineRecord&lt;/em&gt; is created and marked as a &lt;em&gt;RootPipeline&lt;/em&gt;, &lt;em&gt;SlotRecords&lt;/em&gt; are
created for each of the pipelines outputs and marked as children of the
pipeline, and &lt;em&gt;BarrierRecords&lt;/em&gt; are created corresponding to each of the output
slots of the pipeline. Finally, a task is queued to the &lt;code&gt;/run&lt;/code&gt; handler to
execute the pipeline.&lt;/p&gt;

&lt;h3 id=&#34;run-handler:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;/run handler&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;/run&lt;/code&gt; handler transitions the pipeline from the WAITING state to the RUN
state by setting a flag on the &lt;em&gt;PipelineRecord&lt;/em&gt;, the pipeline object instance is
then reconstructed given the data from the request and the pipeline&amp;rsquo;s &lt;code&gt;run()&lt;/code&gt;
method is called. When the &lt;code&gt;run()&lt;/code&gt; method is complete, any outputs are used to
fill &lt;em&gt;SlotRecords&lt;/em&gt; and yielded to the parent pipeline when necessary.  Finally,
any child pipelines and their dependent slots and barriers are created and
marked as children of the parent pipeline. Calls to the &lt;code&gt;/fanout&lt;/code&gt;
handler are made to queue tasks to start any child pipelines.&lt;/p&gt;

&lt;h3 id=&#34;fanout-handler:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;/fanout handler&lt;/h3&gt;

&lt;p&gt;The &lt;code&gt;/fanout&lt;/code&gt; handler loads all child pipelines given a parent pipeline and queues
a task to the &lt;code&gt;/run&lt;/code&gt; handler for each of them.&lt;/p&gt;

&lt;h3 id=&#34;output-handler:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;/output handler&lt;/h3&gt;

&lt;p&gt;Whenever a slot is filled a task is queued to the &lt;code&gt;/output&lt;/code&gt; handler to notify any
barriers to a pipeline&amp;rsquo;s execution that they can be removed. If a pipeline has
all its barriers to completing removed, a task is queued to the &lt;code&gt;/finalize&lt;/code&gt; handler
to mark our pipeline as complete. The &lt;code&gt;/output&lt;/code&gt; handler queues tasks to the
&lt;code&gt;/run&lt;/code&gt; method for any pipelines that have their barriers to starting removed.&lt;/p&gt;

&lt;h3 id=&#34;finalized-handler:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;/finalized handler&lt;/h3&gt;

&lt;p&gt;When the &lt;code&gt;/finalized&lt;/code&gt; handler is called, the pipeline is marked as complete and
our pipeline&amp;rsquo;s &lt;code&gt;finalized()&lt;/code&gt; method is called.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:b0f788db0cc3986803d61b920ecd3adf&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Understanding the Pipeline data model and run-time can help you to visualize and
debug any pipeline problems. Stay tuned for the next article covering
asynchronous pipelines.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 3: Fan In, Fan Out, Sequencing</title>
      <link>http://sookocheff.com/posts/appengine/pipelines/fan-in-fan-out/</link>
      <pubDate>Tue, 19 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/appengine/pipelines/fan-in-fan-out/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://sookocheff.com/posts/appengine/pipelines/connecting-pipelines/&#34;&gt;Last time&lt;/a&gt;,
we studied how to connect two pipelines together. In this post, we expand on
this topic, exploring how to fan-out to do multiple tasks in parallel, fan-in
to combine multiple tasks into one, and how to do sequential work.&lt;/p&gt;

&lt;h2 id=&#34;fan-out:077d7e8a485e72c5c014b45abe98e05c&#34;&gt;Fan-Out&lt;/h2&gt;

&lt;p&gt;Fan-Out refers to spreading a task to multiple destinations in parallel. Using
the Pipelines API, fan-out can be achieved elegantly by yielding a new pipeline
for every task you wish to execute. Each of these pipelines is exeucted
immediately via a Task in the App Engine Task Queue. Fan-out parallelizes
implicitly when additional App Engine instances are started to handle the
increased number of requests arriving in the Task Queue. You can moderate the
amount of fan-out by changing the processing rate on the task queue that
executes your pipelines.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;Squaring: %s&#39;, number)
        return number * number


class FanOutPipeline(pipeline.Pipeline):

    def run(self, count):
        for i in xrange(0, count):
            yield SquarePipeline(i)
        # All children run immediately
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fan-in:077d7e8a485e72c5c014b45abe98e05c&#34;&gt;Fan-In&lt;/h2&gt;

&lt;p&gt;Fan-In implies waiting for a collection of related tasks to complete before
continuing processing. The example can be extended by summing the list of
squared values â€” when we call &lt;code&gt;yield Sum(*results)&lt;/code&gt; the pipeline run-time will
wait until all results are ready before executing Sum. Internally, a &lt;em&gt;barrier&lt;/em&gt;
record is created that blocks execution of Sum and tracks the dependencies
required to lift the barrier. Once all dependencies have been satisfied the
barrier is lifted and Sum can execute.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;Squaring: %s&#39; % number)
        return number * number


class Sum(pipeline.Pipeline):

    def run(self, *args):
        value = sum(list(args))
        logging.info(&#39;Sum: %s&#39;, value)
        return value


class FanInPipeline(pipeline.Pipeline):

    def run(self, count):
        results = []
        for i in xrange(0, count):
            result = yield SquarePipeline(i)
            results.append(result)

        # Waits until all SquarePipeline results are complete
        yield Sum(*results)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;sequencing:077d7e8a485e72c5c014b45abe98e05c&#34;&gt;Sequencing&lt;/h2&gt;

&lt;p&gt;A common workflow is running pipelines in a predefined sequence. The Pipelines
API provides context managers that will force execution ordering using the
&lt;code&gt;with&lt;/code&gt; keyword. This is useful for Pipelines with no output that you wish to
execute in a specific order â€” we cannot wait for the output and so no barrier
must be satisfied, but we still want to enforce an execution order. In the
following example, we extend the FanOutFanInPipeline to update an HTML
dashboard with our results and, once that is complete, send out an e-mail to the
development team. This example is taken from the excellent &lt;a href=&#34;https://www.youtube.com/watch?v=Rsfy_TYA2ZY&#34;&gt;Pipelines API
introductory video&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class FanOutFanInPipeline(pipeline.Pipeline):

    def run(self, count):
        results = []
        for i in xrange(0, count):
            result = yield SquarePipeline(i)
            results.append(result)

        result = yield Sum(*results)
        with pipeline.InOrder():
            yield UpdateDashboard()
            yield EmailTeam()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion:077d7e8a485e72c5c014b45abe98e05c&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This article describes how to coordinate pipeline tasks using fan-in, fan-out
and sequencing. The next article we will discuss Pipeline API internals.&lt;/p&gt;

&lt;p&gt;Full source code of both Fan-In and Fan-Out follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        stage = FanOutFanInPipeline(10)
        stage.start()


class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;Squaring: %s&#39; % number)
        return number * number


class Sum(pipeline.Pipeline):

    def run(self, *args):
        value = sum(list(args))
        logging.info(&#39;Sum: %s&#39;, value)
        return value


class FanOutFanInPipeline(pipeline.Pipeline):

    def run(self, count):
        results = []
        for i in xrange(0, count):
            result = yield SquarePipeline(i)
            results.append(result)

        yield Sum(*results)


routes = [
    webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)
]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 2: Connecting Pipelines</title>
      <link>http://sookocheff.com/posts/appengine/pipelines/connecting-pipelines/</link>
      <pubDate>Tue, 12 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/appengine/pipelines/connecting-pipelines/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&#34;http://sookocheff.com/posts/appengine/pipelines/the-basics/&#34;&gt;Last time&lt;/a&gt;,
we discussed basic pipeline instantiation and execution. This time, we will
cover sequential pipelines, answering the question &amp;ldquo;How do I connect the output
of one pipeline with the input of another pipeline&amp;rdquo;?&lt;/p&gt;

&lt;p&gt;To begin, let&amp;rsquo;s review a basic pipeline that squares its input. If any of this
does not make sense refer to the &lt;a href=&#34;http://sookocheff.com/posts/appengine/pipelines/the-basics/&#34;&gt;first part of this tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        stage = SquarePipeline(10)
        stage.start()


class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        return number * number

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.default.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The first step in passing data between two pipelines is updating our pipeline to
use the generator interface. The generator interface uses the &lt;code&gt;yield&lt;/code&gt; keyword as
a means of connecting pipelines together. For this contrived example, let&amp;rsquo;s
create a &lt;em&gt;parent&lt;/em&gt; pipeline that executes &lt;code&gt;SquarePipeline&lt;/code&gt; twice in succession.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What now? We need a way to access the value stored in &lt;code&gt;second_square&lt;/code&gt;. When
execution hits a &lt;code&gt;yield&lt;/code&gt; statement a task is started to run the pipeline and a
&lt;code&gt;PipelineFuture&lt;/code&gt; is returned. The &lt;code&gt;PipelineFuture&lt;/code&gt; will have a value &lt;em&gt;after&lt;/em&gt; the
task has finished executing but not immediately. So how do we access the value?
With a &lt;em&gt;child&lt;/em&gt; pipeline that can read the result. In this example, we simply log
the value of the computation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
        yield LogResult(second_square)

class LogResult(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;All done! Value is %s&#39;, number)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The rule of thumb here is that &lt;em&gt;anything you instantiate your pipeline with (and
subsequently pass to the &lt;code&gt;run&lt;/code&gt; method) is accessible within your
pipeline&lt;/em&gt;. These are called &lt;em&gt;immediate values&lt;/em&gt; and you can treat them as regular
Python values. When this code is executed, each pipeline started by a &lt;code&gt;yield&lt;/code&gt;
call is a separate App Engine Task that executes in the Task Queue. The Pipeline
runtime coordinates running these tasks and shares the results of execution
between tasks, allowing you to safely connect pipelines together.&lt;/p&gt;

&lt;p&gt;Full source code for this example follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        stage = TwiceSquaredPipeline(10)
        stage.start()


class SquarePipeline(pipeline.Pipeline):

    def run(self, number):
        return number * number


class TwiceSquaredPipeline(pipeline.Pipeline):

    def run(self, number):
        first_square = yield SquarePipeline(number)
        second_square = yield SquarePipeline(first_square)
        yield LogResult(second_square)


class LogResult(pipeline.Pipeline):

    def run(self, number):
        logging.info(&#39;All done! Value is %s&#39;, number)


routes = [
    webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)
]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine Pipelines API - Part 1: The Basics</title>
      <link>http://sookocheff.com/posts/appengine/pipelines/the-basics/</link>
      <pubDate>Tue, 05 May 2015 05:57:19 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/appengine/pipelines/the-basics/</guid>
      <description>

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/series/pipelines-api/&#34;&gt;View all articles in the Pipeline API Series&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines&#34;&gt;Pipelines API&lt;/a&gt;
is a general purpose workflow engine for App Engine applications. With the
Pipelines API we can connect together complex workflows into a coherent run time
backed by the Datastore. This article provides a basic overview of the Pipelines
API and how it can be used for abritrary computational workflows.&lt;/p&gt;

&lt;p&gt;In the most basic sense a Pipeline is an object that takes input, performs some
logic or computation on that input, and produces output. Pipelines can take two
general forms &amp;ndash; synchronous or asynchronous. Synchronous pipelines act as basic
functions that must complete during a single request. Asynchronous pipelines
spawn child pipelines and connect them together into a workflow by passing input
and output parameters around.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A word of warning.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pipelines must be idempotent and it is up to the developer to ensure that they
are &amp;ndash; this is not enforced by the run-time. A pipeline may fail and be retried
and it is important that running the same pipeline with the same set of inputs
will product the same results.&lt;/p&gt;

&lt;h2 id=&#34;getting-started:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Getting Started&lt;/h2&gt;

&lt;p&gt;The first step is to grab the latest version of the Pipelines API (and its
        dependencies) using pip. The following assumes you install third party
App Engine dependencies in the lib directory relative to where pip is being run.
You can also grab the source code from
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-pipelines&#34;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install GoogleAppEnginePipeline -t lib/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Pipeline requests need to be handled by the Pipeline application. We set that up
by adding a handler to &lt;code&gt;app.yaml&lt;/code&gt;. Since these are internal application requrest
we can secure them using the &lt;code&gt;login: admin&lt;/code&gt; directive.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;handlers:
- url: /_ah/pipeline.*
  script: pipeline.handlers._APP
  login: admin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;basic-synchronous-pipelines:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Basic Synchronous Pipelines&lt;/h2&gt;

&lt;p&gt;A synchronous pipeline runs within the bounds of a single App Engine request.
Once the request has been made the pipeline starts and pipeline processing
happens automatically. We can set up this pipeline by defining a handler
responsible for starting the pipeline. For now, create a default handler that
will receive a request at the URL of your choosing.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2

class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        logging.info(&#39;Launch pipeline&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A request processed by this handler will kick off our Pipeline. To define a
pipeline we inherit from the Pipeline object and the method &lt;code&gt;run&lt;/code&gt;. The pipeline
is launched via the &lt;code&gt;start&lt;/code&gt; method. The code below instantiates a custom
pipeline and launches it. Accessing the URL for the RunPipelineHandler will
print the message &amp;lsquo;Do something here&amp;rsquo; to the logs.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline

class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        logging.info(&#39;Launch pipeline&#39;)
        pipeline = MyPipeline()
        pipeline.start()


class MyPipeline(pipeline.Pipeline):
    def run(self, *args, **kwargs):
        logging.info(&#39;Do something here.&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can update our pipeline to do a simple operation, like squaring a number.
You&amp;rsquo;ll notice in the code that follows that the arguments passed when
initializing the pipeline are accessible as parameters to the &lt;code&gt;run&lt;/code&gt; method
within the pipeline.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running this pipeline will show that the pipeline executes correctly. But where
does our return value go? How can we access the output of &lt;code&gt;SquarePipeline&lt;/code&gt;?&lt;/p&gt;

&lt;h2 id=&#34;accessing-pipeline-output:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Accessing Pipeline Output&lt;/h2&gt;

&lt;p&gt;You&amp;rsquo;ll notice that in &lt;code&gt;SquarePipeline&lt;/code&gt; we are returning a value directly but
we never actually access it. Pipeline output can only ever be accessed after the
pipeline has finished executing. We can check for the end of pipeline execution
using the &lt;code&gt;has_finalized&lt;/code&gt; property. This property will be set to &lt;code&gt;True&lt;/code&gt; when all
stages of a pipeline have finished executing. At this point in time our output
will be available as a value on the Pipeline object. Let&amp;rsquo;s see what happens when
we try to check if our pipeline has finalized. To do this we need to store the
pipeline_id generated from our start method and check the &lt;code&gt;has_finalized&lt;/code&gt;
property.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()

        pipeline_id = square_stage.pipeline_id

        stage = SquarePipeline.from_id(pipeline_id)
        if stage.has_finalized:
            logging.info(&#39;Finalized&#39;)
        else:
            logging.info(&#39;Not finalized&#39;)


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Running the preceding code we see that our pipeline is not finalized. What
happened here? The pipeline is executed as an ayschronous task after it has been
started and may or may not complete by the time we check that it has finalized.
The pipeline itself is a future whose value has not materialized. Any output
from a pipeline is not actually available until all child pipeline tasks are
executed. So how do we get the final value of the SquarePipeline?&lt;/p&gt;

&lt;h2 id=&#34;finalized:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Finalized&lt;/h2&gt;

&lt;p&gt;The finalized method is called by the pipeline API once a Pipeline has completed
its work (by filling all of is slots &amp;ndash; to be described later). By overriding
the &lt;code&gt;finalized&lt;/code&gt; method we can see the result of our pipeline and do further
processing on that result if necessary. By default our output is set to
&lt;code&gt;self.outputs.default.value&lt;/code&gt;. As an example, executing the following code will
log the message &amp;ldquo;All done! Square is 100&amp;rdquo;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):
    def run(self, number):
        return number * number

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.default.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We will see in a later article how to connect the output of one pipeline with
another.&lt;/p&gt;

&lt;h2 id=&#34;named-outputs:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Named outputs&lt;/h2&gt;

&lt;p&gt;Pipelines also allow you to explicitly name outputs, this is useful in the case
where you have more than one output to return or as a means of passing data
between one pipeline execution and the next. When using named outputs, instead
of returning a value from the &lt;code&gt;run&lt;/code&gt; method we fill a pipeline slot with our
value. To use named outputs we define an &lt;code&gt;output_names&lt;/code&gt; class variable listing
the names of our outputs. By calling &lt;code&gt;self.fill&lt;/code&gt; on our named output we store
the return value of our pipeline for later access in the &lt;code&gt;run&lt;/code&gt; method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):

    output_names = [&#39;square&#39;]

    def run(self, number):
        self.fill(self.outputs.square, number * number)

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.square.value)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;testing-a-pipeline:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Testing a pipeline&lt;/h2&gt;

&lt;p&gt;Sometimes our pipelines call out over the wire or perform expensive data
operations. The Pipeline API provides a convenient way to test pipelines. By
calling &lt;code&gt;start_test&lt;/code&gt; instead of &lt;code&gt;start&lt;/code&gt;. In our example we verify the
expected output of our squaring pipeline by calling &lt;code&gt;start_test&lt;/code&gt;. The final
value of our pipeline is available immediately.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start_test()
        assert stage.outputs.square.value == 100
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If we need to mock out any behaviour from our &lt;code&gt;run&lt;/code&gt; method, we can supply a
&lt;code&gt;run_test&lt;/code&gt; method that is executed whenever we run our pipeline with
&lt;code&gt;start_test&lt;/code&gt;. Within this method we can mock out or adjust the behaviour of the
pipeline to work under test.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:dd1501e70e3c5b7c7f6f0783a2583eda&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;This article gives a basic outline of how to start and execute pipelines. Full
source code for the final example is listed below. In the next article we will
see how to pass the output of one pipeline to another and understand how parent
and child pipelines interact.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
import webapp2
import pipeline


class RunPipelineHandler(webapp2.RequestHandler):
    def get(self):
        square_stage = SquarePipeline(10)
        square_stage.start()


class SquarePipeline(pipeline.Pipeline):

    output_names = [&#39;square&#39;]

    def run(self, number):
        self.fill(self.outputs.square, number * number)

    def finalized(self):
        logging.info(&#39;All done! Square is %s&#39;, self.outputs.square.value)

routes = [
    webapp2.Route(&#39;/pipeline-test/&#39;, handler=&#39;main.RunPipelineHandler&#39;)
]

APP = webapp2.WSGIApplication(routes)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Durabledict for App Engine</title>
      <link>http://sookocheff.com/posts/2015-04-28-durabledict-for-app-engine/</link>
      <pubDate>Wed, 29 Apr 2015 06:19:23 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-04-28-durabledict-for-app-engine/</guid>
      <description>

&lt;h2 id=&#34;tldr:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;tldr;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/soofaloofa/datastoredict&#34;&gt;DatastoreDict&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-s-a-durabledict:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;What&amp;rsquo;s a durabledict?&lt;/h2&gt;

&lt;p&gt;Good question. &lt;a href=&#34;https://github.com/disqus/durabledict&#34;&gt;Durabledict&lt;/a&gt; is a Python
implementation of a persistent dictionary. The dictionary values are cached
locally and sync with the datastore whenever a value in the datastore changes.&lt;/p&gt;

&lt;p&gt;Disqus provides concrete implementations for Redis, Django, ZooKeeper and in
memory. This blog post details an implementation using the App Engine datastore
and memcache.&lt;/p&gt;

&lt;h2 id=&#34;creating-your-own-durabledict:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;Creating your own durabledict&lt;/h2&gt;

&lt;p&gt;By following the &lt;a href=&#34;https://github.com/disqus/durabledict&#34;&gt;guide the durabledict
README&lt;/a&gt; we can create our own
implementation. We need to subclass &lt;code&gt;durabledict.base.DurableDict&lt;/code&gt; and implement
the following interface methods. Strictly speaking, &lt;code&gt;_pop&lt;/code&gt; and &lt;code&gt;_setdefault&lt;/code&gt; do
not have to be implemented but doing so makes your durabledict behave like a
base dict in all cases.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;persist(key, value)&lt;/code&gt; - Persist value at key to your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;depersist(key)&lt;/code&gt; - Delete the value at key from your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;durables()&lt;/code&gt; - Return a key=val dict of all keys in your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;last_updated()&lt;/code&gt; - A comparable value of when the data in your data store was last updated.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_pop(key, default=None)&lt;/code&gt; - If key is in the dictionary, remove it and return its value, else return default. If default is not given and key is not in the dictionary, a KeyError is raised.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_setdefault(key, default=None)&lt;/code&gt; - If key is in the dictionary, return its value. If not, insert key with a value of default and return default. default defaults to None.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s implement these one-by-one.&lt;/p&gt;

&lt;h3 id=&#34;persist-key-value:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;persist(key, value)&lt;/h3&gt;

&lt;p&gt;Persisting a value to the datastore is a relatively simple operation. If the key
already exists we update it&amp;rsquo;s value. If the key does not already exist we create
it. To aid with this operation we create a &lt;code&gt;get_or_create&lt;/code&gt; method that will
return an existing entity if one exists or create a new entity if one does not
exist.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def persist(self, key, val):
    instance, created = get_or_create(self.model, key, val)

    if not created and instance.value != val:
        instance.value = val
        instance.put()

    self.touch_last_updated()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last line of this function updates the last time this durabledict was
changed. This is used for caching. We create the &lt;code&gt;last_updated&lt;/code&gt; and
&lt;code&gt;touch_last_updated&lt;/code&gt; functions now.&lt;/p&gt;

&lt;h3 id=&#34;last-updated-key-value:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;last_updated(key, value)&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def last_updated(self):
    return self.cache.get(self.cache_key)

def touch_last_updated(self):
    self.cache.incr(self.cache_key, initial_value=self.last_synced + 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;init:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;&lt;strong&gt;init&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We now have the building blocks to create our initial durabledict. Within the
&lt;code&gt;__init__&lt;/code&gt; method we set a manager and cache instance. The manager is
responsible for ndb datastore operations to decouple the ndb interface from the
durabledict implementation. We decouple our caching method in a similar fashion.
We also set the initial value of the cache whenever we create a new instance of
the durabledict.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import memcache

from durabledict.base import DurableDict
from durabledict.encoding import NoOpEncoding


class DatastoreDict(DurableDict):

    def __init__(self,
                 model,
                 value_col=&#39;value&#39;,
                 cache=memcache,
                 cache_key=&#39;__DatastoreDict:LastUpdated__&#39;):

        self.model = model
        self.value_col = value_col
        self.cache = cache
        self.cache_key = cache_key

        self.cache.add(self.cache_key, 1)

        super(DatastoreDict, self).__init__(encoding=NoOpEncoding)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;depersist-key:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;depersist(key)&lt;/h3&gt;

&lt;p&gt;Depersist implies deleting a key from the dictionary (and datastore). Here we
assume a helper method &lt;code&gt;delete&lt;/code&gt; that, given an ndb model and a string
representing it&amp;rsquo;s key deletes the model. Since the data has changed we also
update the last touched value to force a cache invalidation and data refresh.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def depersist(self, key):
    delete(self.model, key)
    self.touch_last_updated()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;durables:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;durables()&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;durables()&lt;/code&gt; returns the entire dictionary. Since we are all matching entities
from the datastore it is important to keep your dictionary relatively small &amp;ndash;
as the dictionary grows in size, resyncing it&amp;rsquo;s state with the datastore will
get more and more expensive. This function assumes a &lt;code&gt;get_all&lt;/code&gt; method that will
return all instances of a model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def durables(self):
    encoded_models = get_all(self.model)
    return dict((model.key.id(), getattr(model, self.value_col)) for model in encoded_models)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;setdefault-key-default-none:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;setdefault(key, default=None)&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;_setdefault()&lt;/code&gt; overrides the dictionary built-in &lt;code&gt;setdefault&lt;/code&gt; which allows you
to insert a key into the dictionary, creating the key with the default value if
it does not exist and returning the existing value if it does exist.&lt;/p&gt;

&lt;p&gt;For example, the following sequence of code creates a key for &lt;code&gt;y&lt;/code&gt;, which does not
exist, and returns the existing value for &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; d = {&#39;x&#39;: 1}
&amp;gt;&amp;gt;&amp;gt; d.setdefault(&#39;y&#39;, 2)
2
&amp;gt;&amp;gt;&amp;gt; d
{&#39;y&#39;: 2, &#39;x&#39;: 1}
&amp;gt;&amp;gt;&amp;gt; d.setdefault(&#39;x&#39;, 3)
1
&amp;gt;&amp;gt;&amp;gt; d
{&#39;y&#39;: 2, &#39;x&#39;: 1}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can implement &lt;code&gt;_setdefault&lt;/code&gt; using the &lt;code&gt;get_or_create&lt;/code&gt; helper method, updating
the cache if we have changed the dictionary.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _setdefault(self, key, default=None):
    instance, created = get_or_create(self.model, key, default)

    if created:
        self.touch_last_updated()

    return getattr(instance, self.value_col)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pop-key-default-none:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;pop(key, default=None)&lt;/h3&gt;

&lt;p&gt;pop returns the value for a key and deletes the key. This is fairly straight
forward given a &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; helper method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _pop(self, key, default=None):
    instance = get(self.model, key)
    if instance:
        value = getattr(instance, self.value_col)
        delete(self.model, key)
        self.touch_last_updated()
        return value
    else:
        if default is not None:
            return default
        else:
            raise KeyError
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-help:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;The Help&lt;/h3&gt;

&lt;p&gt;The previous discussion uses a few helper methods that we haven&amp;rsquo;t defined yet.
Each of these methods takes an arbitrary ndb model and performs an operation on
it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_key(cls, key):
    return ndb.Key(DatastoreDictAncestorModel,
                   DatastoreDictAncestorModel.generate_key(cls).string_id(),
                   cls, key.lower(),
                   namespace=&#39;&#39;)


@ndb.transactional
def get_all(cls):
    return cls.query(
        ancestor=DatastoreDictAncestorModel.generate_key(cls)).fetch()


@ndb.transactional
def get(cls, key):
    return build_key(cls, key).get()


@ndb.transactional
def get_or_create(cls, key, value=None):
    key = build_key(cls, key)

    instance = key.get()
    if instance:
        return instance, False

    instance = cls(key=key, value=value)
    instance.put()

    return instance, True


@ndb.transactional
def delete(cls, key):
    key = build_key(cls, key)
    return key.delete()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last item of note is the use of a parent for each DatastoreDict. This common
ancestor forces strong read consistency for the &lt;code&gt;get_all&lt;/code&gt; method, allowing us to
update a dictionary and have a consistent view of the data on subsequent reads.
We use an additional model to provide the strong read consistency.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DatastoreDictAncestorModel(ndb.Model):

    @classmethod
    def generate_key(cls, child_cls):
        key_name = &#39;__%s-%s__&#39; % (&#39;ancestor&#39;, child_cls.__name__)
        return ndb.Key(cls, key_name, namespace=&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Continuous Delivery Distilled</title>
      <link>http://sookocheff.com/posts/2015-04-23-continuous-delivery-distilled/</link>
      <pubDate>Thu, 23 Apr 2015 08:32:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-04-23-continuous-delivery-distilled/</guid>
      <description>

&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/-B0ZEHmBCH8&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;What if you could deliver more value, with more speed and with more stability?&lt;/p&gt;

&lt;p&gt;What if you could triage bugs faster?&lt;/p&gt;

&lt;p&gt;What if you could fix bugs easier and with less user facing impact?&lt;/p&gt;

&lt;p&gt;You can, with continuous delivery.&lt;/p&gt;

&lt;h2 id=&#34;terminology:682641b8f6aff1fdce173832bfccddae&#34;&gt;Terminology&lt;/h2&gt;

&lt;p&gt;First, some terminology. What distinguishes continuous integration, continuous
deployment and continuous delivery? Continuous integration revolves around the
continuous automated testing of software whenever change to the software is
made. Continuous deployment is the practice of automatically deploying any
change to the code. Continuous delivery implies that you can deploy any change
to production but for any number of reasons you may choose not to. The focus of
this article is on continuous delivery.&lt;/p&gt;

&lt;h2 id=&#34;what-is-continuous-delivery:682641b8f6aff1fdce173832bfccddae&#34;&gt;What is continuous delivery?&lt;/h2&gt;

&lt;p&gt;Continuous delivery is a set of development practices that allow you to release
software to production at any time (&lt;a href=&#34;http://martinfowler.com/bliki/ContinuousDelivery.html&#34; title=&#34;Continuous Delivery&#34;&gt;Fowler, 2014&lt;/a&gt;). By following
these practices you can reduce the cost, development time and risk  of
delivering features to users (&lt;a href=&#34;http://www.thoughtworks.com/insights/blog/case-continuous-delivery&#34; title=&#34;The Case for Continuous Delivery&#34;&gt;Humble, 2014&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s expand upon these definitions by talking about what differentiates
continuous delivery from traditional software development. With continuous
delivery, at any point in time any stakeholder in the business can ask for the
current version of the software to be deployed to production. This implies that
your software is deployable throughout the development lifecycle and that the
development team prioritizes keeping the software stable and deployable over
working on new features (&lt;a href=&#34;http://martinfowler.com/bliki/ContinuousDelivery.html&#34; title=&#34;Continuous Delivery&#34;&gt;Fowler, 2014&lt;/a&gt;). It also implies some level
of testing and deployment automation.&lt;/p&gt;

&lt;p&gt;The following diagram of the continuous delivery process helps to visualize the
automation steps that typically accompany a software change and the subsequent
release of the software to production. The red bars in the diagram signal that
failures at this stage of the process halt the entire process.&lt;/p&gt;

&lt;p&gt;First, the delivery team or development team makes a change and commits that
change to a version control system. This check in triggers automated unit tests
that verify the commit. If those unit tests pass, automated acceptance tests run
and if those pass we move on to manual user testing. Once the user has tested
and approved the change a release can go out. From a development standpoint it
is important to understand that any code commit that passes testing may be
released to customers at &lt;em&gt;any&lt;/em&gt; point in time.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://commons.wikimedia.org/wiki/File:Continuous_Delivery_process_diagram.png#/media/File:Continuous_Delivery_process_diagram.png&#34;&gt;
  &lt;img src=&#34;http://upload.wikimedia.org/wikipedia/commons/7/74/Continuous_Delivery_process_diagram.png&#34; alt=&#34;Continuous Delivery process diagram - Jez Humble&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;why-continuous-delivery:682641b8f6aff1fdce173832bfccddae&#34;&gt;Why Continuous Delivery?&lt;/h2&gt;

&lt;p&gt;Google, Facebook, LinkedIn, Netflix, Etsy, Ebay, Github and Hewlett-Packard,
among many others, have adopted continuous delivery in their products. On
average, Amazon makes changes to production code every 11.6 seconds
(&lt;a href=&#34;https://www.youtube.com/watch?v=dxk8b9rSKOo&#34; title=&#34;Velocity Culture&#34;&gt;Jenkins, 2011&lt;/a&gt;) &amp;ndash; that&amp;rsquo;s 3000 production deployments every
day. Facebook commits to master 5000 times a day deploys to production twice
a day (&lt;a href=&#34;http://www.infoq.com/presentations/Facebook-Release-Process&#34; title=&#34;The Facebook Release Process&#34;&gt;Rossi, 2011&lt;/a&gt;). Etsy deploys more than 50 times a day
(&lt;a href=&#34;http://www.infoq.com/news/2014/03/etsy-deploy-50-times-a-day&#34; title=&#34;How Etsy Deploys More Than 50 Times a Day&#34;&gt;Miranda, 2014&lt;/a&gt;). Why would companies do this? What is the
benefit?&lt;/p&gt;

&lt;h3 id=&#34;reduced-risk:682641b8f6aff1fdce173832bfccddae&#34;&gt;Reduced Risk&lt;/h3&gt;

&lt;p&gt;The first benefit is reduced risk. Each deployment is a smaller change that can
easily be understood in isolation. If an error occurs it is trivially easy to
roll-back a single change or push a new release on top of the change.&lt;/p&gt;

&lt;h3 id=&#34;believable-progress:682641b8f6aff1fdce173832bfccddae&#34;&gt;Believable Progress&lt;/h3&gt;

&lt;p&gt;Developers are generally quite bad at estimating software delivery projects
([Milstein, 2013][Milstein2103]). If the definition of &amp;ldquo;done&amp;rdquo; means &amp;ldquo;developers
declare it to be done&amp;rdquo; that is much less believable than if it&amp;rsquo;s safely deployed
into a production environment.&lt;/p&gt;

&lt;h3 id=&#34;faster-iteration-towards-product-fit:682641b8f6aff1fdce173832bfccddae&#34;&gt;Faster Iteration Towards Product Fit&lt;/h3&gt;

&lt;p&gt;Generally speaking, the biggest risk in software development is building
something that the user doesn&amp;rsquo;t want. Continuous delivery is a great enabler of
A/B testing and allows you to frequently get working software in front of real
users to assess user behaviour and performance impact of software changes.&lt;/p&gt;

&lt;h3 id=&#34;expose-inefficiencies:682641b8f6aff1fdce173832bfccddae&#34;&gt;Expose Inefficiencies&lt;/h3&gt;

&lt;p&gt;Continuous delivery enforces discipline on the software development team to
always keep the product in deployable condition. This discipline naturally
exposes inefficiencies in the development process &amp;ndash; anything that gets in the
way of the goal of releasing working software quickly is an impediment to
development that will quickly be brought to light with continuous delivery.&lt;/p&gt;

&lt;h3 id=&#34;encourage-responsibility:682641b8f6aff1fdce173832bfccddae&#34;&gt;Encourage Responsibility&lt;/h3&gt;

&lt;p&gt;With continuous delivery, the developer making a change and the developer
deploying the code is the same person. This avoids any problems with handing
your deployment &amp;lsquo;over the wall&amp;rsquo; and allowing another person or team to test,
deploy and verify the code. It keeps the onus on working software with the
people most knowledge about how the software works.&lt;/p&gt;

&lt;h2 id=&#34;does-continuous-delivery-actually-work:682641b8f6aff1fdce173832bfccddae&#34;&gt;Does Continuous Delivery Actually Work?&lt;/h2&gt;

&lt;p&gt;Rather than reflect on a few abstract benefits, let&amp;rsquo;s look at some of the
available data on continuous delivery.&lt;/p&gt;

&lt;p&gt;ThoughtWorks (&lt;a href=&#34;http://www.thoughtworks.com/insights/blog/case-continuous-delivery&#34; title=&#34;The Case for Continuous Delivery&#34;&gt;Humble, 2014&lt;/a&gt;) analyzed their data on high performing
companies and found that those practicing continuous delivery ship code 30 times
faster, have 50% fewer failed deployments, and restore service 12 times faster
than their peers.&lt;/p&gt;

&lt;p&gt;In A Practical Approach to Large-Scale Agile Development (&lt;a href=&#34;http://www.amazon.ca/Practical-Approach-Large-Scale-Agile-Development/dp/0321821726&#34; title=&#34;A Practical Approach to Large-Scale Agile Development: How HP Transformed LaserJet FutureSmart Firmware&#34;&gt;Gruver,
2012&lt;/a&gt;). Hewlett-Packard, who had been practicing more traditional
software delivery process, experimented with continuous deployment in an
organization having roughly 400 developers working over 3 continents. After
switching to continuous delivery, they integrated small changesets over 100
times a day and deployed at least ten times a day. What happened? The number
of features under active development increased by 140% and development costs
per feature reduced by 78%. This amounted to a total development cost
reduction of 40%.&lt;/p&gt;

&lt;h2 id=&#34;how-to-do-continuous-delivery:682641b8f6aff1fdce173832bfccddae&#34;&gt;How to do Continuous Delivery?&lt;/h2&gt;

&lt;p&gt;At this point, you may be sold on the benefits of continuous delivery and are
asking how to get started. Continuous delivery requires a few components to be
effective.&lt;/p&gt;

&lt;h4 id=&#34;1-continuous-integration:682641b8f6aff1fdce173832bfccddae&#34;&gt;1. Continuous Integration&lt;/h4&gt;

&lt;p&gt;A build server performing continuous intergration of every commit is a
necessity. Once a code change is committed, the build server triggers the
testing and deployment pipeline ultimately leading to successfully deploying a
production release.&lt;/p&gt;

&lt;h4 id=&#34;2-automated-testing:682641b8f6aff1fdce173832bfccddae&#34;&gt;2. Automated Testing&lt;/h4&gt;

&lt;p&gt;Automated unit testing, and, where applicable, automated performance testing,
makes it easy to spot issues in code about to be deployed. If any of
these tests fail the current release is rejected. This is not a silver
bullet. Manual QA is still needed to verify a build and test before
releasing.&lt;/p&gt;

&lt;h4 id=&#34;3-feature-flags:682641b8f6aff1fdce173832bfccddae&#34;&gt;3. Feature Flags&lt;/h4&gt;

&lt;p&gt;Some features are too big to commit as one chunk. In these cases a &lt;a href=&#34;http://martinfowler.com/bliki/FeatureToggle.html&#34;&gt;feature
flag&lt;/a&gt; is used to hide
functionality that is not ready for general release, while still allowing code
to be released to production.&lt;/p&gt;

&lt;h4 id=&#34;4-monitoring:682641b8f6aff1fdce173832bfccddae&#34;&gt;4. Monitoring&lt;/h4&gt;

&lt;p&gt;Monitoring systems allow the development and test teams to easily see the effect
a given change has on user behaviour, system performance or system stability.&lt;/p&gt;

&lt;h4 id=&#34;5-one-click-deployment-and-roll-back:682641b8f6aff1fdce173832bfccddae&#34;&gt;5. &amp;ldquo;One-Click&amp;rdquo; Deployment and Roll-Back.&lt;/h4&gt;

&lt;p&gt;Deployments and roll-backs must be easy enough for anyone to do at a moments
notice.&lt;/p&gt;

&lt;h2 id=&#34;continuous-delivery-in-practice:682641b8f6aff1fdce173832bfccddae&#34;&gt;Continuous Delivery in Practice&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s run through three examples of how continuous delivery would look like in
practice, contrasting continuous delivery with a more traditional release
process. In these examples, the traditional release process assumes that any
changes scheduled to be released are held in a development environment for one
week, a staging environment for one week, and finally deployed to a production
environment after one week on the staging environment. Each envrionment
corresponds to a unique code branch (develop, test, master) and weekly merges
take place to push the development code up to the test branch (and staging
environment) and the test code up to the master branch (and production
environment).&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/continuous-delivery-in-practice.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/continuous-delivery-in-practice.png&#34; alt=&#34;Traditional Release Structure&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h3 id=&#34;scenario-1-bug-fix:682641b8f6aff1fdce173832bfccddae&#34;&gt;Scenario 1: Bug Fix&lt;/h3&gt;

&lt;p&gt;Imagine a scenario where a customer reports a bug in the system. The bug is
simple enough for a single developer to work on and the fix is small enough to
understand within a single code commit. Let&amp;rsquo;s begin by examining the traditional
release process to see how this bug fix reaches the customer.&lt;/p&gt;

&lt;p&gt;The developer, Brad, begins by checking out the latest copy of the development
branch, and begins work on the bug. Once he is confident that the bug has been
fixed he goes over the changes with QA and merges the bug into the develop
branch where it waits for the weekly deployment to test.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-1.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-1.png&#34; alt=&#34;Merging to Develop&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Brad is now free to pick up another issue and commit the code for that issue to
develop, where it waits once again for the weekly deployment to test. Test now
has two issues that have been committed to development that will be released to
the test environment in one week.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-2.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-2.png&#34; alt=&#34;Second Merge to Develop&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Meanwhile, other developers are working on issues and committing the code to
develop. By the time the weekly deployment to the testing environment comes
around we end up with 13 disparate issues being pushed to the test environment.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-3.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-3.png&#34; alt=&#34;Group of Merges to Develop&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Now, the QA team can perform regression testing of all of these 13 issues for
the week that this release is held in the test environment for staging. After
one week has passed, the test branch is merged with the master branch and a
deployment to production is done.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-4.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/bug-fix-stage-4.png&#34; alt=&#34;Group of Merges to Develop&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also important to note that &lt;em&gt;we still do not know that the bug fix will
address the customer&amp;rsquo;s issues on production&lt;/em&gt;. We can&amp;rsquo;t know for sure that the
bug fix not complete until it fixes the issue on the production environment. So,
after the release a prudent developer will check back to make sure the bug
is no longer an issue and it can be marked as resolved.&lt;/p&gt;

&lt;p&gt;At this point we delivered the bug fix to the customer after a two week waiting
period. We also dedicated testing time to this bug fix before merging it to the
development branch, to regression test the release in the staging environment,
and to test this bug fix on the production environment. For
arguments sake, let&amp;rsquo;s say this testing time took 1 hour on each
environment.&lt;/p&gt;

&lt;h4 id=&#34;total-customer-time-waiting-for-this-bug-fix:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Customer Time Waiting For This Bug Fix:&lt;/h4&gt;

&lt;p&gt;2 weeks&lt;/p&gt;

&lt;h4 id=&#34;total-testing-time:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Testing Time:&lt;/h4&gt;

&lt;p&gt;3 hours&lt;/p&gt;

&lt;p&gt;Now imagine we have 13 issues that have been delivered with this release, we can
compound the total waiting time and total testing time.&lt;/p&gt;

&lt;h4 id=&#34;total-customer-time-waiting-for-this-release:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Customer Time Waiting For This Release:&lt;/h4&gt;

&lt;p&gt;26 weeks&lt;/p&gt;

&lt;h4 id=&#34;total-testing-time-for-this-release:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Testing Time For This Release:&lt;/h4&gt;

&lt;p&gt;39 hours&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s contrast this with a continuous delivery approach. In this scenario, Alice
works on an issue by first checking out the latest production code from the
master branch. Once she is confident she has fixed the bug and it has passed QA,
she merges the bug in to the master branch and deploys the fix to
production. Let&amp;rsquo;s assume that she took two hours to fix the bug and that
the bug required one hour of testing.&lt;/p&gt;

&lt;h4 id=&#34;total-customer-time-waiting-for-this-release-1:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Customer Time Waiting For This Release:&lt;/h4&gt;

&lt;p&gt;2 hours&lt;/p&gt;

&lt;h4 id=&#34;total-testing-time-for-this-release-1:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Testing Time For This Release:&lt;/h4&gt;

&lt;p&gt;1 hour&lt;/p&gt;

&lt;p&gt;We can compound this by assuming we have 13 issues that are being worked on for the week.&lt;/p&gt;

&lt;h4 id=&#34;total-customer-time-waiting-for-this-week:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Customer Time Waiting For This Week:&lt;/h4&gt;

&lt;p&gt;26 hours&lt;/p&gt;

&lt;h4 id=&#34;total-testing-time-for-this-week:682641b8f6aff1fdce173832bfccddae&#34;&gt;Total Testing Time For This Week:&lt;/h4&gt;

&lt;p&gt;13 hours&lt;/p&gt;

&lt;h3 id=&#34;scenario-2-regressions-and-rollback:682641b8f6aff1fdce173832bfccddae&#34;&gt;Scenario 2: Regressions and Rollback&lt;/h3&gt;

&lt;p&gt;Now imagine that the bug fix in the scenario above actually causes a regression
on production that needs to be fixed immediately or rolled back.&lt;/p&gt;

&lt;p&gt;In Brad&amp;rsquo;s case (the weekly release process), someone on the devops or change
management team packages the production release and pushes it to the production
environment. And something goes wrong. Devops knows that one of 13 different
change sets have been released but have no way of knowing which of those change
sets is causing the regression. A critical issue is created identifying the
problem and this issue is handed off to the development team. The team works to
triage the issue, notices that Brad&amp;rsquo;s change caused the problem and Brad is now
in charge of fixing it. But the last time Brad worked on this piece of code was
two weeks ago and his memory is a bit fuzzy about why the change was made. Or
maybe Brad is on holiday and someone else needs to pick up his work without
fully understanding the intricacies and risks involved with the chage.
Ultimately, the team decides they can&amp;rsquo;t go forward and all 13 change sets are
rolled back until they can properly fix the problem.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/weekly-regression.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/weekly-regression.png&#34; alt=&#34;Regression in Weekly Release&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Contrast this with a continuous delivery approach. Alice works with QA to verify
her change. Alice and QA deploy the change to production and immediately verify
that the integrity of the fix. And something goes wrong. In this case, there is
only one change set that could have cause the problem &amp;ndash; Alice&amp;rsquo;s. Alice has
immediate knowledge of the changes she just committed and possible reasons for a
failure. She can choose at this point to fix the issue and release her fix or to
roll-back her single change. In this scenario, Alice is responsible for the
integrity of her changes and for verifying that her work was done correctly. She
is able to work in concert with QA to test the issue and does not simply push
her issue &amp;lsquo;over the wall&amp;rsquo; for someone else to test and deploy.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/continuous-regression.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/continuous-regression.png&#34; alt=&#34;Regression With Continuous Delivery&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;With continuous delivery, each deployment is a smaller change that can be easily
understood, fixed or, when necessary, rolled-back.&lt;/p&gt;

&lt;h3 id=&#34;scenario-3-new-features:682641b8f6aff1fdce173832bfccddae&#34;&gt;Scenario 3: New Features&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve seen how continuous delivery can aid in deploying bug fixes, but what
about delivery new features? Remember that with continuous delivery any commit
at any time can be deployed directly to the production environment. So how can
you deploy partially complete features? The answer is &lt;a href=&#34;http://martinfowler.com/bliki/FeatureToggle.html&#34;&gt;feature
flags&lt;/a&gt;. Feature flags allow
the developer to write a new feature or edit an existing feature without
exposing those changes to the end user.&lt;/p&gt;

&lt;p&gt;For a brand new feature, it&amp;rsquo;s relatively easy to develop the entire feature
behind a feature flag that is inaccessible to the user by not exposing the new
page, button or widget at all. Once the feature matures it can be opened up to
QA or product managers for testing and eventually rolled out to a small
percentage of users. These users are able to test the feature with real
production data and real production load &amp;ndash; making sure everything works as
expected.&lt;/p&gt;

&lt;p&gt;Gradually rolling out the feature also gives you the ability to &lt;em&gt;measure user
behaviour&lt;/em&gt; and &lt;em&gt;gather feedback&lt;/em&gt; before committing to a certain path of action.&lt;/p&gt;

&lt;p&gt;When enhancing existing features or doing refactoring, feature flags work best
with continuous delivery when following a &lt;a href=&#34;http://martinfowler.com/bliki/ParallelChange.html&#34;&gt;parallel
change&lt;/a&gt; design pattern, where
both the old and the new code is run during a request, but only one version of
the result is returned to the user. As a concrete example, imagine we are trying
to improve the performance of a page through a refactoring. When a request comes
in, we route the request to both the old and new code and can measure &amp;ndash; on
production &amp;ndash; the performance of the new code. We can easily see if our proposed
refactoring has measurable performance improvements in a real-world setting.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/parallel-code.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/parallel-code.png&#34; alt=&#34;Parallel Code&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;For example, by extracting performance measurements over each iteration of the
code we visually compare the effect of a code change.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/performance-comparison.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/performance-comparison.png&#34; alt=&#34;Performance Comparison&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We can also use the same pattern to ensure we have confidence in the results of
our new code. For example, on each request, we can run both the old and the new
code, and compare the results on real-world production data. When we are
confident that the differences between the new and old code are within an
acceptable error range the new code is ready to go live. We also have the
ability to use production data to inform our unit tests and guard against future
regressions.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/accuracy-comparison.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-04-23-continuous-delivery-distilled/accuracy-comparison.png&#34; alt=&#34;Accuracy Comparison&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Caution must be exercised whenever using feature flags. Every feature flag that
is in use within the product is technical debt that should be short lived.&lt;/p&gt;

&lt;h2 id=&#34;towards-continuous-delivery:682641b8f6aff1fdce173832bfccddae&#34;&gt;Towards Continuous Delivery&lt;/h2&gt;

&lt;p&gt;Continuous delivery is not a panacea &amp;ndash; it requires diligence and responsibility
on behalf of the development team. However, if the team is able to cross these
hurdles continuous delivery can be used to deliver stable software to customers
faster than ever before.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating a BigQuery Table using the Java Client Library</title>
      <link>http://sookocheff.com/posts/2015-03-23-creating-a-big-query-table-java-api/</link>
      <pubDate>Mon, 23 Mar 2015 15:32:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-03-23-creating-a-big-query-table-java-api/</guid>
      <description>&lt;p&gt;I haven&amp;rsquo;t been able to find great documentation on creating a BigQuery
TableSchema using the Java Client Library. This blog post hopes to rectify that
:).&lt;/p&gt;

&lt;p&gt;You can use the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/bigquery-samples-java&#34;&gt;BigQuery sample
code&lt;/a&gt; for an idea
of how to create a client connection to BigQuery. Assuming you have the
connection set up you can start by creating a new &lt;code&gt;TableSchema&lt;/code&gt;. The
&lt;code&gt;TableSchema&lt;/code&gt; provides a method for setting the list of fields that make up the
columns of your BigQuery Table. Those columns are defined as an Array of
&lt;code&gt;TableFieldSchema&lt;/code&gt; objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;ArrayList&amp;lt;TableFieldSchema&amp;gt; fieldSchema = new ArrayList&amp;lt;TableFieldSchema&amp;gt;();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For simple types you can populate your columns with the correct type and mode
according to the &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/v2/tables#resource&#34;&gt;BigQuery API
documentation&lt;/a&gt;.
For example, to create a STRING field that is NULLABLE you can use the
following.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(new TableFieldSchema().setName(&amp;quot;username&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;NULLABLE&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And for repeated fields you can use the REPEATED mode.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(new TableFieldSchema().setName(&amp;quot;email&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;REPEATED&amp;quot;));
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create nested records you specify the parent as a RECORD mode and then call
&lt;code&gt;setFields&lt;/code&gt; for each column of nested data you want to insert. The columns of a
nested type are the same format as for the parent &amp;ndash; a list of TableFieldSchema
objects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;fieldSchema.add(
  new TableFieldSchema().setName(&amp;quot;location&amp;quot;).setType(&amp;quot;RECORD&amp;quot;).setFields(
    new ArrayList&amp;lt;TableFieldSchema&amp;gt;() {
      {
        add(new TableFieldSchema().setName(&amp;quot;city&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        add(new TableFieldSchema().setName(&amp;quot;address&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        add(new TableFieldSchema().setName(&amp;quot;zipcode&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
      }
    }
  )
);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last step is to set the entire schema as the fields of our table schema.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;TableSchema schema = new TableSchema();
schema.setFields(fieldSchema);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we set a &lt;code&gt;TableReference&lt;/code&gt; that holds the current project id, dataset id and
table id. We use this &lt;code&gt;TableReference&lt;/code&gt; to create our &lt;code&gt;Table&lt;/code&gt; using the &lt;code&gt;TableSchema&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;TableReference ref = new TableReference();
ref.setProjectId(PROJECT_ID);
ref.setDatasetId(&amp;quot;pubsub&amp;quot;);
ref.setTableId(&amp;quot;review_test&amp;quot;);

Table content = new Table();
content.setTableReference(ref);
content.setSchema(schema);

client.tables().insert(ref.getProjectId(), ref.getDatasetId(), content).execute();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together gives you a working sample of creating a BigQuery Table using the Java Client Library.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-java&#34;&gt;public static void main(String[] args) throws IOException, InterruptedException {
  Bigquery client = createAuthorizedClient(); // As per the BQ sample code
  
  ArrayList&amp;lt;TableFieldSchema&amp;gt; fieldSchema = new ArrayList&amp;lt;TableFieldSchema&amp;gt;();
  
  fieldSchema.add(new TableFieldSchema().setName(&amp;quot;username&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;NULLABLE&amp;quot;));
  fieldSchema.add(new TableFieldSchema().setName(&amp;quot;email&amp;quot;).setType(&amp;quot;STRING&amp;quot;).setMode(&amp;quot;REPEATED&amp;quot;));
  fieldSchema.add(
    new TableFieldSchema().setName(&amp;quot;location&amp;quot;).setType(&amp;quot;RECORD&amp;quot;).setFields(
      new ArrayList&amp;lt;TableFieldSchema&amp;gt;() {
        {
          add(new TableFieldSchema().setName(&amp;quot;city&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
          add(new TableFieldSchema().setName(&amp;quot;address&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
          add(new TableFieldSchema().setName(&amp;quot;zipcode&amp;quot;).setType(&amp;quot;STRING&amp;quot;));
        }
  }));
  
  TableSchema schema = new TableSchema();
  schema.setFields(fieldSchema);
  
  TableReference ref = new TableReference();
  ref.setProjectId(&amp;quot;&amp;lt;YOUR_PROJECT_ID&amp;gt;&amp;quot;);
  ref.setDatasetId(&amp;quot;&amp;lt;YOUR_DATASET_ID&amp;gt;&amp;quot;);
  ref.setTableId(&amp;quot;&amp;lt;YOUR_TABLE_ID&amp;gt;&amp;quot;);
  
  Table content = new Table();
  content.setTableReference(ref);
  content.setSchema(schema);
  
  client.tables().insert(ref.getProjectId(), ref.getDatasetId(), content).execute();
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Deploying R Studio on Compute Engine</title>
      <link>http://sookocheff.com/posts/r/deploying-r-studio-to-compute-engine/</link>
      <pubDate>Mon, 23 Mar 2015 15:32:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/r/deploying-r-studio-to-compute-engine/</guid>
      <description>

&lt;p&gt;Sometimes you have a data analysis problem that is just too big for your desktop
or laptop. The limiting factor here is generally RAM. Thankfully, services like
Google Compute Engine allow you to lease servers with up to 208GB of RAM, large
enough for a wide variety of intensive tasks. An ancillary benefit of using a
service like Compute Engine is that it allows you to easily load your data from
a Cloud Storage Bucket, meaning you don&amp;rsquo;t need to keep a copy of the large
dataset locally at all times.&lt;/p&gt;

&lt;p&gt;R Studio has a remote mode allowing you to install it on a server with access
through a remote interface. This tutorial details how to start a Compute Engine
instance, install R Studio on it and access R Studio from the remote interface.&lt;/p&gt;

&lt;p&gt;The rest of this tutorial assumes that you have a Google Cloud Platform account
with billing enabled and have installed the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;Google Cloud
SDK&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;deploying-a-compute-engine-instance:a84679a16a118ba5a9c46f36f1d63ec8&#34;&gt;Deploying a Compute Engine Instance&lt;/h2&gt;

&lt;p&gt;The first step is to deploy your Compute Engine instance. The &lt;code&gt;gcloud compute&lt;/code&gt;
command allows you to create instances. The only required parameter to create an
instance is the instance name. We will call our instance &lt;code&gt;r-studio&lt;/code&gt; but you can
choose any name you like. R Studio Server is typically built on Ubuntu so it is
safest to use the Ubuntu distribution for your server.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute instances create r-studio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will be prompted to choose a
&lt;a href=&#34;https://cloud.google.com/compute/docs/zones&#34;&gt;Zone&lt;/a&gt;. Just choose a zone close to
you. You can also specify the zone when creating the instance using the &lt;code&gt;--zone&lt;/code&gt;
parameter. For example.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute instances create r-studio --zone us-central1-a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will also have to open the Compute Engine firewall to allow port 8787 for R
Studio.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcutil addfirewall allow-r-studio --allowed=tcp:8787
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;installing-r-studio:a84679a16a118ba5a9c46f36f1d63ec8&#34;&gt;Installing R Studio&lt;/h2&gt;

&lt;p&gt;Once we have our Compute Engine instance set up, we log in to the machine using ssh.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute ssh r-studio --zone us-central1-a
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we are logged in to the Compute Engine instance, it&amp;rsquo;s time to install
R by first updating the Debian apt-get repository and then installing R.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo apt-get update
sudo apt-get install r-base r-base-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R Studio currently requries OpenSSL version 0.9.8. We need to install this
separately and then install install R Studio&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;wget http://ftp.us.debian.org/debian/pool/main/o/openssl/libssl0.9.8_0.9.8o-4squeeze14_amd64.deb
sudo dpkg -i libssl0.9.8_0.9.8o-4squeeze14_amd64.deb
sudo apt-get install gdebi-core
wget http://download2.rstudio.org/rstudio-server-0.98.1103-amd64.deb
sudo gdebi rstudio-server-0.98.1103-amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should be up and running with R Studio on your compute engine instance. To
verify, navigate to the IP address of your Compute Engine instance on port 8787
(the default R Studio port).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;http://&amp;lt;ipaddress&amp;gt;:8787
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;R Studio only permits access to users of the system, we can add a user with
standard Linux tools like adduser. For example, to create a new user named
rstudio and specify the password you could execute the following commands.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo adduser rstudio
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You will be prompted to enter a password for the user and confirm the users name
and phone number.&lt;/p&gt;

&lt;p&gt;Afterwards, logging in with the user you created will present a web UI of the
familiar R Studio. You can now perform analysis on those larger data sets using
the R Studio that just weren&amp;rsquo;t possible on a laptop.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Keeping App Engine Search Documents and Datastore Entities In Sync</title>
      <link>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</link>
      <pubDate>Mon, 23 Feb 2015 08:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</guid>
      <description>

&lt;p&gt;At Vendasta the App Engine Datastore serves as the single point of truth for
most operational data and the majority of interactions are against this single
point of truth. However, a piece of required functionality in many of our
products is to provide a searchable view of the data in the App Engine
Datastore. Search is difficult using the Datastore and so we have moved to using
the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/search/&#34;&gt;Search API&lt;/a&gt; as a
managed solution for searching datastore entities. In this use case, every edit
to an entity in the Datastore is reflected as a change to a Search Document.
This article details an architecture for keeping Datastore entities and Search
Documents in sync throughout failure and race conditions.&lt;/p&gt;

&lt;h2 id=&#34;updating-the-search-document-using-a-post-put-hook:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Updating the Search Document Using a _post_put_hook&lt;/h2&gt;

&lt;p&gt;To ensure that every put of an entity to the Datastore results in an update to
the associated search document, we update the search document in the
_post_put_hook of the entity. The _post_put_hook is executed every time
the entity is put so each time the entity has changed we will put a new and
updated search document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def _post_put_hook(self, future):
        document = search.Document(
            doc_id = self.username,
            fields=[
               search.TextField(name=&#39;username&#39;, value=self.username),
               search.TextField(name=&#39;email&#39;, value=self.email),
               ])
        try:
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)
        except search.Error:
            logging.exception(&#39;Put failed&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Updating the search document during every put as part of the post put hook is a
light weight way to keep the search document up-to-date with changes to the
entity. However, this design does not account for the potential error conditions
where putting the search document or the Datastore entity fails. We will need
some additional functionality to handle these cases.&lt;/p&gt;

&lt;h2 id=&#34;handling-search-document-put-failures:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Search Document Put Failures&lt;/h2&gt;

&lt;p&gt;The first obstacle to overcome is handling failures when putting the search
document. One method for handling failures is retrying. We can add retrying to
our workflow by separating updating the search document into its own task and
deferring that task using the deferred library. This accomplishes two things.
First, moving the search document functionality into its own function makes our
code more modular. Second, the App Engine task queue mechanism allows us to
specify our retry semantics, handling backoff and failure conditions gracefully.
In this example, we allow infinite retries of failed tasks, allowing DevOps to find
search documents that may have become out of sync with their Datastore entities
and correct any problems that may arise. We assume in the example below that the
username acts as the ndb Key.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document, self.username)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;handling-datastore-put-failures:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Datastore Put Failures&lt;/h2&gt;

&lt;p&gt;The second obstacle to overcome is safely handling Datastore put failures. In
this architecture, each change to a Datastore entity is required to run within a
transaction. We update the _post_put_hook to queue a transactional task &amp;ndash; which
forces the task to only be queued if the current transaction has successfully
completed. This guarantees that failed Datastore puts will not result in search
documents being updated and becoming out of sync with the Datastore.&lt;/p&gt;

&lt;p&gt;We specify that the task should be run as a transaction by passing the result of
the &lt;code&gt;in_transaction&lt;/code&gt; function to the &lt;code&gt;_transactional&lt;/code&gt; parameter of &lt;code&gt;defer&lt;/code&gt;.
&lt;code&gt;in_transaction&lt;/code&gt; returns &lt;code&gt;True&lt;/code&gt; if the currently executing code is running in a
transaction and &lt;code&gt;False&lt;/code&gt; otherwise.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have sastisfied the case where either the search document or the
Datastore put has failed. If the search document put has failed we retry, if the
Datastore put has failed we do not put the search document. We still have one
remaining problem: Dirty Reads.&lt;/p&gt;

&lt;h2 id=&#34;handling-dirty-reads:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Dirty Reads&lt;/h2&gt;

&lt;p&gt;The last obstacle to overcome is dealing with race conditions that could lead to
reading stale data and writing that data to the search document. Consider the
case where two subsequent puts to the Datastore occur back-to-back within a
short time frame. Each of these puts will write new data to the Datastore and
queue a task to put the updated search document to the Datastore. The dirty
read problem arises when the second task to update the search document reads old
data from the Datastore that may not have been fully replicated throughout the
Datastore.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34; alt=&#34;Syncing Search Documents&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We can overcome this problem by versioning our tasks to coincide with the
version of our Datastore entity. We add a version number to the entity and
update the version number during a _pre_put_hook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    def _pre_put_hook(self):
        self.version = self.version + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now during the _post_put_hook we queue a task corresponding to the version number
of the Datastore entity we are putting. This ties the task to the point in time
when the Datastore entity was put.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    @classmethod
    def put_search_document(cls, username, version):
        model = ndb.Key(cls, username).get()
        if model:
            if version &amp;lt; model.version:
                logging.warning(&#39;Attempting to write stale data. Ignore&#39;)
                return

            if version &amp;gt; model.version:
                msg = &#39;Attempting to write future data. Retry to await consistency.&#39;
                logging.warning(msg)
                raise Exception(msg)

            # Versions match. Update the search document
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=model.username),
                   search.TextField(name=&#39;email&#39;, value=model.email),
                   search.TextField(name=&#39;version&#39;, value=model.version),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _pre_put_hook(self):
        self.version = self.version + 1

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       self.version,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the version number of the task being executed is less than the version number
written to the Datastore, we are attempting to write stale data and do not need
to process this request. If the version number is greater than the task being
executed, we are attempting to write data to the search document that has not
been fully replicated throughout the Datastore. In this case, we raise an
exception to retry putting the search document. In subsequent retries the data
will have propagated and our put will succeed. Note that if another task is
executed while the current task is retrying, the version number of our retrying
task will become stale and when the task is next executed we do not write the
now stale data to the search document.&lt;/p&gt;

&lt;p&gt;This still handles the case when a search document put fails &amp;ndash; whenever our
version number becomes out of sync due to the failed put, we do not write the
data to the search document. Furthermore, if our Datastore put fails then our
task to put the search document will not be queued &lt;em&gt;as long as the Datastore put
is run within a transaction&lt;/em&gt;. The version number will not be incremented in this
case because the value set during the _pre_put_hook will not be persisted during
a failed transaction.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Putting this all together, we&amp;rsquo;ve developed a solution for keeping search
documents in sync with Datastore entities that is robust to failure and race
conditions. This same technique can be used for syncing the state of any number
of datasets that are dependent on the Datastore being the single point of truth
in your system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Halting Python unittest Execution on First Error</title>
      <link>http://sookocheff.com/posts/2015-02-12-halting-unittest-execution-at-first-error/</link>
      <pubDate>Thu, 12 Feb 2015 06:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-12-halting-unittest-execution-at-first-error/</guid>
      <description>&lt;p&gt;We all know the importance of unit tests. Especially in a dynamic language like
Python. Occasionally you have a set of unit tests that are failing in a
cascading fashion where the first error case causes subsequent tests to fail
(these tests are likely no longer unit tests, but that&amp;rsquo;s a different
 discussion). To help isolate the offending test case in a see of failures you
can set the &lt;code&gt;unittest.TestCase&lt;/code&gt; class to halt after the first error by
overriding the &lt;code&gt;run&lt;/code&gt; method as follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class HaltingTestCase(unittest.TestCase):

    def run(self, result=None):
        &amp;quot;&amp;quot;&amp;quot; Stop after first error &amp;quot;&amp;quot;&amp;quot;
        if not result.errors:
            super(HaltingTestCase, self).run(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In this block of code, if we do not have any errors we call the super class to
continue running tests. If we have an error execution stops after this method
call. This allows you to pinpoint the first error case, fix it, and continue on
fixing subsequent tests.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Create a Google Cloud Dataflow Project with Gradle</title>
      <link>http://sookocheff.com/posts/2015-02-11-cloud-dataflow-with-gradle/</link>
      <pubDate>Wed, 11 Feb 2015 06:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-11-cloud-dataflow-with-gradle/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been experimenting with the &lt;a href=&#34;https://cloud.google.com/dataflow/&#34;&gt;Google Cloud
Dataflow&lt;/a&gt; &lt;a href=&#34;https://github.com/GoogleCloudPlatform/DataflowJavaSDK&#34;&gt;Java
SDK&lt;/a&gt; for running managed
data processing pipelines. One of the first tasks is getting a build environment
up and running. For this I chose Gradle.&lt;/p&gt;

&lt;p&gt;We start by declaring this a java application and listing the configuration
variables that declare the source compatibility level (which for now must be
1.7) and the main class to be executed by the &lt;code&gt;run&lt;/code&gt; task to be defined later.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apply plugin: &#39;java&#39;
apply plugin: &#39;application&#39;

sourceCompatibility = &#39;1.7&#39;

mainClassName = &#39;com.sookocheff.dataflow.Main&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then declare the mavenCentral repository where the dependencies are located
and the basic dependencies for a Cloud Dataflow application.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;repositories {
    mavenCentral()
}

dependencies {
    compile &#39;com.google.guava:guava:18.0&#39;
    compile &#39;com.google.cloud.dataflow:google-cloud-dataflow-java-sdk-all:0.3.150109&#39;
    
    testCompile &#39;junit:junit:4.11&#39;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last, we create our run task that will launch the Cloud Dataflow application.
The Cloud Dataflow runtime expects the folder &lt;code&gt;resources/main&lt;/code&gt; to exist in your
build. If you are not actually shipping any resources with your application you
will need to tell Gradle to create the correct directory. We also pass any
parameters to our main class using the -P flag.  These two steps are
encapsulated below.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;task resources {
    def resourcesDir = new File(&#39;build/resources/main&#39;)
    resourcesDir.mkdirs()
}

run {
    if (project.hasProperty(&#39;args&#39;)) {
        args project.args.split(&#39;\\s&#39;)
    }
}

run.mustRunAfter &#39;resources&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You should now be able to launch your Cloud Dataflow application using the
&lt;code&gt;gradle run&lt;/code&gt; task, passing your project identifiers as parameters. For example,&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gradle run -Pargs=&amp;quot;--project=&amp;lt;your-project&amp;gt; --runner=BlockingDataflowPipelineRunner --stagingLocation=gs://&amp;lt;staging-location&amp;gt;&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A pypiserver Deployment Script</title>
      <link>http://sookocheff.com/posts/2015-02-01-a-pypiserver-deployment-script/</link>
      <pubDate>Sun, 01 Feb 2015 14:53:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-01-a-pypiserver-deployment-script/</guid>
      <description>&lt;p&gt;At Vendasta we&amp;rsquo;ve been slowly adopting pypi and pip for our internal code
libraries and the time has come to deploy our own private pypi server. After
evaluating a few options I settled on the simplistic
&lt;a href=&#34;https://pypi.python.org/pypi/pypiserver&#34;&gt;pypiserver&lt;/a&gt; &amp;ndash; a barebones
implementation of the &lt;a href=&#34;https://pypi.python.org/simple/&#34;&gt;simple HTTP API&lt;/a&gt; to
pypi.&lt;/p&gt;

&lt;p&gt;The deployment uses nginx as a front-end to pypiserver. pypiserver itself is ran
and monitored using supervisord. I created a bash script that creates a user and
group to run pypiserver and installs and runs nginx, supervisord and pypiserver.
I&amp;rsquo;ve been running this bash script through Vagrant to deploy a custom pypiserver
for private use. I wanted to save this code for posterity and hopefully help
someone else working on the same task.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/usr/bin/env bash

STARTUP_VERSION=1
STARTUP_MARK=/var/startup.script.$STARTUP_VERSION

# Exit if this script has already ran
if [[ -f $STARTUP_MARK ]]; then
  exit 0  
fi

set -o nounset
set -o pipefail
set -o errexit

# Install prerequesites
sudo apt-get update
sudo apt-get install -y vim
sudo apt-get install -y apache2-utils
sudo apt-get install -y nginx
sudo apt-get install -y supervisor

# Install pip
wget &amp;quot;https://bootstrap.pypa.io/get-pip.py&amp;quot;
sudo python get-pip.py

# Install pypiserver with passlib for upload support
sudo pip install passlib
sudo pip install pypiserver

# Set the port configuration
proxy_port=8080
pypi_port=7201

# Create a user and group to run pypiserver
user=pypiusername
password=pypipasswrd
group=$user

sudo groupadd &amp;quot;$group&amp;quot;
sudo useradd $user -m -g $group -G $group
sudo -u $user -H -s eval &#39;htpasswd -scb $HOME/.htaccess&#39; &amp;quot;$user $password&amp;quot;
sudo -u $user -H -s eval &#39;mkdir -p $HOME/packages&#39;

##############
# nginx config
##############
echo &amp;quot;$user:$(openssl passwd -crypt $password)&amp;quot; &amp;gt; /etc/nginx/user.pwd

# nginx can&#39;t run as a daemon to work with supervisord
echo &amp;quot;daemon off;&amp;quot; &amp;gt;&amp;gt; /etc/nginx/nginx.conf

cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/nginx/sites-enabled/pypi-server.conf
server {
  listen $proxy_port;
  location / {
    proxy_pass http://localhost:$pypi_port;
    auth_basic &amp;quot;PyPi Authentication&amp;quot;;
    auth_basic_user_file /etc/nginx/user.pwd;
  }
}
EOF

rm /etc/nginx/sites-enabled/default

###################
# supervisor config
###################
cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/supervisor/conf.d/pypi-server.conf
[program:pypi-server]
command=pypi-server -p $pypi_port -P /home/$user/.htaccess /home/$user/packages
directory=/home/$user
user=$user
autostart=true
autorestart=true
stderr_logfile=/var/log/pypi-server.err.log
stdout_logfile=/var/log/pypi-server.out.log
EOF

cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/supervisor/conf.d/nginx.conf
[program:nginx]
command=/usr/sbin/nginx
autostart=true
autorestart=true
stdout_events_enabled=true
stderr_events_enabled=true
EOF

sudo supervisorctl reread
sudo supervisorctl update

touch $STARTUP_MARK
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Downloading files from Google Cloud Storage with webapp2</title>
      <link>http://sookocheff.com/posts/2015-01-27-webapp2-download-handler/</link>
      <pubDate>Tue, 27 Jan 2015 06:07:12 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-27-webapp2-download-handler/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been working on a simple App Engine application that offers upload and
download functionality to and from Google Cloud Storage. When it came time to
actually download the content I needed to write a webapp2 &lt;code&gt;RequestHandler&lt;/code&gt; that
will retrieve the file from Cloud Storage and return it to the client.&lt;/p&gt;

&lt;p&gt;The trick to this is to set the proper content type in your response header. In
the example below I used the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/googlecloudstorageclient/&#34;&gt;Cloud Storage Client
Library&lt;/a&gt;
to open and read the file, then set the response appropriately.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import webapp2
import cloudstorage

class FileDownloadHandler(webapp2.RequestHandler):

  def get(self, filename):
    self.response.headers[&#39;Content-Type&#39;] = &#39;application/x-gzip&#39;
    self.response.headers[&#39;Content-Disposition&#39;] = &#39;attachment; filename=%s&#39; % filename

    filename = &#39;/bucket/&#39; + filename
    gcs_file = cloudstorage.open(filename)
    data = gcs_file.read()
    gcs_file.close()

    self.response.write(data)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Querying App Engine Logs with Elasticsearch</title>
      <link>http://sookocheff.com/posts/2015-01-23-querying-app-engine-logs-with-elasticsearch/</link>
      <pubDate>Fri, 23 Jan 2015 06:15:07 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-23-querying-app-engine-logs-with-elasticsearch/</guid>
      <description>

&lt;p&gt;From a DevOps perspective having a historical record of application logs can aid
immensely in tracking down bugs, responding to customer questions, or finding
out when and why that critical piece of data was updated to the wrong value. One
of the biggest grievances with the built-in log handling of Google App Engine is
that historical logs are only available for the previous three days. We wanted
to do a little bit better and have logs available for a 30 day time period. This
article outlines a method we&amp;rsquo;ve developed for pushing App Engine logs to an
elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;A side benefit of this approach is that if you have multiple App Engine
projects, all of their logs can be searched at the same time. This provides an
immediate benefit when tracking down systems integration problems or parsing API
traffic between applications.&lt;/p&gt;

&lt;p&gt;The solution we chose for this problem revolves around the MapReduce API. If you
need a refresher on this API please check out my &lt;a href=&#34;http://sookocheff.com/series/mapreduce-api/&#34;&gt;MapReduce tutorial
series&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;overview:910e0d5fe0b44f1ddf1d486376d42c6d&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The gist of this solution is to run a MapReduce job that reads data from the
&lt;a href=&#34;https://cloud.google.com/appengine/docs/python/logs/&#34;&gt;App Engine Logs API&lt;/a&gt; using the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/master/python/src/mapreduce/input_readers.py#L1952&#34;&gt;LogInputReader&lt;/a&gt;,
converts the data to a JSON format for ingestion into elasticsearch, and finally
write the parsed data to the elasticsearch cluster using a &lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;custom MapReduce
OutputWriter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We execute this MapReduce job on a timer using cron to push logs to
elasticsearch on a specific schedule. In our case, we run this job every 15
minutes to provide a relatively recent view of current operational data.&lt;/p&gt;

&lt;p&gt;The following diagram presents the architecture of our solution.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-01-23-querying-app-engine-logs-with-elasticsearch/elasticsearch-sequence-diagram.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-01-23-querying-app-engine-logs-with-elasticsearch/elasticsearch-sequence-diagram.png&#34; alt=&#34;Architecture for Logging to elasticsearch&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;example:910e0d5fe0b44f1ddf1d486376d42c6d&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;The majority of the solution is contained in a MapperPipeline. The following
code illustrates how to setup the MapperPipeline. What&amp;rsquo;s remaining is to write a
&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;custom MapReduce OutputWriter&lt;/a&gt; that pushes data to
elasticsearch and a function that converts a RequestLog object to JSON suitable
for elasticsearch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class CronHandler(webapp2.RequestHandler):

    def get(self):
        run()


def run():
    start_time, end_time = get_time_range()
    logging.debug(&#39;Dumping logs for date range (%s, %s).&#39;, start_time, end_time)

    start_time = float(start_time.strftime(&#39;%s.%f&#39;))
    end_time = float(end_time.strftime(&#39;%s.%f&#39;))

    p = Log2ElasticSearch(start_time, end_time)
    p.start()


class Log2Elasticsearch(pipeline.Pipeline):

    def run(self, start_time, end_time, module_name, module_versions):
        &amp;quot;&amp;quot;&amp;quot;
        Args:
            module_versions: A list of tuples of the form (module, version), that
                indicate that the logs for the given module/version combination should be
                fetched.  Duplicate tuples will be ignored.
        &amp;quot;&amp;quot;&amp;quot;
        yield mapreduce_pipeline.MapperPipeline(
            &amp;quot;vlogs-elasticsearch-injestion&amp;quot;,
            handler_spec=&amp;quot;log2json&amp;quot;,
            input_reader_spec=&amp;quot;mapreduce.input_readers.LogInputReader&amp;quot;,
            output_writer_spec=&amp;quot;mapreduce.output_writers.ElasticSearchOutputWriter&amp;quot;,
            params={
                &amp;quot;input_reader&amp;quot;: {
                    &amp;quot;start_time&amp;quot;: start_time,
                    &amp;quot;end_time&amp;quot;: end_time,
                    &amp;quot;include_app_logs&amp;quot;: True,
                },
            },
            shards=16
        )
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
