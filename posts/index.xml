<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Kevin Sookocheff</title>
    <link>http://sookocheff.com/posts/index.xml/</link>
    <language>en-us</language>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <lastBuildDate>Sun, 04 Jan 2015 12:31:51 UTC</lastBuildDate>
    
    <item>
      <title>Parsing bash script options with getopts</title>
      <link>http://sookocheff.com/posts/2015-01-04-parsing-bash-script-arguments-with-shopts/</link>
      <pubDate>Sun, 04 Jan 2015 12:31:51 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-04-parsing-bash-script-arguments-with-shopts/</guid>
      <description>

&lt;p&gt;A common task in shell scripting is to parse command line arguments to your
script. Bash provides the &lt;code&gt;getopts&lt;/code&gt; built-in function to do just that. This
tutorial explains how to use the &lt;code&gt;getopts&lt;/code&gt; built-in function to parse arguments and options to a bash script.&lt;/p&gt;

&lt;p&gt;The &lt;code&gt;getopts&lt;/code&gt; function takes three parameters. The first is a specification of
which options are valid, listed as a sequence of letters. For example, the
string &lt;code&gt;&#39;ht&#39;&lt;/code&gt; signifies that the options &lt;code&gt;-a&lt;/code&gt; and &lt;code&gt;-l&lt;/code&gt; are valid.&lt;/p&gt;

&lt;p&gt;The second argument to &lt;code&gt;getopts&lt;/code&gt; is a variable that will be populated with the
option or argument to be processed next. In the following loop, &lt;code&gt;opt&lt;/code&gt; will hold
the value of the current option that has been parsed by &lt;code&gt;getopts&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:ht&amp;quot; opt; do
  case ${opt} in
    h ) # process option a
      ;;
    t ) # process option l
      ;;
    \? ) echo &amp;quot;Usage: cmd [-h] [-t]
      ;;
  esac
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This example shows a few additional features of &lt;code&gt;getopts&lt;/code&gt;. First, if an invalid
option is provided, the option variable is assigned the value &lt;code&gt;?&lt;/code&gt;. You can catch
this case and provide an appropriate usage message to the user. Second, this
behaviour is only true when you prepend the list of valid options with &lt;code&gt;:&lt;/code&gt; to
disable the default error handling of invalid options. It is recommended to
always disable the default error handling in your scripts.&lt;/p&gt;

&lt;p&gt;The third argument to &lt;code&gt;getopts&lt;/code&gt; is the list of arguments and options to be
processed. When not provided, this defaults to the arguments and options
provided to the application (&lt;code&gt;$@&lt;/code&gt;). You can provide this third argument to use
&lt;code&gt;getopts&lt;/code&gt; to parse any list of arguments and options you provide.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Shifting processed options&lt;/h2&gt;

&lt;p&gt;The variable &lt;code&gt;OPTIND&lt;/code&gt; holds the number of options parsed by the last call to
&lt;code&gt;getopts&lt;/code&gt;. It is common practice to call the &lt;code&gt;shift&lt;/code&gt; command at the end of your
processing loop to remove options that have already been handled from &lt;code&gt;$@&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Parsing options with arguments&lt;/h2&gt;

&lt;p&gt;Options that themselves have arguments are signified with a &lt;code&gt;:&lt;/code&gt;. The argument to
an option is placed in the variable &lt;code&gt;OPTARG&lt;/code&gt;. In the following example, the
option &lt;code&gt;t&lt;/code&gt; takes an argument. When the argument is provided, we copy its value
to the variable &lt;code&gt;target&lt;/code&gt;. If no argument is provided &lt;code&gt;getopts&lt;/code&gt; will set &lt;code&gt;opt&lt;/code&gt; to
&lt;code&gt;:&lt;/code&gt;. We can recognize this error condition by catching the &lt;code&gt;:&lt;/code&gt; case and printing
an appropriate error message.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:t:&amp;quot; opt; do
  case ${opt} in 
    t )
      target=$OPTARG
      ;;
    \? )
      echo &amp;quot;Invalid option: $OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
      ;;
    : )
      echo &amp;quot;Invalid option: $OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
      ;;
  esac
done
shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;An extended example &amp;ndash; parsing nested arguments and options&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s walk through an extended example of processing a command that takes
options, has a sub-command, and whose sub-command takes an additional option
that has an argument. This is a mouthful so let&amp;rsquo;s break it down using an
example. Let&amp;rsquo;s say we are writing our own version of the &lt;a href=&#34;https://pip.pypa.io/en/latest/&#34;&gt;&lt;code&gt;pip&lt;/code&gt;
command&lt;/a&gt;. In this version you can call &lt;code&gt;pip&lt;/code&gt;
with the &lt;code&gt;-h&lt;/code&gt; option to display a help message.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip -h
Usage: 
    pip -h                      Display this help message.
    pip install                 Install a Python package.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can use &lt;code&gt;getopts&lt;/code&gt; to parse the &lt;code&gt;-h&lt;/code&gt; option with the following &lt;code&gt;while&lt;/code&gt; loop.
In it we catch invalid options with &lt;code&gt;\?&lt;/code&gt; and &lt;code&gt;shift&lt;/code&gt; all arguments that have
been processed with &lt;code&gt;shift $((OPTIND -1))&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;while getopts &amp;quot;:h&amp;quot; opt; do
  case ${opt} in
    h )
      echo &amp;quot;Usage:&amp;quot;
      echo &amp;quot;    pip -h                      Display this help message.&amp;quot;
      echo &amp;quot;    pip install                 Install a Python package.&amp;quot;
      exit 0
      ;;
    \? )
      echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
      exit 1
      ;;
  esac
done
shift $((OPTIND -1))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s add the sub-command &lt;code&gt;install&lt;/code&gt; to our script.  &lt;code&gt;install&lt;/code&gt; takes as an
argument the Python package to install.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip install urllib3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;install&lt;/code&gt; also takes an option, &lt;code&gt;-t&lt;/code&gt;. &lt;code&gt;-t&lt;/code&gt; takes as an argument the location to
install the package to relative to the current directory.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; pip install urllib3 -t ./src/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To process this line we must find the sub-command to execute. This value is the
first argument to our script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;subcommand=$1
shift # Remove `pip` from the argument list
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can process the sub-command &lt;code&gt;install&lt;/code&gt;. In our example, the option &lt;code&gt;-t&lt;/code&gt; is
actually an option that follows the package argument so we begin by removing
&lt;code&gt;install&lt;/code&gt; from the argument list and processing the remainder of the line.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;case &amp;quot;$subcommand&amp;quot; in
  install)
    package=$1
    shift # Remove `install` from the argument list
    ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After shifting the argument list we can process the remaining arguments as if
they are of the form &lt;code&gt;package -t src/lib&lt;/code&gt;. The &lt;code&gt;-t&lt;/code&gt; option takes an argument
itself. This argument will be stored in the variable &lt;code&gt;OPTARG&lt;/code&gt; and we save it to
the variable &lt;code&gt;target&lt;/code&gt; for further work.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;case &amp;quot;$subcommand&amp;quot; in
  install)
    package=$1
    shift # Remove `install` from the argument list

  while getopts &amp;quot;:t:&amp;quot; opt; do
    case ${opt} in
      t )
        target=$OPTARG
        ;;
      \? )
        echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
        exit 1
        ;;
      : )
        echo &amp;quot;Invalid Option: -$OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
        exit 1
        ;;
    esac
  done
  shift $((OPTIND -1))
  ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together, we end up with the following script that parses
arguments to our version of &lt;code&gt;pip&lt;/code&gt; and its sub-command &lt;code&gt;install&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;package=&amp;quot;&amp;quot;  # Default to empty package
target=&amp;quot;&amp;quot;  # Default to empty target

# Parse options to the `pip` command
while getopts &amp;quot;:h&amp;quot; opt; do
  case ${opt} in
    h )
      echo &amp;quot;Usage:&amp;quot;
      echo &amp;quot;    pip -h                      Display this help message.&amp;quot;
      echo &amp;quot;    pip install &amp;lt;package&amp;gt;       Install &amp;lt;package&amp;gt;.&amp;quot;
      exit 0
      ;;
   \? )
     echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
     exit 1
     ;;
  esac
done
shift $((OPTIND -1))

subcommand=$1; shift  # Remove &#39;pip&#39; from the argument list
case &amp;quot;$subcommand&amp;quot; in
  # Parse options to the install sub command
  install)
    package=$1; shift  # Remove &#39;install&#39; from the argument list

    # Process package options
    while getopts &amp;quot;:t:&amp;quot; opt; do
      case ${opt} in
        t )
          target=$OPTARG
          ;;
        \? )
          echo &amp;quot;Invalid Option: -$OPTARG&amp;quot; 1&amp;gt;&amp;amp;2
          exit 1
          ;;
        : )
          echo &amp;quot;Invalid Option: -$OPTARG requires an argument&amp;quot; 1&amp;gt;&amp;amp;2
          exit 1
          ;;
      esac
    done
    shift $((OPTIND -1))
    ;;
esac
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After processing the above sequence of commands, the variable &lt;code&gt;package&lt;/code&gt; will
hold the package to install and the variable &lt;code&gt;target&lt;/code&gt; will hold the target to
install the package to. You can use this as a template for processing any set of
arguments and options to your scripts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Managing App Engine Dependencies Using pip</title>
      <link>http://sookocheff.com/posts/2014-12-40-managing-app-engine-dependencies-with-pip/</link>
      <pubDate>Tue, 30 Dec 2014 20:35:48 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-40-managing-app-engine-dependencies-with-pip/</guid>
      <description>&lt;p&gt;One unfortunate difficulty when working with App Engine is managing your local
dependencies. You don&amp;rsquo;t have access to your Python environment so all libraries
you wish to use must be &lt;em&gt;vendored&lt;/em&gt; with your installation. That is, you need to
copy all of your library code into a local folder to ship along with your app.&lt;/p&gt;

&lt;p&gt;This usually doesn&amp;rsquo;t cause any problems but difficulties start to crop up when
you manage multiple dependencies that rely on each other. For example, the
official &lt;a href=&#34;https://github.com/elasticsearch/elasticsearch-py&#34;&gt;elasticsearch
client&lt;/a&gt; requires
&lt;a href=&#34;https://github.com/shazow/urllib3&#34;&gt;urllib3&lt;/a&gt; between version &lt;code&gt;1.8&lt;/code&gt; and &lt;code&gt;2.0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Traditionally, &lt;a href=&#34;https://pip.pypa.io/en/latest/&#34;&gt;pip&lt;/a&gt; is used to install these
dependencies on your behalf. The command &lt;code&gt;pip install elasticsearch&lt;/code&gt; will
automatically fetch the urllib3 dependency for you and install it to your local
Python environment. By adding the &lt;code&gt;-t&lt;/code&gt; flag you can provide a destination folder
to install your libraries. As an example, we can install the elasticsearch
and urllib3 libraries to the folder &lt;code&gt;src/lib&lt;/code&gt; with the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install elasticsearch -t src/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works great for App Engine which requires the source of your libraries to
be shipped with your application. Unfortunately, it starts to break down when
you need to upgrade your dependencies. Installing with the &lt;code&gt;-t&lt;/code&gt; flag does not
overwrite the contents of the existing folder so running the same command again
results in an error.&lt;/p&gt;

&lt;p&gt;A solution to this can be found with some basic shell scripting. The first portion of our script installs the requested package and it&amp;rsquo;s
dependencies to a temporary directory and removes any extra files that we don&amp;rsquo;t
need.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install elasticsearch -t $TEMP_DIRECTORY
rm -r $TEMP_DIRECTORY/*.egg-info &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
rm -r $TEMP_DIRECTORY/*.dist-info &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to remove the specific libraries being installed from our App
Engine library directory and copy the contents of our temporary directory in their place.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TARGET=src/lib
for i in $(ls $TEMP_DIRECTORY); do
  rm -r $TARGET/$i &amp;gt;/dev/null 2&amp;gt;&amp;amp;1  # remove existing module
  cp -R $TEMP_DIRECTORY/$i $TARGET  # copy the replacement in
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code can be used as a starting point to write a more user friendly and
robust script. Although this does not truly solve the problem of dependency
management with App Engine it does provide a way to seamlessly vendor Python
libraries and all of their dependencies with your application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine MapReduce API - Part 7: Writing a Custom Output Writer</title>
      <link>http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/</link>
      <pubDate>Mon, 22 Dec 2014 07:07:35 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/</guid>
      <description>

&lt;h2 id=&#34;toc_0&#34;&gt;MapReduce API Series&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-15-app-engine-mapreduce-api-part-1-the-basics/&#34;&gt;Part 1: The Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-22-app-engine-mapreduce-api-part-2-running-a-mapreduce-job-using-mapreduceyaml/&#34;&gt;Part 2: Running a MapReduce Job Using mapreduce.yaml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;Part 3: Programmatic MapReduce using Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-13-app-engine-mapreduce-api-part-4-combining-sequential-mapreduce-jobs/&#34;&gt;Part 4: Combining Sequential MapReduce Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput/&#34;&gt;Part 5: Using Combiners to Reduce Data Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/&#34;&gt;Part 6: Writing a Custom Input Reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;Part 7: Writing a Custom Output Writer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The MapReduce library supports a number of default output writers. You can also
write your own that implements the output writer interface. This article
examines how to write a custom output writer that pushes data from the App
Engine datastore to an elasticsearch cluster. A similar pattern can be followed
to push the output from your MapReduce job to any number of places.&lt;/p&gt;

&lt;p&gt;An output writer must implement the abstract interface defined by the MapReduce
library. You can find the interface
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/a1844a2652d51c3bef4448c9265c7c5790c9e476/python/src/mapreduce/output_writers.py#L95&#34;&gt;here&lt;/a&gt;.
It may be a good idea to keep a reference to that interface available while
reading this article.&lt;/p&gt;

&lt;p&gt;The most important methods of the interface are &lt;code&gt;create&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.  &lt;code&gt;create&lt;/code&gt;
is used to create a new OutputWriter that will handle writing for a single
shard. Our elasiticsearch OutputWriter takes parameters specifying the
elasticsearch index to write to and the document type. We take advantage of a
helper function provided by the library (&lt;code&gt;_get_params&lt;/code&gt;) to get the parameters of
a MapReduce job given the MapReduce specification.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mapreduce.output_writers import OutputWriter, _get_params

class ElasticSearchOutputWriter(OutputWriter):

    def __init__(self, default_index_name=None, default_doc_type=None):
        super(ElasticSearchOutputWriter, self).__init__()
        self.default_index_name = default_index_name
        self.default_doc_type = default_doc_type
        
    @classmethod
    def create(cls, mr_spec, shard_number, shard_attempt, _writer_state=None):
        params = _get_params(mr_spec)
        return cls(default_index_name=params.get(&#39;default_index_name&#39;,
                   default_doc_type=params.get(&#39;default_doc_type&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we can create an instance of our OutputWriter we can implement the
&lt;code&gt;write&lt;/code&gt; method to write data to elasticsearch. We use a MutationPool for this
(the MutationPool itself will be discussed shortly). The MutationPool is
attached to the current execution context of this MapReduce job. Every MapReduce
job has it&amp;rsquo;s own persistent context that can store information required for the
current execution of the job. This allows multiple OutputWriter shards to write
into the MutationPool and have the MutationPool write data out to its final
destination.&lt;/p&gt;

&lt;p&gt;In this piece of code we check if we have a MutationPool associated with our
context and create a new MutationPool if we don&amp;rsquo;t.  Once we&amp;rsquo;ve retrieved or
created the MutationPool we add the output operation to the pool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mapreduce import context

def write(self, data):
   ctx = context.get()
   es_pool = ctx.get_pool(&#39;elasticsearch_pool&#39;)
   if not es_pool:
       es_pool = _ElasticSearchPool(ctx=ctx,
                                    default_index_name=default_index_name,
                                    default_doc_type=default_doc_type)
       ctx.register_pool(&#39;elasticsearch_pool&#39;, es_pool)

   es_pool.append(data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two methods provide the basis of our OutputWriter, implementing the
&lt;code&gt;to_json&lt;/code&gt;, &lt;code&gt;from_json&lt;/code&gt; and &lt;code&gt;finalize&lt;/code&gt; methods is left up to the reader.
&lt;code&gt;finalize&lt;/code&gt; does not need any functionality but you may want to log a message
upon completion.&lt;/p&gt;

&lt;p&gt;Now on to the MutationPool. The MutationPool acts as a buffered writer of data
changes. It acts as an abstraction that collects any sequence of operations that
are to be performed together. After &lt;code&gt;x&lt;/code&gt; number of operations have been collected
we operate on them all at once.  Mutation pools are strictly a performance
improvement but they can quickly become essential when processing large amounts
of data. For example, rather than writing to the datastore after each map
operation with &lt;code&gt;ndb.put&lt;/code&gt; we can collect a sequence of writes and put them all at
once with &lt;code&gt;ndb.put_multi&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For an &lt;code&gt;elasticsearch&lt;/code&gt; OutputWriter our mutation pool will collect and buffer
indexing tasks and perform them all during a single &lt;a href=&#34;http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/bulk.html&#34;&gt;streaming
bulk&lt;/a&gt;
operation. Within our OutputWriter we collect our sequence of operations in a
private list variable &lt;code&gt;_actions&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class _ElasticSearchPool(context.Pool):
    def __init__(self, ctx=None, default_index_name=None, default_doc_type=None):
        self._actions = []
        self._size = 0
        self._ctx = ctx
        self.default_index_name = default_index_name
        self.default_doc_type = default_doc_type
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then implement the &lt;code&gt;append&lt;/code&gt; method to add an action to the current
MutationPool. In this example we simply add the action to our list. If our list
is greater than &lt;code&gt;200&lt;/code&gt; elements we flush our MutationPool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def append(self, action):
    self._actions.append(action)
    self._size += 1
    if self._size &amp;gt; 200:
        self.flush()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, to flush the MutationPool we write all the data collected so far to
elasticsearch and clear our list of actions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def flush(self):
   es_client = elasticsearch(hosts=[&amp;quot;127.0.0.1&amp;quot;])  # instantiate elasticsearch client
   if self._actions:
       results = helpers.streaming_bulk(es_client,
                                                                   self._actions,
                                                                   chunk_size=200)
    self._actions = []
    self._size = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, as long as the map function of our MapReduce job outputs operations in a
format recognizeable by elasticsearch the OutputWriter will collect those
operations into a MutationPool and periodically flush the results to our
elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;You can use this code as the basis for writing OutputWriters for almost any
custom destination.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Bash String Operators</title>
      <link>http://sookocheff.com/posts/2014-12-11-bash-string-operators/</link>
      <pubDate>Thu, 11 Dec 2014 19:54:12 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-11-bash-string-operators/</guid>
      <description>

&lt;p&gt;A common task in &lt;em&gt;bash&lt;/em&gt; programming is to manipulate portions of a string and
return the result. &lt;em&gt;bash&lt;/em&gt; provides rich support for these manipulations via
string operators. The syntax is not always intuitive so I wanted to use this
blog post to serve as a permanent reminder of the operators.&lt;/p&gt;

&lt;p&gt;The string operators are signified with the &lt;code&gt;${}&lt;/code&gt; notation. The operations can be
grouped in to a few classes. Each heading in this article describes a class of
operation.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Substring Extraction&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_1&#34;&gt;Extract from a position&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${string:position}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Extraction returns a substring of &lt;code&gt;string&lt;/code&gt; starting at &lt;code&gt;position&lt;/code&gt; and ending at the end of &lt;code&gt;string&lt;/code&gt;. &lt;code&gt;string&lt;/code&gt; is treated as an array of characters starting at 0.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world&amp;quot;
&amp;gt; echo ${string:1}
ello world
&amp;gt; echo ${string:6}
world
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Extract from a position with a length&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${string:position:length}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Adding a length returns a substring only as long as the &lt;code&gt;length&lt;/code&gt; parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world&amp;quot;
&amp;gt; echo ${string:1:2}
el
&amp;gt; echo ${string:6:3}
wor
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Substring Removal&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Remove shortest starting match&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable#pattern}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;variable&lt;/code&gt; &lt;em&gt;starts&lt;/em&gt; with &lt;code&gt;pattern&lt;/code&gt;, delete the &lt;em&gt;shortest&lt;/em&gt; part that matches the pattern.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string#*hello}
world, hello jim
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_5&#34;&gt;Remove longest starting match&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable##pattern}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;variable&lt;/code&gt; &lt;em&gt;starts&lt;/em&gt; with &lt;code&gt;pattern&lt;/code&gt;, delete the &lt;em&gt;longest&lt;/em&gt; match from &lt;code&gt;variable&lt;/code&gt; and return the rest.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string##*hello}
jim
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_6&#34;&gt;Remove shortest ending match&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable%pattern}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;variable&lt;/code&gt; ends with &lt;code&gt;pattern&lt;/code&gt;, delete the shortest match from the end of &lt;code&gt;variable&lt;/code&gt; and return the rest.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string%hello*}
hello world,
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;Remove longest ending match&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable%%pattern}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If &lt;code&gt;variable&lt;/code&gt; ends with &lt;code&gt;pattern&lt;/code&gt;, delete the longest match from the end of &lt;code&gt;variable&lt;/code&gt; and return the rest.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string%%hello*}

&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;Substring Replacement&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_9&#34;&gt;Replace first occurrence of word&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable/pattern/string}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Find the first occurrence of &lt;code&gt;pattern&lt;/code&gt; in &lt;code&gt;variable&lt;/code&gt; and replace it with &lt;code&gt;string&lt;/code&gt;. If &lt;code&gt;string&lt;/code&gt; is null, &lt;code&gt;pattern&lt;/code&gt; is deleted from &lt;code&gt;variable&lt;/code&gt;. If &lt;code&gt;pattern&lt;/code&gt; starts with &lt;code&gt;#&lt;/code&gt;, the match must occur at the beginning of &lt;code&gt;variable&lt;/code&gt;. If &lt;code&gt;pattern&lt;/code&gt; starts with &lt;code&gt;%&lt;/code&gt;, the match must occur at the end of the &lt;code&gt;variable&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string/hello/goodbye}
goodbye world, hello jim
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;toc_10&#34;&gt;Replace all occurrences of word&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;${variable//pattern/string}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Same as above but finds &lt;strong&gt;all&lt;/strong&gt; occurrences of &lt;code&gt;pattern&lt;/code&gt; in &lt;code&gt;variable&lt;/code&gt; and replace them with &lt;code&gt;string&lt;/code&gt;. If &lt;code&gt;string&lt;/code&gt; is null, &lt;code&gt;pattern&lt;/code&gt; is deleted from &lt;code&gt;variable&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; string=&amp;quot;hello world, hello jim&amp;quot;
&amp;gt; echo ${string//hello/goodbye}
goodbye world, goodbye jim
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>App Engine MapReduce API - Part 6: Writing a Custom Input Reader</title>
      <link>http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/</link>
      <pubDate>Thu, 04 Dec 2014 22:54:12 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/</guid>
      <description>

&lt;h2 id=&#34;toc_0&#34;&gt;MapReduce API Series&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-15-app-engine-mapreduce-api-part-1-the-basics/&#34;&gt;Part 1: The Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-22-app-engine-mapreduce-api-part-2-running-a-mapreduce-job-using-mapreduceyaml/&#34;&gt;Part 2: Running a MapReduce Job Using mapreduce.yaml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;Part 3: Programmatic MapReduce using Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-13-app-engine-mapreduce-api-part-4-combining-sequential-mapreduce-jobs/&#34;&gt;Part 4: Combining Sequential MapReduce Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput/&#34;&gt;Part 5: Using Combiners to Reduce Data Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/&#34;&gt;Part 6: Writing a Custom Input Reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;Part 7: Writing a Custom Output Writer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the great things about the MapReduce library is the abilitiy to write a
cutom InputReader to process data from any data source. In this post we will
explore how to write an InputReader the leases tasks from an AppEngine pull
queue by implementing the &lt;code&gt;InputReader&lt;/code&gt; interface.&lt;/p&gt;

&lt;p&gt;The interface we need to implement is available at
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/master/python/src/mapreduce/input_readers.py#L119&#34;&gt;&lt;code&gt;mapreduce.input_readers.InputReader&lt;/code&gt;&lt;/a&gt;.
Take a minute to examine the abstract methods that need to be implmemented.
Relevant portions of the source are copied below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class InputReader(json_util.JsonMixin):
  &amp;quot;&amp;quot;&amp;quot;Abstract base class for input readers.
  InputReaders have the following properties:
   * They are created by using the split_input method to generate a set of
     InputReaders from a MapperSpec.
   * They generate inputs to the mapper via the iterator interface.
   * After creation, they can be serialized and resumed using the JsonMixin
     interface.
  &amp;quot;&amp;quot;&amp;quot;

  def next(self):
    &amp;quot;&amp;quot;&amp;quot;Returns the next input from this input reader as a key, value pair.
    Returns:
      The next input from this input reader.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;next() not implemented in %s&amp;quot; % self.__class__)

  @classmethod
  def from_json(cls, input_shard_state):
    &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.
    Args:
      input_shard_state: The InputReader state as a dict-like object.
    Returns:
      An instance of the InputReader configured using the values of json.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;from_json() not implemented in %s&amp;quot; % cls)

  def to_json(self):
    &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.
    Returns:
      A json-izable version of the remaining InputReader.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;to_json() not implemented in %s&amp;quot; %
                              self.__class__)

  @classmethod
  def split_input(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;Returns a list of input readers.
    This method creates a list of input readers, each for one shard.
    It attempts to split inputs among readers evenly.
    Args:
      mapper_spec: model.MapperSpec specifies the inputs and additional
        parameters to define the behavior of input readers.
    Returns:
      A list of InputReaders. None or [] when no input data can be found.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;split_input() not implemented in %s&amp;quot; % cls)

  @classmethod
  def validate(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;Validates mapper spec and all mapper parameters.
    Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
    subdictionary in mapper_spec.params.
    Pre 1.6.4 API mixes input reader parameters with all other parameters. Thus
    to be compatible, input reader check mapper_spec.params as well and
    issue a warning if &amp;quot;input_reader&amp;quot; subdicationary is not present.
    Args:
      mapper_spec: The MapperSpec for this InputReader.
    Raises:
      BadReaderParamsError: required parameters are missing or invalid.
    &amp;quot;&amp;quot;&amp;quot;
    if mapper_spec.input_reader_class() != cls:
      raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s fill out this interface with our InputReader that leases tasks from an
AppEngine pull queue. To start, we implement the &lt;code&gt;split_input&lt;/code&gt; method that
instantiates a list of InputReaders, splitting the work among each reader. One
of the standard parameters for a MapReduce job is the number of shards you want
to use. For leasing tasks we will create one InputReader for shard
parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def split_input(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;
    Returns a list of input readers
    &amp;quot;&amp;quot;&amp;quot;
    shard_count = mapper_spec.shard_count

    return [cls()] * shard_count
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;split_input&lt;/code&gt; is called to start our InputReader and returns a list of readers.
Each of these reader instances must implement a the &lt;code&gt;next&lt;/code&gt; method which returns
a single value from our Reader. This method is part of the generator interface
and will be called during MapReduce operation. We can use &lt;code&gt;next&lt;/code&gt; to attempt to lease
a single task from our queue, returning the task as a key-value tuple.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def next(self):
    &amp;quot;&amp;quot;&amp;quot;
    Returns the queue, and a task leased from it as a tuple
    Returns:
      The next input from this input reader.
    &amp;quot;&amp;quot;&amp;quot;
    ctx = context.get()
    input_reader_params = ctx.mapreduce_spec.mapper.params.get(&#39;input_reader&#39;, {})
    queue_name = input_reader_params.get(self.QUEUE_PARAM)
    tag = input_reader_params.get(self.TAG_PARAM)
    lease_seconds = input_reader_params.get(self.LEASE_SECONDS_PARAM, 60)

    # Attempt to lease a task
    queue = taskqueue.Queue(queue_name)
    if tag:
        tasks = queue.lease_tasks_by_tag(lease_seconds, 1, tag=tag)
    else:
        tasks = queue.lease_tasks(lease_seconds, 1)

    if tasks:
        operation.counters.Increment(self.TASKS_LEASED_COUNTER)(ctx)
        return (queue, tasks[0])
    raise StopIteration()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We begin this function by reading in our parameters, using the context helper to
find the current parameters for this InputReder. We then attempt to lease a
task. If tasks are available to lease we return the task, otherwise we raise
&lt;code&gt;StopIteration&lt;/code&gt; to halt the generator.&lt;/p&gt;

&lt;p&gt;This basic implementation is all that&amp;rsquo;s needed to write an InputReader &amp;ndash; split
our source into multiple shards and return a single &lt;code&gt;next&lt;/code&gt; value from within
each shard. The MapReduce library will use this skeleton to call your &lt;code&gt;map&lt;/code&gt;
function for each &lt;code&gt;next&lt;/code&gt; value that is returned by the input reader.&lt;/p&gt;

&lt;p&gt;To finish this up, we add some boilerplate required for serialization of reader
state and parameter validation.&lt;/p&gt;

&lt;p&gt;If your InputReader needs to hold any state between execution of the &lt;code&gt;next&lt;/code&gt;
method you must serialize that state using the &lt;code&gt;to_json&lt;/code&gt; and &lt;code&gt;from_json&lt;/code&gt;
methods. &lt;code&gt;to_json&lt;/code&gt; returns the current state of the reader in JSON format.
&lt;code&gt;from_json&lt;/code&gt; creates an instance of an InputReader given a JSON format. Typically
we use this to save the constructor values used to create our InputReader. We&amp;rsquo;ll
also need to formally define our constructor here.&lt;/p&gt;

&lt;p&gt;The constructor takes only a few parameters. A queue name, a tag to lease tasks
with and the number of seconds to hold the lease.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __init__(self, queue_name=&#39;default&#39;, tag=None, lease_seconds=60):
    super(TaskInputReader, self).__init__()
    self.queue_name = queue_name
    self.tag = tag
    self.lease_seconds = lease_seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can define how to serialize and deserialize the state of our reader.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def from_json(cls, input_shard_state):
    &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.
    Args:
      input_shard_state: The InputReader state as a dict-like object.
    Returns:
      An instance of the InputReader configured using the values of json.
    &amp;quot;&amp;quot;&amp;quot;
    return cls(input_shard_state.get(&#39;queue_name&#39;),
               input_shard_state.get(&#39;tag&#39;),
               input_shard_state.get(&#39;lease_seconds&#39;)))

def to_json(self):
    &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.
    Returns:
      A json-izable version of the remaining InputReader.
    &amp;quot;&amp;quot;&amp;quot;
    return {
        &#39;queue_name&#39;: self.queue_name,
        &#39;tag&#39;: self.tag,
        &#39;lease_seconds&#39;: self.lease_seconds,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last method to implement is &lt;code&gt;validate&lt;/code&gt;. This method parses the parameters
used to start your InputReader to make sure they are valid. In our example we
validate that the &lt;code&gt;queue_name&lt;/code&gt; we are attempting to lease tasks from is valid
and that the number of seconds we wish to lease is an integer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def validate(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;
    Validates mapper spec and all mapper parameters.
    Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
    subdictionary in mapper_spec.params.
    Args:
      mapper_spec: The MapperSpec for this InputReader.
    Raises:
      BadReaderParamsError: required parameters are missing or invalid.
    &amp;quot;&amp;quot;&amp;quot;
    if mapper_spec.input_reader_class() != cls:
        raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)

    # Check that a valid queue is specified
    input_reader_params = mapper_spec.params.get(&#39;input_reader&#39;, {})
    queue_name = input_reader_params.get(&#39;queue_name&#39;)
    lease_seconds = input_reader_params.get(&#39;lease_seconds&#39;, 60)
    if not queue_name:
        raise BadReaderParamsError(&#39;queue_name is required&#39;)
    if not isinstance(lease_seconds, int):
        raise BadReaderParamsError(&#39;lease_seconds must be an integer&#39;)
    try:
        queue = taskqueue.Queue(name=queue_name)
        queue.fetch_statistics()
    except Exception as e:
        raise BadReaderParamsError(&#39;queue_name is invalid&#39;, e.message)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together we get our final InputReader. We can use this as a
basis to make more complex readers for additional data sources.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot;
TaskInputReader
&amp;quot;&amp;quot;&amp;quot;
from google.appengine.api import taskqueue

from mapreduce.input_readers import InputReader
from mapreduce.errors import BadReaderParamsError
from mapreduce import context
from mapreduce import operation


class TaskInputReader(InputReader):
    &amp;quot;&amp;quot;&amp;quot;
    Input reader for Pull-queue tasks
    &amp;quot;&amp;quot;&amp;quot;

    QUEUE_PARAM = &#39;queue&#39;
    TAG_PARAM = &#39;tag&#39;
    LEASE_SECONDS_PARAM = &#39;lease-seconds&#39;

    TASKS_LEASED_COUNTER = &#39;tasks leased&#39;

    def next(self):
        &amp;quot;&amp;quot;&amp;quot;
        Returns the queue, and a task leased from it as a tuple

        Returns:
          The next input from this input reader.
        &amp;quot;&amp;quot;&amp;quot;
        ctx = context.get()
        input_reader_params = ctx.mapreduce_spec.mapper.params.get(&#39;input_reader&#39;, {})
        queue_name = input_reader_params.get(self.QUEUE_PARAM)
        tag = input_reader_params.get(self.TAG_PARAM)
        lease_seconds = input_reader_params.get(self.LEASE_SECONDS_PARAM, 60)

        # Attempt to lease a task
        queue = taskqueue.Queue(queue_name)
        if tag:
            tasks = queue.lease_tasks_by_tag(lease_seconds, 1, tag=tag)
        else:
            tasks = queue.lease_tasks(lease_seconds, 1)

        if tasks:
            operation.counters.Increment(self.TASKS_LEASED_COUNTER)(ctx)
            return (queue, tasks[0])
        raise StopIteration()

    @classmethod
    def from_json(cls, input_shard_state):
        &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.

        Args:
          input_shard_state: The InputReader state as a dict-like object.

        Returns:
          An instance of the InputReader configured using the values of json.
        &amp;quot;&amp;quot;&amp;quot;
        return cls(input_shard_state.get(cls.QUEUE_NAME),
               input_shard_state.get(cls.TAG),
               input_shard_state.get(cls.LEASE_SECONDS)))

    def to_json(self):
        &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.

        Returns:
          A json-izable version of the remaining InputReader.
        &amp;quot;&amp;quot;&amp;quot;
        return {
            &#39;queue_name&#39;: self.queue_name,
            &#39;tag&#39;: self.tag,
            &#39;lease_seconds&#39;: self.lease_seconds,
        }

    @classmethod
    def split_input(cls, mapper_spec):
        &amp;quot;&amp;quot;&amp;quot;
        Returns a list of input readers
        &amp;quot;&amp;quot;&amp;quot;
        shard_count = mapper_spec.shard_count

        return [cls()] * shard_count

    @classmethod
    def validate(cls, mapper_spec):
        &amp;quot;&amp;quot;&amp;quot;
        Validates mapper spec and all mapper parameters.

        Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
        subdictionary in mapper_spec.params.

        Args:
          mapper_spec: The MapperSpec for this InputReader.

        Raises:
          BadReaderParamsError: required parameters are missing or invalid.
        &amp;quot;&amp;quot;&amp;quot;
        if mapper_spec.input_reader_class() != cls:
            raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)

        # Check that a valid queue is specified
        input_reader_params = mapper_spec.params.get(&#39;input_reader&#39;, {})
        queue_name = input_reader_params.get(cls.QUEUE_NAME)
        lease_seconds = input_reader_params.get(cls.LEASE_SECONDS, 60)
        if not queue_name:
            raise BadReaderParamsError(&#39;%s is required&#39; % cls.QUEUE_NAME)
        if not isinstance(lease_seconds, int):
            raise BadReaderParamsError(&#39;%s must be an integer&#39; % cls.LEASE_SECONDS)
        try:
            queue = taskqueue.Queue(name=queue_name)
            queue.fetch_statistics()
        except Exception as e:
            raise BadReaderParamsError(&#39;%s is invalid&#39; % cls.QUEUE_NAME, e.message)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Installing MySQL-Python on OS X Yosemite</title>
      <link>http://sookocheff.com/posts/2014-11-18-installing-mysql-python-on-yosemite/</link>
      <pubDate>Tue, 18 Nov 2014 11:15:23 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-11-18-installing-mysql-python-on-yosemite/</guid>
      <description>&lt;p&gt;Installing the MySQL-Python package requires a few steps. In an effort to aid
future Internet travellers, this post will document how to install the
MySQL-Python package on OS X Yosemite.&lt;/p&gt;

&lt;p&gt;First, install MariaDB, the drop-in replacement for MySQL. I chose MacPorts for
this task, though Homebrew would work just fine. Second, update your PATH to
include the mariadb executables. Third, install the Python MySQL connector.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo port install mariadb
PATH=/opt/local/lib/mariadb/bin:$PATH
pip install MySQL-Python
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That&amp;rsquo;s it! You should be able to &lt;code&gt;import MySQLdb&lt;/code&gt; in your Python code and
interact with your MariaDB database.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automatically Resizing a Compute Engine Disk</title>
      <link>http://sookocheff.com/posts/2014-11-11-automatically-resizing-a-compute-engine-disk/</link>
      <pubDate>Tue, 11 Nov 2014 19:21:45 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-11-11-automatically-resizing-a-compute-engine-disk/</guid>
      <description>&lt;p&gt;A recurring issue when working with &lt;a href=&#34;https://cloud.google.com/compute/&#34;&gt;Compute
Engine&lt;/a&gt; is that newly created Instances have
only 10GB of free space available. To take advantage of the full disk size you
need to manually partition and resize it. This article shows one method of
accomplishing this task.&lt;/p&gt;

&lt;p&gt;To correctly partition the disk we need to find the start sector. We can find this using &lt;code&gt;fdisk&lt;/code&gt; with the &lt;code&gt;-l&lt;/code&gt;
option to list the full disk output.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;&amp;gt; sudo fdisk -l

Disk /dev/sda: 10.7 GB, 10737418240 bytes
4 heads, 32 sectors/track, 163840 cylinders, total 20971520 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 4096 bytes
I/O size (minimum/optimal): 4096 bytes / 4096 bytes
Disk identifier: 0x0009c3f5

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        4096    20971519    10483712   83  Linux
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only one disk exists and its size is 10.7 GB. We need to resize this disk
ourselves to make it fully useable. The last line of the &lt;code&gt;fdisk&lt;/code&gt; output lists the start
sector. We can extract it using a combination of &lt;code&gt;grep&lt;/code&gt; and &lt;code&gt;awk&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# fisk -l: get the full disk output
# grep ^/dev/sda1: filter the line for the boot disk
# awk -F&amp;quot; &amp;quot; &#39;{ print $3 }&#39;: get the third token where the separator is space
start_sector=$(sudo fdisk -l | grep ^/dev/sda1 |  awk -F&amp;quot; &amp;quot;  &#39;{ print $3 }&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we have the start sector we can run through the sequence of commands
required for &lt;code&gt;fdisk&lt;/code&gt; to create a new partition of the full disk size.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;cat &amp;lt;&amp;lt;EOF | sudo fdisk -c -u /dev/sda
d
n
p
1
$start_sector

w
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;-c&lt;/code&gt; and &lt;code&gt;-u&lt;/code&gt; option make &lt;code&gt;fdisk&lt;/code&gt; behave the same on both CentOS and Debian.
&lt;code&gt;d&lt;/code&gt; deletes the first (default) partition. &lt;code&gt;n&lt;/code&gt; creates a new partition. &lt;code&gt;p&lt;/code&gt;
selects the new partition as a primary. &lt;code&gt;1&lt;/code&gt; is the partition number.
&lt;code&gt;$start_sector&lt;/code&gt; is the value we extracted from &lt;code&gt;fdisk -l&lt;/code&gt;. The blank line
specifies the default end sector. Finally, &lt;code&gt;w&lt;/code&gt; writes our changes.&lt;/p&gt;

&lt;p&gt;We need to reboot our machine for these changes to take effect.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo reboot
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once our machine comes back to life we need to resize our disk to the full
partition size. This is done withe the &lt;code&gt;resize2fs&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;resize2fs /dev/sda1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together, we can write a script that will automatically resize
a Compute Engine disk. Using this as a startup script will make any Compute
Engine instance you create have a fully sized disk available for use.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;STARTUP_VERSION=1
PARTITION_MARK=/var/startup.partition.$STARTUP_VERSION
RESIZE_MARK=/var/startup.resize.$STARTUP_VERSION

# Repartition the disk to full size
function partition() {
  start_sector=$(sudo fdisk -l | grep ^/dev/sda1 |  awk -F&amp;quot; &amp;quot;  &#39;{ print $3 }&#39;)

  cat &amp;lt;&amp;lt;EOF | sudo fdisk -c -u /dev/sda
d
n
p
1
$start_sector

w
EOF

  # We&#39;ve made the changes to the partition table, they haven&#39;t taken effect; we need to reboot.
  touch $PARTITION_MARK

  sudo reboot
  exit
}

# Resize the filesystem
function resize() {
  resize2fs /dev/sda1
  touch $RESIZE_MARK
}

if [ ! -f $PARTITION_MARK ]; then
  partition
fi

if [ ! -f $RESIZE_MARK ]; then
  resize
fi
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>A Guided Tour of Google Compute Engine</title>
      <link>http://sookocheff.com/posts/2014-11-05-a-guided-tour-of-compute-engine/</link>
      <pubDate>Wed, 05 Nov 2014 04:45:05 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-11-05-a-guided-tour-of-compute-engine/</guid>
      <description>

&lt;h2 id=&#34;toc_0&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The Google &lt;a href=&#34;https://www.youtube.com/watch?v=43gvHZyPRVk&#34;&gt;Compute Engine core
concepts video&lt;/a&gt; provides a great
overview of the technology. Please take a moment to watch it. As the video
shows, Compute Engine is defined by Resources, and each of these Resources is
available through the Compute Engine API. You can access this API through the
Google API Explorer or by installing the &lt;a href=&#34;https://cloud.google.com/sdk/&#34;&gt;gcloud sdk&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Key Resources&lt;/h2&gt;

&lt;p&gt;There are few basic resources required to get up and running with any virtual
machine running on Compute Engine. These are Images, Instances and
Disks.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/ImagesInstancesDisks.png&#34;&gt;
  &lt;img src=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/ImagesInstancesDisks.png&#34; alt=&#34;Images, Instances and Disks&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_2&#34;&gt;Images&lt;/h3&gt;

&lt;p&gt;An Image contains a boot loader, an operating system and a root file system that
are necessary for starting up an instance.  For all intents and purposes, you
can think of the Image as the OS that you are running.&lt;/p&gt;

&lt;h3 id=&#34;toc_3&#34;&gt;Disks&lt;/h3&gt;

&lt;p&gt;The Disk acts as a filesystem. They operate as physical disks attached to your
virtual machine but are more analogous to a network attached file system. As an
example, we can separate a disk from a virtual machine and reattach it to
another virtual machine.&lt;/p&gt;

&lt;h3 id=&#34;toc_4&#34;&gt;Instances&lt;/h3&gt;

&lt;p&gt;An Instance encapsulates the concept of a virtual machine. The virtual machine
runs the OS and root filesystem defined by the image you choose at create time.&lt;/p&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Instance Groups&lt;/h2&gt;

&lt;p&gt;The Key Resources show how a single virtual machine as viewed through the lens of
the Compute Engine ecosystem.&lt;/p&gt;

&lt;p&gt;A single virtual machine is great, but modern web services usually require
multiple machines working in concert to provide a scalable and fault tolerant
system. Compute Engine offers the abstraction of &lt;code&gt;Instance Groups&lt;/code&gt; to represent
a group of virtual machines.&lt;/p&gt;

&lt;p&gt;Instance Groups allow you to collectively manage a set of virtual machines and
attach services to that group of machines. A service is a simple label attached
to the group that advertises this group of virtual machines as providing the
same backend web service.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/InstanceGroup.png&#34;&gt;
  &lt;img src=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/InstanceGroup.png&#34; alt=&#34;Instance Groups&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Instance Group Manager&lt;/h2&gt;

&lt;p&gt;Instance Groups operate over a set of virtual machines. To add a virtual machine
to an Instance Group you first spin up your virtual machine and then call the
Instance Group API to tell Compute Engine that this Instance is now part of an
Instance Group.&lt;/p&gt;

&lt;p&gt;The Instance Group Manager automates how virtual machines are added to the
Instance Group. There are a few caveats. First, instances in a managed Instance
Group must be homogenous.  That means they must all be created using the same
base image, disk and machine type. To facilitate this homogeneity the Instance
Group Manager operates over an Instance abstraction called an Instance Template.&lt;/p&gt;

&lt;h3 id=&#34;toc_7&#34;&gt;Instance Templates&lt;/h3&gt;

&lt;p&gt;Instance templates are configuration files that define the settings of an
Instance (and by extension of an Image and Disk). Instance templates can define
some or all of the configuration options. For example, the OS, the RAM, or the
hard drive size. Instance templates separate the configuration of your virtual
machine from the creation and running of the virtual machine.&lt;/p&gt;

&lt;p&gt;When creating a new managed Instance Group you specify the Instance Template to
use for the Group. You also tell the Instance Group Manager the size of your
Instance Group. The Instance Group Manager will create that number of instances
and assign them to your managed group. You can then resize your Group with one
command and those Instances will be brought to life and join your Instance
Group. If any Instances are destroyed or deleted the manager will bring them
back to life for you.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/InstanceGroupManager.png&#34;&gt;
  &lt;img src=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/InstanceGroupManager.png&#34; alt=&#34;Instance Group Manager.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;Regions &amp;amp; Zones&lt;/h2&gt;

&lt;p&gt;Compute Engine allows you to specify the region and zone where your virtual
machine exists. This gives you control over where your data is stored and used.&lt;/p&gt;

&lt;p&gt;Regions are broad categories such as central US or eastern Asia. Zones are
subdivisions within a region such as zone &lt;code&gt;a&lt;/code&gt; or zone &lt;code&gt;b&lt;/code&gt;. The way to reference
a zone is as &lt;code&gt;&amp;lt;region&amp;gt;-&amp;lt;zone&amp;gt;&lt;/code&gt;. For example &lt;code&gt;&amp;lt;us-central1-a&amp;gt;&lt;/code&gt; has the region
us-central1 and the zone &lt;code&gt;a&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To build a fault tolerant system ideally you want to have your Instances spread
out over multiple zones. To distribute your system globally you can spread your
instances across multiple regions. In the following example, we have two
separate Managed Instance Groups in two different zones.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/RegionsAndZones.png&#34;&gt;
  &lt;img src=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/RegionsAndZones.png&#34; alt=&#34;Instance Group Manager&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_9&#34;&gt;Load Balancing&lt;/h2&gt;

&lt;p&gt;Now we have a group of virtual machines running the same hardware that can be
easily resized with the Instance Group Manager. The question now is how do we
address these machines?  This is where a load balancer comes into play. There
are a lot of moving parts in the load balancer. Thankfully they all perform very
particular tasks and once you set it up you don&amp;rsquo;t have to worry about it
anymore.&lt;/p&gt;

&lt;h3 id=&#34;toc_10&#34;&gt;The Backend Service&lt;/h3&gt;

&lt;p&gt;To use HTTP load balancing we need to declare our instance groups as backend
services. This is a simple label attached to the group. In effect we are telling
Compute Engine which groups are related to one another.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/BackendServices.png&#34;&gt;
  &lt;img src=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/BackendServices.png&#34; alt=&#34;Backend Services&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_11&#34;&gt;URL Maps&lt;/h3&gt;

&lt;p&gt;URL maps define what URL requests to send to what backend service. For example ,
we can route requests to the url &lt;code&gt;/static&lt;/code&gt; to a different backend service
than requests to the url &lt;code&gt;/&lt;/code&gt;. In this example we have a URL Map that routes
requests with the pattern &lt;code&gt;/static&lt;/code&gt; and &lt;code&gt;/images&lt;/code&gt; to the &lt;code&gt;static&lt;/code&gt; Backend
Service. Any other requests are routed to the &lt;code&gt;web&lt;/code&gt; service.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/UrlMap.png&#34;&gt;
  &lt;img src=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/UrlMap.png&#34; alt=&#34;Url Map&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_12&#34;&gt;Target HTTP Proxy&lt;/h3&gt;

&lt;p&gt;The Target HTTP Proxy is a simple proxy that receives requests and routes them
to the URL Maps.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/TargetHttpProxy.png&#34;&gt;
  &lt;img src=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/TargetHttpProxy.png&#34; alt=&#34;Target HTTP Proxy&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;toc_13&#34;&gt;Global Forwarding Rule&lt;/h3&gt;

&lt;p&gt;The global forwarding rule provides an external IP address that we can use to
address our instances. This IP address routes to the target HTTP proxy which
ultimately directs our traffic to the proper Backend Service through the URL Map.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/GlobalForwardingRule.png&#34;&gt;
  &lt;img src=&#34;/img/2014-11-05-a-guided-tour-of-compute-engine/GlobalForwardingRule.png&#34; alt=&#34;Global Forwarding Rule&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_14&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;That completes our guided tour of Compute Engine. Given this foundation you
should have a solid base with which to navigate the documentation and explore
the API.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Extracting the Start Sector of a Disk with fdisk</title>
      <link>http://sookocheff.com/posts/2014-11-03-extracting-the-start-sector-with-fdisk/</link>
      <pubDate>Mon, 03 Nov 2014 05:49:31 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-11-03-extracting-the-start-sector-with-fdisk/</guid>
      <description>&lt;p&gt;fdisk is a wonderful little utility for managing partitions. I recently had to
script a series of fdisk commands for resizing a partition and needed to extract
the start sector from the existing disk to do so. I ended up using this
combination of &lt;code&gt;grep&lt;/code&gt; and &lt;code&gt;awk&lt;/code&gt; to do the job.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;start_sector=$(sudo fdisk -l | grep ^/dev/sda1 |  awk -F&amp;quot; &amp;quot;  &#39;{ print $3 }&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This line executes &lt;code&gt;fdisk&lt;/code&gt; with the &lt;code&gt;-l&lt;/code&gt; option to list the current disks. Then
runs &lt;code&gt;grep&lt;/code&gt; to find the current boot disk. Finally, &lt;code&gt;awk&lt;/code&gt; retrieves the
third token of the string where the token seperator is empty space.&lt;/p&gt;

&lt;p&gt;Having the start sector I was able to script the fdisk sequence by piping the
necessary sequence of resize commands into fdisk.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;  cat &amp;lt;&amp;lt;EOF | sudo fdisk -c -u /dev/sda
d
n
p
1
$start_sector

w
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Packaging a Compute Engine Virtual Machine Image</title>
      <link>http://sookocheff.com/posts/2014-10-28-packaging-a-compute-engine-vm/</link>
      <pubDate>Tue, 28 Oct 2014 06:03:42 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-10-28-packaging-a-compute-engine-vm/</guid>
      <description>

&lt;p&gt;Google Compute Engine allows you to make custom images from a running virtual
machine. The documentation provides a sufficient example but is a little bit
scattered. This article collects and presents all the steps necessary to create
your own Compute Engine images that you can use as a base for virtual machines.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;1. Create a Packaged Image&lt;/h2&gt;

&lt;p&gt;Once your have booted your instanced and ssh&amp;rsquo;d in you can create your image
using the &lt;code&gt;gcimagebundle&lt;/code&gt; command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gcimagebundle -d &amp;lt;boot-device&amp;gt; -o &amp;lt;output-directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The boot device is &lt;code&gt;/dev/sda&lt;/code&gt; so a complete example might look like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;sudo gcimagebundle -d /dev/sda -o /tmp
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;2. Upload to Cloud Storage&lt;/h2&gt;

&lt;p&gt;Once you have a Compute Engine image you need to upload it to Cloud Storage by
logging into your gcloud account and copying your file to a Cloud Storage
bucket. Thankfully, Compute Engine installs the necessary gcloud and gsutil
executables in the base virtual machines.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# login to your gcloud account
gcloud auth login

# make a bucket for compute engine images
gsutil mb gs://&amp;lt;bucket-name&amp;gt;

# copy your image to the bucket
gsutil cp /tmp/&amp;lt;image-name&amp;gt;.image.tar.gz gs://&amp;lt;bucket-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;3. Create a Compute Engine Image&lt;/h2&gt;

&lt;p&gt;We don&amp;rsquo;t need our virtual machine any more. You can exit the machine and head
back to your console. From your console you can create a compute engine image
using &lt;code&gt;gcloud compute&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute images create &amp;lt;image-name&amp;gt; --source-uri gs://&amp;lt;bucket-name&amp;gt;/&amp;lt;image-name&amp;gt;.image.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;4. Use your Image to Create a Compute Engine Instance&lt;/h2&gt;

&lt;p&gt;Finally, now that we have a predefined image, you can create a compute engine
instance using your image name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gcloud compute instances create --image &amp;lt;image-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your images offer the same functionality as the Google provided versions. You
can mark them as &lt;code&gt;DEPRECATED&lt;/code&gt;, &lt;code&gt;OBSOLETE&lt;/code&gt; or &lt;code&gt;DELETED&lt;/code&gt; to manage your
deployment.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using the Google Prediction API to Predict the Sentiment of a Tweet</title>
      <link>http://sookocheff.com/posts/2014-10-20-prediction-api/</link>
      <pubDate>Mon, 20 Oct 2014 06:23:04 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-10-20-prediction-api/</guid>
      <description>

&lt;p&gt;The &lt;a href=&#34;https://cloud.google.com/prediction/&#34;&gt;Google Prediction API&lt;/a&gt; offers the
power of Google&amp;rsquo;s machine learning algorithms over a RESTful API interface. The
machine learning algorithms themselves are a complete black box. As a user you
upload the training data and, once it has been analyzed, start classifying new
observations based on the analysis of the training data. I recently spent some
time investigating how to use the API to determine the sentiment of a tweet.
This article collects my thoughts on the experience and a few recommendations
for future work.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;The Data&lt;/h2&gt;

&lt;p&gt;For our experiment we took the text and rating of one million online reviews and
normalized them within a scale of zero to 1000 &amp;ndash; ratings on a scale of one to
four and ratings on a scale of one to ten would be roughly equivalent. We then
segmented the reviews into five broad categories: very negative (0-200),
negative (200-400), neutral (400-600), positive (600-800), very
positive (800-1000). The prediction API requires the data to be in a
specific format; following their guidelines, we stripped the review
text of all punctuation except the apostrophe and lower
cased all characters. What was left was a one million row table with
two columns: the review category and the review content.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;very negative, &amp;quot;the waiter was so mean&amp;quot;
positive, &amp;quot;the bisque is the best in town&amp;quot;
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Our data was roughly 1 GB. We uploaded this file to Google Cloud Storage
and used the Prediction API to train our model given this dataset.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Once we had a trained model it was time to make predictions. For our application
we took tweets from Twitter mentioning a business and asked the Prediction API
to classify the text of the tweet for sentiment between very negative to very
positive using the normalized review categories of our model. The results were
decidedly mixed as the following examples show. In the first example we attempt
to classify the text &amp;ldquo;this restaurant has the best soup in town&amp;rdquo; and correctly
receive a &amp;ldquo;very positive&amp;rdquo; result.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-20-prediction-api/bestsoup.png&#34;&gt;
  &lt;img src=&#34;/img/2014-10-20-prediction-api/bestsoup.png&#34; alt=&#34;The Best Soup in Town&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As a counter example, the text &amp;ldquo;this restaurant has the worst soup in town&amp;rdquo; also
recieves a &amp;ldquo;very positive&amp;rdquo; result, although with less confidence and with &amp;ldquo;very
negative&amp;rdquo; being the most likely second choice.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-20-prediction-api/worstsoup.png&#34;&gt;
  &lt;img src=&#34;/img/2014-10-20-prediction-api/worstsoup.png&#34; alt=&#34;The Worst Soup in Town&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;Most of the tweets were categorized as very positive, regardless of content. In
addition, most of the tweets had almost equal likelihood of being in the very
negative or very positive category with very positive being more likely most of
the time.&lt;/p&gt;

&lt;p&gt;Why is this?&lt;/p&gt;

&lt;p&gt;Most Internet reviews are either very positive or very negative so most of the
content from the tweet will fall into one of these categories in our model. I
believe that by adjusting our training data to have equal amounts of reviews for
each category we would get better results.&lt;/p&gt;

&lt;p&gt;My recommendation is that if you intend to use the Prediction API for a serious
business task that you also have a strong enough background in machine learning
to tweak your model &lt;em&gt;before&lt;/em&gt; using the Prediction API to analyze and host it. In
short, use the Prediction API as cloud-based access to your existing model that
you already know works. Don&amp;rsquo;t use the Prediction API to help you build a working
model. The black box nature of the Prediction API makes it difficult to diagnose
and correct any data problems you may have.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Suggested Searches with Google App Engine</title>
      <link>http://sookocheff.com/posts/2014-10-06-suggested-searches/</link>
      <pubDate>Mon, 06 Oct 2014 05:52:29 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-10-06-suggested-searches/</guid>
      <description>

&lt;p&gt;At &lt;a href=&#34;http://www.vendasta.com&#34;&gt;VendAsta&lt;/a&gt; we have a few APIs that are backed by
search documents built using the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/search/&#34;&gt;App Engine Search
API&lt;/a&gt;. These APIs are
queried using a search string entered in a text box. One way to improve the user
experience of this text box is to offer the user suggestions of popular searches
to use as their query. This article describes how to achieve this.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Finding the most likely search terms&lt;/h2&gt;

&lt;p&gt;Before presenting suggestions to the user we need to collect the data
determining which searches are popular. This data contains the most likely
choice of search term given a prefix (i.e., an incomplete search term). For
example, given the incomplete search term &lt;code&gt;ne&lt;/code&gt; we need to return the most
frequent searches that have been made using that prefix.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search1.png&#34;&gt;
  &lt;img src=&#34;/img/2014-10-06-search-suggestions/search1.png&#34; alt=&#34;Incomplete Search.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Suppose the user searches for the term &lt;code&gt;Netflix&lt;/code&gt;. Given the search term we
increment the frequency of a &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple for each prefix of
the search term &lt;code&gt;Netflix&lt;/code&gt;. The end result is a datastore model with entries for
each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search2.png&#34;&gt;
  &lt;img src=&#34;/img/2014-10-06-search-suggestions/search2.png&#34; alt=&#34;Netflix Search.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If that term &lt;code&gt;Netflix&lt;/code&gt; is searched for a second time we increment the frequency
count of each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search3.png&#34;&gt;
  &lt;img src=&#34;/img/2014-10-06-search-suggestions/search3.png&#34; alt=&#34;Tuples of Netflix.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now suppose one person searched for the term &lt;code&gt;news&lt;/code&gt;. We build up our frequency
table with each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple again, using &lt;code&gt;news&lt;/code&gt; as the search
term.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search4.png&#34;&gt;
  &lt;img src=&#34;/img/2014-10-06-search-suggestions/search4.png&#34; alt=&#34;Tuples of news.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Once we&amp;rsquo;ve assembled the data we can go back to our original problem of finding
the most likely searches for a given incomplete search. Given our dataset this
is lookup for each record matching our prefix in the dataset ordered by
frequency.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;/img/2014-10-06-search-suggestions/search5.png&#34;&gt;
  &lt;img src=&#34;/img/2014-10-06-search-suggestions/search5.png&#34; alt=&#34;Ordered table.&#34;&gt;
&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;A sample implementation&lt;/h2&gt;

&lt;p&gt;The following is a sample implementation encapsulating the ideas presented
above.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.ext import ndb


class SearchSuggestionModel(ndb.Model):
    &amp;quot;&amp;quot;&amp;quot; Model class for scoring of search frequency. &amp;quot;&amp;quot;&amp;quot;

    created = ndb.DateTimeProperty(auto_now_add=True)
    updated = ndb.DateTimeProperty(auto_now=True)

    prefix = ndb.StringProperty(required=True)
    search_term = ndb.StringProperty(required=True)
    frequency = ndb.IntegerProperty(required=True, default=0)

    @classmethod
    def build_key(cls, prefix, search_term, pid):
        &amp;quot;&amp;quot;&amp;quot; Builds a key in the default namespace. &amp;quot;&amp;quot;&amp;quot;
        id_ = &amp;quot;%s:%s&amp;quot; % (prefix, search_term)
        return ndb.Key(cls, id_, namespace=pid.upper())

    @classmethod
    def prefix_query(cls, prefix, pid):
        &amp;quot;&amp;quot;&amp;quot; Return all models with the matching prefix. Ordered by frequency. &amp;quot;&amp;quot;&amp;quot;
        return cls.query(cls.prefix == prefix, namespace=pid).order(-cls.frequency)

    @classmethod
    def increment(cls, search_term, partner_id):
        &amp;quot;&amp;quot;&amp;quot;
        Given a search_term, increment each (prefix, search_term) combination for all prefixes of that search_term
        &amp;quot;&amp;quot;&amp;quot;
        if not search_term:
            return

        entities = []

        for index, _ in enumerate(search_term):
            prefix = search_term[0:index]
            if prefix:
                key = cls.build_key(prefix, search_term, partner_id)
                entity = key.get()
                if entity:
                    entity.frequency = entity.frequency + 1
                else:
                    # Put new entity
                    entity = cls(key=key, prefix=prefix, search_term=search_term, frequency=1)

                entities.append(entity)

        ndb.put_multi(entities)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Composing Asynchronous Functions With Tasklets</title>
      <link>http://sookocheff.com/posts/2014-09-27-composing-asynchronous-functions-with-tasklets/</link>
      <pubDate>Sat, 27 Sep 2014 15:25:29 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-09-27-composing-asynchronous-functions-with-tasklets/</guid>
      <description>

&lt;p&gt;Asynchronous functions can provide a boon to application performance by allowing time consuming functions to operate in parallel and without blocking the main execution thread. This article explains how to use the Tasklet API to compose and execute asynchronous functions in Google App Engine.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;ndb.Future&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&#34;https://developers.google.com/appengine/docs/python/ndb/futureclass&#34;&gt;Future&lt;/a&gt; is a class representing an asynchronous I/O operation. &lt;code&gt;ndb&lt;/code&gt; provides asynchronous versions of datastore operations that will return a future instead of immediately returning data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = User.get_by_id_async(uid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When a Future is first created it has no data while the I/O operation is running. By calling &lt;code&gt;get_result()&lt;/code&gt; on the Future the application will stop execution of the current thread until the data is available from the I/O operation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = User.get_by_id_async(uid)
user = future.get_result()  # Return the data, blocking execution until the data is ready.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code is equivalent to calling the non-asynchronous &lt;code&gt;ndb.get&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user = User.get_by_id(uid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using futures in this way allows you to run multiple I/O operations in parallel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run two asynchronous operations in parallel
user_future = User.get_by_id_async(uid)
accounts_future = Account.query(Account.user_id==uid).fetch_async()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;ndb.tasklet&lt;/h2&gt;

&lt;p&gt;Tasklets allow you to create your own asynchronous functions that return a Future. The application can call &lt;code&gt;get_result()&lt;/code&gt; on that Future to return the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tasklet_future = my_tasklet()  # A tasklet
result = tasklet_future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use Tasklets to create fine grained asynchronous functions, in some cases simplifying how a method is programmed.&lt;/p&gt;

&lt;p&gt;When AppEngine encounters a tasklet function the Tasklet framework inserts the tasklet into an event loop. The event loop will cycle through all tasklets and execute them until a &lt;code&gt;yield&lt;/code&gt; statement is reached within the tasklet. The &lt;code&gt;yield&lt;/code&gt; statement is where you put the asynchronous work so that the framework can execute your &lt;code&gt;yield&lt;/code&gt; statement (asynchronously) and then move on to another tasklet to resume execution until its &lt;code&gt;yield&lt;/code&gt; statement is reached. In this way all of the &lt;code&gt;yield&lt;/code&gt; statements are done asynchronously. For even more performance, NDB implements a batch job framework that will bundle up multiple requests in a single batch RPC to the server.&lt;/p&gt;

&lt;p&gt;As a simple example, we can use a tasklet to define an asynchronous query and return the result.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@ndb.tasklet
def query_tasklet():
    result = yield Model.query().fetch_async()
    raise ndb.Return(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The line &lt;code&gt;result = yield Model.query().fetch_async()&lt;/code&gt; will alert the tasklet framework that this is an asynchronous line of code and that the framework can wait here and execute other code while the asynchronous line completes. To force the asynchronous code to complete you call &lt;code&gt;get_result()&lt;/code&gt; on the return value of the tasklet function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = query_tasklet()
future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So how do we use this in our code? There are three distinct cases.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Case 1: Processing an asynchronous result&lt;/h2&gt;

&lt;p&gt;Suppose that you have an asynchronous function that returns a Future and you want to do some processing on the result before returning from your function. In that case you may have code like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def process_a_query():
	future = Model.query().fetch_async()
	return process_result(future.get_result())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To turn this into an asynchronous tasklet function you can add the tasklet decorator and yield your asynchronous fetch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@ndb.tasklet
def process_a_query():
	result = yield Model.query().fetch_async()
	raise ndb.Return(process_result(result))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now your function &lt;code&gt;process_a_query&lt;/code&gt; can be called asynchronously.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = process_a_query()
# ...
future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Case 2: Composing two asynchronous functions&lt;/h2&gt;

&lt;p&gt;In this case, suppose you have two asynchronous functions that depend on each other and you want to combine them with the tasklet framework.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def multiple_query():
	future_a = ModelA.query().fetch_async()
	a = future_a.get_result()
	future_b = ModelB.query(ModelB.id==a).fetch_async()
	return future_b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code becomes simpler with tasklets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@ndb.tasklet
def multiple_query():
    a = yield ModelA.query().fetch_async()
    b = yield ModelB.query(ModelB.id==a).fetch_async()
    raise ndb.Return(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Case 3: Parallel Computation&lt;/h2&gt;

&lt;p&gt;The last case to discuss is parallel computation. In this scenario you have two independent asynchronous functions that you want to run in parallel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@ndb.tasklet
def parallel_query():
	  a, b = yield ModelA.query().fetch_async(), yield ModelB.query().fetch_async()
	  raise ndb.Return((a,b))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In all of these cases we show how to combine and compose asynchronous functions using the tasklet framework. This allows you to define your own asynchronous functions that are can be used just like the ndb asynchronous functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Working From Home: A Retrospective</title>
      <link>http://sookocheff.com/posts/2014-09-15-working-from-home-retrospective/</link>
      <pubDate>Thu, 18 Sep 2014 12:57:32 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-09-15-working-from-home-retrospective/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve spent the past eight months working from home thanks to some great support
from my &lt;a href=&#34;http://www.vendasta.com&#34;&gt;employer&lt;/a&gt;, allowing me to support my wife and
children and still contribute as a meaningful employee. Working from home with
four small children and a loving and supportive wife has brought its fair share
of both challenges and delights. This post will describe the working from home
experience and the lessons I&amp;rsquo;ve learned along the way.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Personal Productivity&lt;/h2&gt;

&lt;p&gt;Without a doubt my personal productivity went up. Working from home meant fewer
distractions, noise and interruptions. VendAsta follows an open office plan
which can be great for ad-hoc communication but terrible for worker satisfaction
and personal productivity. In fact, a recent New Yorker article summarizing the
issue (&lt;a href=&#34;http://www.newyorker.com/business/currency/the-open-office-trap&#34;&gt;The Open-Office
        Trap&lt;/a&gt;)
details numerous studies showing that open office plans are detrimental to
productivity and satisfaction at work. In one study, researchers from the
University of Calgary found that moving to an open office design is &amp;ldquo;negatively
related to workers satisfaction with their physical environment and perceived
productivity&amp;rdquo;. In another, Researchers at the University of Sidney found that
&lt;a href=&#34;http://theconversation.com/open-plan-offices-attract-highest-levels-of-worker-dissatisfaction-study-18246&#34;&gt;open office plans have the lowest level of worker
satisfaction&lt;/a&gt;
stating that &amp;ldquo;the disadvantages brought by noise disruption were bigger than the
predicted benefits of increased interaction&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;My experience working from home confirmed these findings and I found that the
decreasing number of interruptions meant I could perform more and better work in
less time.&lt;/p&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Team Communication&lt;/h2&gt;

&lt;p&gt;Team communication suffered. At VendAsta, we practice &lt;a href=&#34;http://scrummethodology.com/&#34;&gt;agile development using
scrum&lt;/a&gt; with all of the encompassing roles and
responsibilities. As a team member my role is to estimate stories in the
backlog, commit to a set number of stories per sprint, and to execute on those
stories during the sprint. When the stories were written with careful business
analysis and clear expectations of the acceptance criteria I felt no discernable
difference between working from home and working remotely. When the stories were
written with vague acceptance criteria or without careful thought of the
business implications it was significantly harder to contribute to team
productivity. Our usual approach in this case is to get together as a team in an
ad-hoc meeting to hash out the details of the particular story so that work on
it can resume. Unfortunately, even with modern video chat, screen sharing and
other remote working tools, communicating to a group of people is very difficult
as a remote employee. There is just no substitute to sitting together in the
same room working out a problem on a shared white board.&lt;/p&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Work-Life Balance&lt;/h2&gt;

&lt;p&gt;The difference in work-life balance is difficult to analyze. I&amp;rsquo;ll start with the
biggest and most glaring postive &amp;ndash; I was home and helpful to our family during
breakfast, lunch, and dinner for almost a year. You really can&amp;rsquo;t over value how
important that is to a young family. Living in a small city I don&amp;rsquo;t have a very
long commute but the difference between a 20 minute commute and no commute at
all is striking. Being around for lunch or to help put a child to sleep. Just
being in the house so my wife could grab some groceries while everyone was
napping. All of these minor benefits add up to really make the experience of
raising our children that much easier.&lt;/p&gt;

&lt;h2 id=&#34;toc_3&#34;&gt;Guilt&lt;/h2&gt;

&lt;p&gt;Working from home with my wife and children in the same house meant I was always
aware of the sheer pile of &lt;em&gt;work&lt;/em&gt; my wife was doing each day to help our family
&amp;ndash; while I had the much easier task of sitting at a computer typing on a
keyboard. The dichotomy between the pace and stress level of her day and mine
meant that I was, more often than not, feeling guilty about not helping her
more.  At the same time, if I did stay an extra 15 minutes after lunch to help
her get a particularly fussy child to nap I felt guilty to my employer and team
for not being back at my desk and working. The reality is that, as a remote
worker, I assume the rest of my team thinks I&amp;rsquo;m relaxing in a hammock with a
cocktail if I&amp;rsquo;m unavailable for a few minutes. I&amp;rsquo;m not sure how to assuage that
guilt.&lt;/p&gt;

&lt;h2 id=&#34;toc_4&#34;&gt;Helpful Tools&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.google.ca/chrome/business/solutions/for-meetings.html&#34;&gt;ChromeBox&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Our company was sent a set of ChromeBox units for free as part of a trial
program. They are, without a doubt, excellent for holding meetings with remote
employees.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.hipchat.com/&#34;&gt;HipChat&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We started using HipChat internally and the area where it excels is group to
group communication. HipChat makes it effortless to jump into a different teams
chat and ask a quick question.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_5&#34;&gt;Not So Helpful Tools&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Google Hangouts.&lt;/p&gt;

&lt;p&gt;This is a tough item to put on the list because it is so great at what it is
meant to do &amp;ndash; one to one video chat. Unfortunately, when you get more than one
person in the same Hangout and that Hangout is being shared on a projector it
quickly degrades into a useability problem. I dare not count the number of
minutes and hours wasted getting Hangouts to properly  share the presenters
screen or to get the proper microphone unmuted for a presentation.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pen and Paper. Whiteboards.&lt;/p&gt;

&lt;p&gt;By far my most used and most useful communication tool is
rendered useless by remote work. I&amp;rsquo;m the type of person that needs time writing,
reading and thinking to come up with solutions to a problem. By
stripping those tools away during remote meetings I found it very
difficult to contribute to joint discussions in a meaningful way.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_6&#34;&gt;Qualities of a Remote Worker&lt;/h2&gt;

&lt;p&gt;Remote working is not for everyone. Although I can&amp;rsquo;t claim to have any authority
on the subject I did compile a list of qualities that make a successful remote
worker.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Action Oriented&lt;/p&gt;

&lt;p&gt;A great remote worker needs to gravitate towards taking action rather than
waiting for instruction. Being remote implies that you may not always be up to
date on the discussions and decisions being made in the office. At times it is
up to you to pick up a task and start working on it.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Able to Prioritize Independently&lt;/p&gt;

&lt;p&gt;Closely linked to being action oriented is the ability to prioritize. Not being
privy or available for every in office conversation means you need to be able to
judge independently what tasks are high priority. It&amp;rsquo;s important to note that
you &lt;em&gt;may be wrong&lt;/em&gt; in your choice of tasks. As long as you have some level of
trust with your manager or employer and you aren&amp;rsquo;t too far off the mark this
shouldn&amp;rsquo;t be an issue.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Strong Written Communicator&lt;/p&gt;

&lt;p&gt;As a remote worker one of your primary communication channels is e-mail. Being a
strong writer complements so many other professional attributes that you really
cannot value this enough. It&amp;rsquo;s my opinion that every knowledge worker should be
comfortable writing and presenting their ideas.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_7&#34;&gt;Qualities of a Remote Manager&lt;/h2&gt;

&lt;p&gt;Remote work is not for every job. The remote employee&amp;rsquo;s job should be well
defined with clear goals. There also must be some level of trust between the
employee and the employer. In the end, you are a professional and it is up to
you to act as a professional no matter where you are working. That said, there
are a few qualities that the employer needs to bring to the table as well.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Trust&lt;/p&gt;

&lt;p&gt;The employer has to trust their employee to get the job down to the best of
their ability. You hired the person to begin with so some level of trust must be
present. The trust level cannot be different for remote and in office workers.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Planning&lt;/p&gt;

&lt;p&gt;The product manager is generally responsible for determining the project goals
and how they are prioritized. Remote workers need a manager with a strong
ability to distill and communicate the project goals so that the worker can
independently choose tasks without unnecessary communication barriers.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;toc_8&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;I really enjoyed my time as a remote worker, but equally enjoy being back in the
office. Things are not better or worse in either case. Just different.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Converting an ndb model to a BigQuery schema</title>
      <link>http://sookocheff.com/posts/2014-08-14-converting-an-ndb-model-to-a-bigquery-schema/</link>
      <pubDate>Thu, 14 Aug 2014 17:58:03 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-08-14-converting-an-ndb-model-to-a-bigquery-schema/</guid>
      <description>

&lt;p&gt;I have been working on the problem of recording changes to an ndb model. One way to accomplish this is to stream data changes to a BigQuery table corresponding to the ndb model. It would be great to do this in a generic way which gives us the problem of generating a BigQuery table given an ndb model. This article will describe one solution to this problem.&lt;/p&gt;

&lt;h2 id=&#34;toc_0&#34;&gt;Accessing the properties of an ndb class&lt;/h2&gt;

&lt;p&gt;The first step in the process is to find all the properties of the class via the
ndb &lt;code&gt;_properties&lt;/code&gt; accessor. By iterating over this field we can find all of the
properties on the class and their ndb types.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def tablify(ndb_model):
    &amp;quot;&amp;quot;&amp;quot;
    Convert ndb_model into a BigQuery table schema.
    &amp;quot;&amp;quot;&amp;quot;
    for name, ndb_type in ndb_model.__class__._properties.iteritmes():
       print name, ndb_type
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_1&#34;&gt;Converting properties to BigQuery schema types&lt;/h2&gt;

&lt;p&gt;Now that we have the set of properties on the class we can map from the type of
each property to a BigQuery type. Here is a helper function that provides a
simple mapping.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ndb_type_to_bigquery_type(_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert a python type to a bigquery type.
    &amp;quot;&amp;quot;&amp;quot;
    if isinstance(_type, ndb.IntegerProperty):
        return &amp;quot;INTEGER&amp;quot;
    elif isinstance(_type, ndb.FloatProperty):
        return &amp;quot;FLOAT&amp;quot;
    elif isinstance(_type, ndb.BooleanProperty):
        return &amp;quot;BOOLEAN&amp;quot;
    elif type(_type) in [ndb.StringProperty, ndb.TextProperty, ndb.ComputedProperty]:
        return &amp;quot;STRING&amp;quot;
    elif type(_type) in [ndb.DateTimeProperty, ndb.DateProperty, ndb.TimeProperty]:
        return &amp;quot;TIMESTAMP&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last task is to format everything as a &lt;a href=&#34;https://developers.google.com/bigquery/docs/reference/v2/tables&#34;&gt;BigQuery table
resource&lt;/a&gt;. This
involves adding some boiler-plate around each field in our BigQuery schema and
fleshing out the structure of the JSON.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from google.appengine.ext import ndb

SUPPORTED_TYPES = [ndb.IntegerProperty,
                   ndb.FloatProperty,
                   ndb.BooleanProperty,
                   ndb.StringProperty,
                   ndb.TextProperty,
                   ndb.DateTimeProperty,
                   ndb.DateProperty,
                   ndb.TimeProperty,
                   ndb.ComputedProperty]


def ndb_type_to_bigquery_type(_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert a python type to a bigquery type.
    &amp;quot;&amp;quot;&amp;quot;
    if isinstance(_type, ndb.IntegerProperty):
        return &amp;quot;INTEGER&amp;quot;
    elif isinstance(_type, ndb.FloatProperty):
        return &amp;quot;FLOAT&amp;quot;
    elif isinstance(_type, ndb.BooleanProperty):
        return &amp;quot;BOOLEAN&amp;quot;
    elif type(_type) in [ndb.StringProperty, ndb.TextProperty, ndb.ComputedProperty]:
        return &amp;quot;STRING&amp;quot;
    elif type(_type) in [ndb.DateTimeProperty, ndb.DateProperty, ndb.TimeProperty]:
        return &amp;quot;TIMESTAMP&amp;quot;


def ndb_property_to_bigquery_field(name, ndb_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert from ndb property to a BigQuery schema table field.
    &amp;quot;&amp;quot;&amp;quot;
    if type(ndb_type) not in SUPPORTED_TYPES:
        raise ValueError(&#39;Unsupported object property&#39;)

    field = {
        &amp;quot;description&amp;quot;: name,
        &amp;quot;name&amp;quot;: name,
        &amp;quot;type&amp;quot;: ndb_type_to_bigquery_type(ndb_type)
    }

    if ndb_type._repeated:
        field[&#39;mode&#39;] = &#39;REPEATED&#39;

    return field


def tablify_schema(obj):
    &amp;quot;&amp;quot;&amp;quot;
    Convert ndb_model into a BigQuery table schema.
    &amp;quot;&amp;quot;&amp;quot;
    table_schema = {&#39;fields&#39;: []}
    
    for name, ndb_type in obj.__class__._properties.iteritems():
        table_schema[&#39;fields&#39;].append(ndb_property_to_bigquery_field(name, ndb_type))

    return table_schema


def tablify(obj, project_id, dataset_id, table_id):
    &amp;quot;&amp;quot;&amp;quot;
    Return a BigQuery table resource representing an ndb object.
    &amp;quot;&amp;quot;&amp;quot;
    return {
        &amp;quot;kind&amp;quot;: &amp;quot;bigquery#table&amp;quot;,
        &amp;quot;id&amp;quot;: table_id,
        &amp;quot;tableReference&amp;quot;: {
            &amp;quot;projectId&amp;quot;: project_id,
            &amp;quot;datasetId&amp;quot;: dataset_id,
            &amp;quot;tableId&amp;quot;: table_id
        },
        &amp;quot;description&amp;quot;: &amp;quot;Table Resource&amp;quot;,
        &amp;quot;schema&amp;quot;: tablify_schema(obj)
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;toc_2&#34;&gt;Creating the new table on BigQuery.&lt;/h2&gt;

&lt;p&gt;Now that we have a BigQuery schema we can create the table in BigQuery using the BigQuery api client.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from oauth2client.appengine import AppAssertionCredentials
from apiclient.discovery import build

credentials = AppAssertionCredentials(scope=&#39;https://www.googleapis.com/auth/bigquery&#39;)
http = credentials.authorize(httplib2.Http())
big_query_service = build(&#39;bigquery&#39;, &#39;v2&#39;, http=http)
        
table_resource = tablify(ndb_model, project_id, dataset_id, table_id)
                response = big_query_service.tables().insert(projectId=project_id,
                                                             datasetId=dataset_id,
                                                             body=table_resource).execute()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it. This article outlined a quick method of generating a BigQuery
table scheme from an ndb model. If you found this useful let me know in the
comments.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
