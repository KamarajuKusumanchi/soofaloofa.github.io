<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kevin Sookocheff</title>
    <link>http://sookocheff.com/tags/app-engine/</link>
    <language>en-us</language>
    <copyright>Copyright Kevin Sookocheff.</copyright>
    <lastBuildDate>Wed, 29 Apr 2015 06:19:23 CST</lastBuildDate>
    
    <item>
      <title>Durabledict for App Engine</title>
      <link>http://sookocheff.com/posts/2015-04-28-durabledict-for-app-engine/</link>
      <pubDate>Wed, 29 Apr 2015 06:19:23 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-04-28-durabledict-for-app-engine/</guid>
      <description>

&lt;h2 id=&#34;tldr:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;tldr;&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/vendasta/datastoredict&#34;&gt;DatastoreDict&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-s-a-durabledict:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;What&amp;rsquo;s a durabledict?&lt;/h2&gt;

&lt;p&gt;Good question. &lt;a href=&#34;https://github.com/disqus/durabledict&#34;&gt;Durabledict&lt;/a&gt; is a Python
implementation of a persistent dictionary. The dictionary values are cached
locally and sync with the datastore whenever a value in the datastore changes.&lt;/p&gt;

&lt;p&gt;Disqus provides concrete implementations for Redis, Django, ZooKeeper and in
memory. This blog post details an implementation using the App Engine datastore
and memcache.&lt;/p&gt;

&lt;h2 id=&#34;creating-your-own-durabledict:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;Creating your own durabledict&lt;/h2&gt;

&lt;p&gt;By following the &lt;a href=&#34;https://github.com/disqus/durabledict&#34;&gt;guide the durabledict
README&lt;/a&gt; we can create our own
implementation. We need to subclass &lt;code&gt;durabledict.base.DurableDict&lt;/code&gt; and implement
the following interface methods. Strictly speaking, &lt;code&gt;_pop&lt;/code&gt; and &lt;code&gt;_setdefault&lt;/code&gt; do
not have to be implemented but doing so makes your durabledict behave like a
base dict in all cases.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;persist(key, value)&lt;/code&gt; - Persist value at key to your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;depersist(key)&lt;/code&gt; - Delete the value at key from your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;durables()&lt;/code&gt; - Return a key=val dict of all keys in your data store.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;last_updated()&lt;/code&gt; - A comparable value of when the data in your data store was last updated.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_pop(key, default=None)&lt;/code&gt; - If key is in the dictionary, remove it and return its value, else return default. If default is not given and key is not in the dictionary, a KeyError is raised.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;_setdefault(key, default=None)&lt;/code&gt; - If key is in the dictionary, return its value. If not, insert key with a value of default and return default. default defaults to None.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s implement these one-by-one.&lt;/p&gt;

&lt;h3 id=&#34;persist-key-value:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;persist(key, value)&lt;/h3&gt;

&lt;p&gt;Persisting a value to the datastore is a relatively simple operation. If the key
already exists we update it&amp;rsquo;s value. If the key does not already exist we create
it. To aid with this operation we create a &lt;code&gt;get_or_create&lt;/code&gt; method that will
return an existing entity if one exists or create a new entity if one does not
exist.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def persist(self, key, val):
    instance, created = get_or_create(self.model, key, val)

    if not created and instance.value != val:
        instance.value = val
        instance.put()

    self.touch_last_updated()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last line of this function updates the last time this durabledict was
changed. This is used for caching. We create the &lt;code&gt;last_updated&lt;/code&gt; and
&lt;code&gt;touch_last_updated&lt;/code&gt; functions now.&lt;/p&gt;

&lt;h3 id=&#34;last-updated-key-value:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;last_updated(key, value)&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def last_updated(self):
    return self.cache.get(self.cache_key)

def touch_last_updated(self):
    self.cache.incr(self.cache_key, initial_value=self.last_synced + 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;init:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;&lt;strong&gt;init&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;We now have the building blocks to create our initial durabledict. Within the
&lt;code&gt;__init__&lt;/code&gt; method we set a manager and cache instance. The manager is
responsible for ndb datastore operations to decouple the ndb interface from the
durabledict implementation. We decouple our caching method in a similar fashion.
We also set the initial value of the cache whenever we create a new instance of
the durabledict.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import memcache

from durabledict.base import DurableDict
from durabledict.encoding import NoOpEncoding


class DatastoreDict(DurableDict):

    def __init__(self,
                 model,
                 value_col=&#39;value&#39;,
                 cache=memcache,
                 cache_key=&#39;__DatastoreDict:LastUpdated__&#39;):

        self.model = model
        self.value_col = value_col
        self.cache = cache
        self.cache_key = cache_key

        self.cache.add(self.cache_key, 1)

        super(DatastoreDict, self).__init__(encoding=NoOpEncoding)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;depersist-key:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;depersist(key)&lt;/h3&gt;

&lt;p&gt;Depersist implies deleting a key from the dictionary (and datastore). Here we
assume a helper method &lt;code&gt;delete&lt;/code&gt; that, given an ndb model and a string
representing it&amp;rsquo;s key deletes the model. Since the data has changed we also
update the last touched value to force a cache invalidation and data refresh.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def depersist(self, key):
    delete(self.model, key)
    self.touch_last_updated()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;durables:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;durables()&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;durables()&lt;/code&gt; returns the entire dictionary. Since we are all matching entities
from the datastore it is important to keep your dictionary relatively small &amp;ndash;
as the dictionary grows in size, resyncing it&amp;rsquo;s state with the datastore will
get more and more expensive. This function assumes a &lt;code&gt;get_all&lt;/code&gt; method that will
return all instances of a model.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def durables(self):
    encoded_models = get_all(self.model)
    return dict((model.key.id(), getattr(model, self.value_col)) for model in encoded_models)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;setdefault-key-default-none:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;setdefault(key, default=None)&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;_setdefault()&lt;/code&gt; overrides the dictionary built-in &lt;code&gt;setdefault&lt;/code&gt; which allows you
to insert a key into the dictionary, creating the key with the default value if
it does not exist and returning the existing value if it does exist.&lt;/p&gt;

&lt;p&gt;For example, the following sequence of code creates a key for &lt;code&gt;y&lt;/code&gt;, which does not
exist, and returns the existing value for &lt;code&gt;x&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; d = {&#39;x&#39;: 1}
&amp;gt;&amp;gt;&amp;gt; d.setdefault(&#39;y&#39;, 2)
2
&amp;gt;&amp;gt;&amp;gt; d
{&#39;y&#39;: 2, &#39;x&#39;: 1}
&amp;gt;&amp;gt;&amp;gt; d.setdefault(&#39;x&#39;, 3)
1
&amp;gt;&amp;gt;&amp;gt; d
{&#39;y&#39;: 2, &#39;x&#39;: 1}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can implement &lt;code&gt;_setdefault&lt;/code&gt; using the &lt;code&gt;get_or_create&lt;/code&gt; helper method, updating
the cache if we have changed the dictionary.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _setdefault(self, key, default=None):
    instance, created = get_or_create(self.model, key, default)

    if created:
        self.touch_last_updated()

    return getattr(instance, self.value_col)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;pop-key-default-none:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;pop(key, default=None)&lt;/h3&gt;

&lt;p&gt;pop returns the value for a key and deletes the key. This is fairly straight
forward given a &lt;code&gt;get&lt;/code&gt; and &lt;code&gt;delete&lt;/code&gt; helper method.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def _pop(self, key, default=None):
    instance = get(self.model, key)
    if instance:
        value = getattr(instance, self.value_col)
        delete(self.model, key)
        self.touch_last_updated()
        return value
    else:
        if default is not None:
            return default
        else:
            raise KeyError
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;the-help:46a321b74ae6cefada7eb3c5a5d8c50f&#34;&gt;The Help&lt;/h3&gt;

&lt;p&gt;The previous discussion uses a few helper methods that we haven&amp;rsquo;t defined yet.
Each of these methods takes an arbitrary ndb model and performs an operation on
it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def build_key(cls, key):
    return ndb.Key(DatastoreDictAncestorModel,
                   DatastoreDictAncestorModel.generate_key(cls).string_id(),
                   cls, key.lower(),
                   namespace=&#39;&#39;)


@ndb.transactional
def get_all(cls):
    return cls.query(
        ancestor=DatastoreDictAncestorModel.generate_key(cls)).fetch()


@ndb.transactional
def get(cls, key):
    return build_key(cls, key).get()


@ndb.transactional
def get_or_create(cls, key, value=None):
    key = build_key(cls, key)

    instance = key.get()
    if instance:
        return instance, False

    instance = cls(key=key, value=value)
    instance.put()

    return instance, True


@ndb.transactional
def delete(cls, key):
    key = build_key(cls, key)
    return key.delete()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last item of note is the use of a parent for each DatastoreDict. This common
ancestor forces strong read consistency for the &lt;code&gt;get_all&lt;/code&gt; method, allowing us to
update a dictionary and have a consistent view of the data on subsequent reads.
We use an additional model to provide the strong read consistency.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class DatastoreDictAncestorModel(ndb.Model):

    @classmethod
    def generate_key(cls, child_cls):
        key_name = &#39;__%s-%s__&#39; % (&#39;ancestor&#39;, child_cls.__name__)
        return ndb.Key(cls, key_name, namespace=&#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Keeping App Engine Search Documents and Datastore Entities In Sync</title>
      <link>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</link>
      <pubDate>Mon, 23 Feb 2015 08:20:37 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-02-23-syncing-search-documents-with-datastore-entities/</guid>
      <description>

&lt;p&gt;At Vendasta the App Engine Datastore serves as the single point of truth for
most operational data and the majority of interactions are against this single
point of truth. However, a piece of required functionality in many of our
products is to provide a searchable view of the data in the App Engine
Datastore. Search is difficult using the Datastore and so we have moved to using
the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/search/&#34;&gt;Search API&lt;/a&gt; as a
managed solution for searching datastore entities. In this use case, every edit
to an entity in the Datastore is reflected as a change to a Search Document.
This article details an architecture for keeping Datastore entities and Search
Documents in sync throughout failure and race conditions.&lt;/p&gt;

&lt;h2 id=&#34;updating-the-search-document-using-a-post-put-hook:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Updating the Search Document Using a _post_put_hook&lt;/h2&gt;

&lt;p&gt;To ensure that every put of an entity to the Datastore results in an update to
the associated search document, we update the search document in the
_post_put_hook of the entity. The _post_put_hook is executed every time
the entity is put so each time the entity has changed we will put a new and
updated search document.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def _post_put_hook(self, future):
        document = search.Document(
            doc_id = self.username,
            fields=[
               search.TextField(name=&#39;username&#39;, value=self.username),
               search.TextField(name=&#39;email&#39;, value=self.email),
               ])
        try:
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)
        except search.Error:
            logging.exception(&#39;Put failed&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Updating the search document during every put as part of the post put hook is a
light weight way to keep the search document up-to-date with changes to the
entity. However, this design does not account for the potential error conditions
where putting the search document or the Datastore entity fails. We will need
some additional functionality to handle these cases.&lt;/p&gt;

&lt;h2 id=&#34;handling-search-document-put-failures:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Search Document Put Failures&lt;/h2&gt;

&lt;p&gt;The first obstacle to overcome is handling failures when putting the search
document. One method for handling failures is retrying. We can add retrying to
our workflow by separating updating the search document into its own task and
deferring that task using the deferred library. This accomplishes two things.
First, moving the search document functionality into its own function makes our
code more modular. Second, the App Engine task queue mechanism allows us to
specify our retry semantics, handling backoff and failure conditions gracefully.
In this example, we allow infinite retries of failed tasks, allowing DevOps to find
search documents that may have become out of sync with their Datastore entities
and correct any problems that may arise. We assume in the example below that the
username acts as the ndb Key.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document, self.username)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;handling-datastore-put-failures:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Datastore Put Failures&lt;/h2&gt;

&lt;p&gt;The second obstacle to overcome is safely handling Datastore put failures. In
this architecture, each change to a Datastore entity is required to run within a
transaction. We update the _post_put_hook to queue a transactional task &amp;ndash; which
forces the task to only be queued if the current transaction has successfully
completed. This guarantees that failed Datastore puts will not result in search
documents being updated and becoming out of sync with the Datastore.&lt;/p&gt;

&lt;p&gt;We specify that the task should be run as a transaction by passing the result of
the &lt;code&gt;in_transaction&lt;/code&gt; function to the &lt;code&gt;_transactional&lt;/code&gt; parameter of &lt;code&gt;defer&lt;/code&gt;.
&lt;code&gt;in_transaction&lt;/code&gt; returns &lt;code&gt;True&lt;/code&gt; if the currently executing code is running in a
transaction and &lt;code&gt;False&lt;/code&gt; otherwise.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    @classmethod
    def put_search_document(cls, username):
        model = ndb.Key(cls, username).get()
        if model:
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=self.username),
                   search.TextField(name=&#39;email&#39;, value=self.email),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have sastisfied the case where either the search document or the
Datastore put has failed. If the search document put has failed we retry, if the
Datastore put has failed we do not put the search document. We still have one
remaining problem: Dirty Reads.&lt;/p&gt;

&lt;h2 id=&#34;handling-dirty-reads:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Handling Dirty Reads&lt;/h2&gt;

&lt;p&gt;The last obstacle to overcome is dealing with race conditions that could lead to
reading stale data and writing that data to the search document. Consider the
case where two subsequent puts to the Datastore occur back-to-back within a
short time frame. Each of these puts will write new data to the Datastore and
queue a task to put the updated search document to the Datastore. The dirty
read problem arises when the second task to update the search document reads old
data from the Datastore that may not have been fully replicated throughout the
Datastore.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-02-23-syncing-search-documents-with-datastore-entities/SyncingSearchDocuments.png&#34; alt=&#34;Syncing Search Documents&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;We can overcome this problem by versioning our tasks to coincide with the
version of our Datastore entity. We add a version number to the entity and
update the version number during a _pre_put_hook.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.ext import ndb

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    def _pre_put_hook(self):
        self.version = self.version + 1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now during the _post_put_hook we queue a task corresponding to the version number
of the Datastore entity we are putting. This ties the task to the point in time
when the Datastore entity was put.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import logging
from google.appengine.api import search
from google.appengine.ext import ndb
from google.appengine.ext import deferred

class UserModel(ndb.model):

    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)
    version = ndb.IntegerProperty(default=0)

    @classmethod
    def put_search_document(cls, username, version):
        model = ndb.Key(cls, username).get()
        if model:
            if version &amp;lt; model.version:
                logging.warning(&#39;Attempting to write stale data. Ignore&#39;)
                return

            if version &amp;gt; model.version:
                msg = &#39;Attempting to write future data. Retry to await consistency.&#39;
                logging.warning(msg)
                raise Exception(msg)

            # Versions match. Update the search document
            document = search.Document(
                doc_id = username,
                fields=[
                   search.TextField(name=&#39;username&#39;, value=model.username),
                   search.TextField(name=&#39;email&#39;, value=model.email),
                   search.TextField(name=&#39;version&#39;, value=model.version),
                   ])
            index = search.Index(name=&amp;quot;UserIndex&amp;quot;)
            index.put(document)

    def _pre_put_hook(self):
        self.version = self.version + 1

    def _post_put_hook(self, future):
        deferred.defer(UserModel.put_search_document,
                       self.username,
                       self.version,
                       _transactional=ndb.in_transaction())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If the version number of the task being executed is less than the version number
written to the Datastore, we are attempting to write stale data and do not need
to process this request. If the version number is greater than the task being
executed, we are attempting to write data to the search document that has not
been fully replicated throughout the Datastore. In this case, we raise an
exception to retry putting the search document. In subsequent retries the data
will have propagated and our put will succeed. Note that if another task is
executed while the current task is retrying, the version number of our retrying
task will become stale and when the task is next executed we do not write the
now stale data to the search document.&lt;/p&gt;

&lt;p&gt;This still handles the case when a search document put fails &amp;ndash; whenever our
version number becomes out of sync due to the failed put, we do not write the
data to the search document. Furthermore, if our Datastore put fails then our
task to put the search document will not be queued &lt;em&gt;as long as the Datastore put
is run within a transaction&lt;/em&gt;. The version number will not be incremented in this
case because the value set during the _pre_put_hook will not be persisted during
a failed transaction.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:1beee7f8beb62848f15162ab6a0273af&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Putting this all together, we&amp;rsquo;ve developed a solution for keeping search
documents in sync with Datastore entities that is robust to failure and race
conditions. This same technique can be used for syncing the state of any number
of datasets that are dependent on the Datastore being the single point of truth
in your system.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Downloading files from Google Cloud Storage with webapp2</title>
      <link>http://sookocheff.com/posts/2015-01-27-webapp2-download-handler/</link>
      <pubDate>Tue, 27 Jan 2015 06:07:12 CST</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-27-webapp2-download-handler/</guid>
      <description>&lt;p&gt;I&amp;rsquo;ve been working on a simple App Engine application that offers upload and
download functionality to and from Google Cloud Storage. When it came time to
actually download the content I needed to write a webapp2 &lt;code&gt;RequestHandler&lt;/code&gt; that
will retrieve the file from Cloud Storage and return it to the client.&lt;/p&gt;

&lt;p&gt;The trick to this is to set the proper content type in your response header. In
the example below I used the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/googlecloudstorageclient/&#34;&gt;Cloud Storage Client
Library&lt;/a&gt;
to open and read the file, then set the response appropriately.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import webapp2
import cloudstorage

class FileDownloadHandler(webapp2.RequestHandler):

  def get(self, filename):
    self.response.headers[&#39;Content-Type&#39;] = &#39;application/x-gzip&#39;
    self.response.headers[&#39;Content-Disposition&#39;] = &#39;attachment; filename=%s&#39; % filename

    filename = &#39;/bucket/&#39; + filename
    gcs_file = cloudstorage.open(filename)
    data = gcs_file.read()
    gcs_file.close()

    self.response.write(data)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Querying App Engine Logs with Elasticsearch</title>
      <link>http://sookocheff.com/posts/2015-01-23-querying-app-engine-logs-with-elasticsearch/</link>
      <pubDate>Fri, 23 Jan 2015 06:15:07 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2015-01-23-querying-app-engine-logs-with-elasticsearch/</guid>
      <description>

&lt;p&gt;From a DevOps perspective having a historical record of application logs can aid
immensely in tracking down bugs, responding to customer questions, or finding
out when and why that critical piece of data was updated to the wrong value. One
of the biggest grievances with the built-in log handling of Google App Engine is
that historical logs are only available for the previous three days. We wanted
to do a little bit better and have logs available for a 30 day time period. This
article outlines a method we&amp;rsquo;ve developed for pushing App Engine logs to an
elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;A side benefit of this approach is that if you have multiple App Engine
projects, all of their logs can be searched at the same time. This provides an
immediate benefit when tracking down systems integration problems or parsing API
traffic between applications.&lt;/p&gt;

&lt;p&gt;The solution we chose for this problem revolves around the MapReduce API. If you
need a refresher on this API please check out my &lt;a href=&#34;http://sookocheff.com/series/mapreduce-api/&#34;&gt;MapReduce tutorial
series&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;overview:910e0d5fe0b44f1ddf1d486376d42c6d&#34;&gt;Overview&lt;/h2&gt;

&lt;p&gt;The gist of this solution is to run a MapReduce job that reads data from the
&lt;a href=&#34;https://cloud.google.com/appengine/docs/python/logs/&#34;&gt;App Engine Logs API&lt;/a&gt; using the &lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/master/python/src/mapreduce/input_readers.py#L1952&#34;&gt;LogInputReader&lt;/a&gt;,
converts the data to a JSON format for ingestion into elasticsearch, and finally
write the parsed data to the elasticsearch cluster using a &lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;custom MapReduce
OutputWriter&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We execute this MapReduce job on a timer using cron to push logs to
elasticsearch on a specific schedule. In our case, we run this job every 15
minutes to provide a relatively recent view of current operational data.&lt;/p&gt;

&lt;p&gt;The following diagram presents the architecture of our solution.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2015-01-23-querying-app-engine-logs-with-elasticsearch/elasticsearch-sequence-diagram.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2015-01-23-querying-app-engine-logs-with-elasticsearch/elasticsearch-sequence-diagram.png&#34; alt=&#34;Architecture for Logging to elasticsearch&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;example:910e0d5fe0b44f1ddf1d486376d42c6d&#34;&gt;Example&lt;/h2&gt;

&lt;p&gt;The majority of the solution is contained in a MapperPipeline. The following
code illustrates how to setup the MapperPipeline. What&amp;rsquo;s remaining is to write a
&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;custom MapReduce OutputWriter&lt;/a&gt; that pushes data to
elasticsearch and a function that converts a RequestLog object to JSON suitable
for elasticsearch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class CronHandler(webapp2.RequestHandler):

    def get(self):
        run()


def run():
    start_time, end_time = get_time_range()
    logging.debug(&#39;Dumping logs for date range (%s, %s).&#39;, start_time, end_time)

    start_time = float(start_time.strftime(&#39;%s.%f&#39;))
    end_time = float(end_time.strftime(&#39;%s.%f&#39;))

    p = Log2ElasticSearch(start_time, end_time)
    p.start()


class Log2Elasticsearch(pipeline.Pipeline):

    def run(self, start_time, end_time, module_name, module_versions):
        &amp;quot;&amp;quot;&amp;quot;
        Args:
            module_versions: A list of tuples of the form (module, version), that
                indicate that the logs for the given module/version combination should be
                fetched.  Duplicate tuples will be ignored.
        &amp;quot;&amp;quot;&amp;quot;
        yield mapreduce_pipeline.MapperPipeline(
            &amp;quot;vlogs-elasticsearch-injestion&amp;quot;,
            handler_spec=&amp;quot;log2json&amp;quot;,
            input_reader_spec=&amp;quot;mapreduce.input_readers.LogInputReader&amp;quot;,
            output_writer_spec=&amp;quot;mapreduce.output_writers.ElasticSearchOutputWriter&amp;quot;,
            params={
                &amp;quot;input_reader&amp;quot;: {
                    &amp;quot;start_time&amp;quot;: start_time,
                    &amp;quot;end_time&amp;quot;: end_time,
                    &amp;quot;include_app_logs&amp;quot;: True,
                },
            },
            shards=16
        )
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Managing App Engine Dependencies Using pip</title>
      <link>http://sookocheff.com/posts/2014-12-40-managing-app-engine-dependencies-with-pip/</link>
      <pubDate>Tue, 30 Dec 2014 20:35:48 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-40-managing-app-engine-dependencies-with-pip/</guid>
      <description>&lt;p&gt;One unfortunate difficulty when working with App Engine is managing your local
dependencies. You don&amp;rsquo;t have access to your Python environment so all libraries
you wish to use must be &lt;em&gt;vendored&lt;/em&gt; with your installation. That is, you need to
copy all of your library code into a local folder to ship along with your app.&lt;/p&gt;

&lt;p&gt;This usually doesn&amp;rsquo;t cause any problems but difficulties start to crop up when
you manage multiple dependencies that rely on each other. For example, the
official &lt;a href=&#34;https://github.com/elasticsearch/elasticsearch-py&#34;&gt;elasticsearch
client&lt;/a&gt; requires
&lt;a href=&#34;https://github.com/shazow/urllib3&#34;&gt;urllib3&lt;/a&gt; between version &lt;code&gt;1.8&lt;/code&gt; and &lt;code&gt;2.0&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Traditionally, &lt;a href=&#34;https://pip.pypa.io/en/latest/&#34;&gt;pip&lt;/a&gt; is used to install these
dependencies on your behalf. The command &lt;code&gt;pip install elasticsearch&lt;/code&gt; will
automatically fetch the urllib3 dependency for you and install it to your local
Python environment. By adding the &lt;code&gt;-t&lt;/code&gt; flag you can provide a destination folder
to install your libraries. As an example, we can install the elasticsearch
and urllib3 libraries to the folder &lt;code&gt;src/lib&lt;/code&gt; with the following command.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install elasticsearch -t src/lib
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This works great for App Engine which requires the source of your libraries to
be shipped with your application. Unfortunately, it starts to break down when
you need to upgrade your dependencies. Installing with the &lt;code&gt;-t&lt;/code&gt; flag does not
overwrite the contents of the existing folder so running the same command again
results in an error.&lt;/p&gt;

&lt;p&gt;A solution to this can be found with some basic shell scripting. The first portion of our script installs the requested package and it&amp;rsquo;s
dependencies to a temporary directory and removes any extra files that we don&amp;rsquo;t
need.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;pip install elasticsearch -t $TEMP_DIRECTORY
rm -r $TEMP_DIRECTORY/*.egg-info &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
rm -r $TEMP_DIRECTORY/*.dist-info &amp;gt;/dev/null 2&amp;gt;&amp;amp;1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to remove the specific libraries being installed from our App
Engine library directory and copy the contents of our temporary directory in their place.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;TARGET=src/lib
for i in $(ls $TEMP_DIRECTORY); do
  rm -r $TARGET/$i &amp;gt;/dev/null 2&amp;gt;&amp;amp;1  # remove existing module
  cp -R $TEMP_DIRECTORY/$i $TARGET  # copy the replacement in
done
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This code can be used as a starting point to write a more user friendly and
robust script. Although this does not truly solve the problem of dependency
management with App Engine it does provide a way to seamlessly vendor Python
libraries and all of their dependencies with your application.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine MapReduce API - Part 7: Writing a Custom Output Writer</title>
      <link>http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/</link>
      <pubDate>Mon, 22 Dec 2014 07:07:35 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/</guid>
      <description>

&lt;h2 id=&#34;mapreduce-api-series:5ca834111719e09d1ef6cc6ef5cbc0cd&#34;&gt;MapReduce API Series&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-15-app-engine-mapreduce-api-part-1-the-basics/&#34;&gt;Part 1: The Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-22-app-engine-mapreduce-api-part-2-running-a-mapreduce-job-using-mapreduceyaml/&#34;&gt;Part 2: Running a MapReduce Job Using mapreduce.yaml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;Part 3: Programmatic MapReduce using Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-13-app-engine-mapreduce-api-part-4-combining-sequential-mapreduce-jobs/&#34;&gt;Part 4: Combining Sequential MapReduce Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput/&#34;&gt;Part 5: Using Combiners to Reduce Data Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/&#34;&gt;Part 6: Writing a Custom Input Reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;Part 7: Writing a Custom Output Writer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The MapReduce library supports a number of default output writers. You can also
write your own that implements the output writer interface. This article
examines how to write a custom output writer that pushes data from the App
Engine datastore to an elasticsearch cluster. A similar pattern can be followed
to push the output from your MapReduce job to any number of places.&lt;/p&gt;

&lt;p&gt;An output writer must implement the abstract interface defined by the MapReduce
library. You can find the interface
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/a1844a2652d51c3bef4448c9265c7c5790c9e476/python/src/mapreduce/output_writers.py#L95&#34;&gt;here&lt;/a&gt;.
It may be a good idea to keep a reference to that interface available while
reading this article.&lt;/p&gt;

&lt;p&gt;The most important methods of the interface are &lt;code&gt;create&lt;/code&gt; and &lt;code&gt;write&lt;/code&gt;.  &lt;code&gt;create&lt;/code&gt;
is used to create a new OutputWriter that will handle writing for a single
shard. Our elasiticsearch OutputWriter takes parameters specifying the
elasticsearch index to write to and the document type. We take advantage of a
helper function provided by the library (&lt;code&gt;_get_params&lt;/code&gt;) to get the parameters of
a MapReduce job given the MapReduce specification.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mapreduce.output_writers import OutputWriter, _get_params

class ElasticSearchOutputWriter(OutputWriter):

    def __init__(self, default_index_name=None, default_doc_type=None):
        super(ElasticSearchOutputWriter, self).__init__()
        self.default_index_name = default_index_name
        self.default_doc_type = default_doc_type
        
    @classmethod
    def create(cls, mr_spec, shard_number, shard_attempt, _writer_state=None):
        params = _get_params(mr_spec)
        return cls(default_index_name=params.get(&#39;default_index_name&#39;,
                   default_doc_type=params.get(&#39;default_doc_type&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now that we can create an instance of our OutputWriter we can implement the
&lt;code&gt;write&lt;/code&gt; method to write data to elasticsearch. We use a MutationPool for this
(the MutationPool itself will be discussed shortly). The MutationPool is
attached to the current execution context of this MapReduce job. Every MapReduce
job has it&amp;rsquo;s own persistent context that can store information required for the
current execution of the job. This allows multiple OutputWriter shards to write
into the MutationPool and have the MutationPool write data out to its final
destination.&lt;/p&gt;

&lt;p&gt;In this piece of code we check if we have a MutationPool associated with our
context and create a new MutationPool if we don&amp;rsquo;t.  Once we&amp;rsquo;ve retrieved or
created the MutationPool we add the output operation to the pool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from mapreduce import context

def write(self, data):
   ctx = context.get()
   es_pool = ctx.get_pool(&#39;elasticsearch_pool&#39;)
   if not es_pool:
       es_pool = _ElasticSearchPool(ctx=ctx,
                                    default_index_name=default_index_name,
                                    default_doc_type=default_doc_type)
       ctx.register_pool(&#39;elasticsearch_pool&#39;, es_pool)

   es_pool.append(data)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;These two methods provide the basis of our OutputWriter, implementing the
&lt;code&gt;to_json&lt;/code&gt;, &lt;code&gt;from_json&lt;/code&gt; and &lt;code&gt;finalize&lt;/code&gt; methods is left up to the reader.
&lt;code&gt;finalize&lt;/code&gt; does not need any functionality but you may want to log a message
upon completion.&lt;/p&gt;

&lt;p&gt;Now on to the MutationPool. The MutationPool acts as a buffered writer of data
changes. It acts as an abstraction that collects any sequence of operations that
are to be performed together. After &lt;code&gt;x&lt;/code&gt; number of operations have been collected
we operate on them all at once.  Mutation pools are strictly a performance
improvement but they can quickly become essential when processing large amounts
of data. For example, rather than writing to the datastore after each map
operation with &lt;code&gt;ndb.put&lt;/code&gt; we can collect a sequence of writes and put them all at
once with &lt;code&gt;ndb.put_multi&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;For an &lt;code&gt;elasticsearch&lt;/code&gt; OutputWriter our mutation pool will collect and buffer
indexing tasks and perform them all during a single &lt;a href=&#34;http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/bulk.html&#34;&gt;streaming
bulk&lt;/a&gt;
operation. Within our OutputWriter we collect our sequence of operations in a
private list variable &lt;code&gt;_actions&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class _ElasticSearchPool(context.Pool):
    def __init__(self, ctx=None, default_index_name=None, default_doc_type=None):
        self._actions = []
        self._size = 0
        self._ctx = ctx
        self.default_index_name = default_index_name
        self.default_doc_type = default_doc_type
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We then implement the &lt;code&gt;append&lt;/code&gt; method to add an action to the current
MutationPool. In this example we simply add the action to our list. If our list
is greater than &lt;code&gt;200&lt;/code&gt; elements we flush our MutationPool.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def append(self, action):
    self._actions.append(action)
    self._size += 1
    if self._size &amp;gt; 200:
        self.flush()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, to flush the MutationPool we write all the data collected so far to
elasticsearch and clear our list of actions.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def flush(self):
   es_client = elasticsearch(hosts=[&amp;quot;127.0.0.1&amp;quot;])  # instantiate elasticsearch client
   if self._actions:
       results = helpers.streaming_bulk(es_client,
                                                                   self._actions,
                                                                   chunk_size=200)
    self._actions = []
    self._size = 0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, as long as the map function of our MapReduce job outputs operations in a
format recognizeable by elasticsearch the OutputWriter will collect those
operations into a MutationPool and periodically flush the results to our
elasticsearch cluster.&lt;/p&gt;

&lt;p&gt;You can use this code as the basis for writing OutputWriters for almost any
custom destination.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>App Engine MapReduce API - Part 6: Writing a Custom Input Reader</title>
      <link>http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/</link>
      <pubDate>Thu, 04 Dec 2014 22:54:12 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/</guid>
      <description>

&lt;h2 id=&#34;mapreduce-api-series:8a4ce915aba4ff843eb6b0f21aab7dc8&#34;&gt;MapReduce API Series&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-15-app-engine-mapreduce-api-part-1-the-basics/&#34;&gt;Part 1: The Basics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-22-app-engine-mapreduce-api-part-2-running-a-mapreduce-job-using-mapreduceyaml/&#34;&gt;Part 2: Running a MapReduce Job Using mapreduce.yaml&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-04-30-app-engine-mapreduce-api-part-3-programmatic-mapreduce-using-pipelines/&#34;&gt;Part 3: Programmatic MapReduce using Pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-13-app-engine-mapreduce-api-part-4-combining-sequential-mapreduce-jobs/&#34;&gt;Part 4: Combining Sequential MapReduce Jobs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-05-20-app-engine-mapreduce-api-part-5-using-combiners-to-reduce-data-throughput/&#34;&gt;Part 5: Using Combiners to Reduce Data Throughput&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-04-app-engine-mapreduce-api-part-6-writing-a-custom-input-reader/&#34;&gt;Part 6: Writing a Custom Input Reader&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://sookocheff.com/posts/2014-12-20-app-engine-mapreduce-api-part-7-writing-a-custom-output-writer/&#34;&gt;Part 7: Writing a Custom Output Writer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the great things about the MapReduce library is the abilitiy to write a
cutom InputReader to process data from any data source. In this post we will
explore how to write an InputReader the leases tasks from an AppEngine pull
queue by implementing the &lt;code&gt;InputReader&lt;/code&gt; interface.&lt;/p&gt;

&lt;p&gt;The interface we need to implement is available at
&lt;a href=&#34;https://github.com/GoogleCloudPlatform/appengine-mapreduce/blob/master/python/src/mapreduce/input_readers.py#L119&#34;&gt;&lt;code&gt;mapreduce.input_readers.InputReader&lt;/code&gt;&lt;/a&gt;.
Take a minute to examine the abstract methods that need to be implmemented.
Relevant portions of the source are copied below.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class InputReader(json_util.JsonMixin):
  &amp;quot;&amp;quot;&amp;quot;Abstract base class for input readers.
  InputReaders have the following properties:
   * They are created by using the split_input method to generate a set of
     InputReaders from a MapperSpec.
   * They generate inputs to the mapper via the iterator interface.
   * After creation, they can be serialized and resumed using the JsonMixin
     interface.
  &amp;quot;&amp;quot;&amp;quot;

  def next(self):
    &amp;quot;&amp;quot;&amp;quot;Returns the next input from this input reader as a key, value pair.
    Returns:
      The next input from this input reader.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;next() not implemented in %s&amp;quot; % self.__class__)

  @classmethod
  def from_json(cls, input_shard_state):
    &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.
    Args:
      input_shard_state: The InputReader state as a dict-like object.
    Returns:
      An instance of the InputReader configured using the values of json.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;from_json() not implemented in %s&amp;quot; % cls)

  def to_json(self):
    &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.
    Returns:
      A json-izable version of the remaining InputReader.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;to_json() not implemented in %s&amp;quot; %
                              self.__class__)

  @classmethod
  def split_input(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;Returns a list of input readers.
    This method creates a list of input readers, each for one shard.
    It attempts to split inputs among readers evenly.
    Args:
      mapper_spec: model.MapperSpec specifies the inputs and additional
        parameters to define the behavior of input readers.
    Returns:
      A list of InputReaders. None or [] when no input data can be found.
    &amp;quot;&amp;quot;&amp;quot;
    raise NotImplementedError(&amp;quot;split_input() not implemented in %s&amp;quot; % cls)

  @classmethod
  def validate(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;Validates mapper spec and all mapper parameters.
    Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
    subdictionary in mapper_spec.params.
    Pre 1.6.4 API mixes input reader parameters with all other parameters. Thus
    to be compatible, input reader check mapper_spec.params as well and
    issue a warning if &amp;quot;input_reader&amp;quot; subdicationary is not present.
    Args:
      mapper_spec: The MapperSpec for this InputReader.
    Raises:
      BadReaderParamsError: required parameters are missing or invalid.
    &amp;quot;&amp;quot;&amp;quot;
    if mapper_spec.input_reader_class() != cls:
      raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s fill out this interface with our InputReader that leases tasks from an
AppEngine pull queue. To start, we implement the &lt;code&gt;split_input&lt;/code&gt; method that
instantiates a list of InputReaders, splitting the work among each reader. One
of the standard parameters for a MapReduce job is the number of shards you want
to use. For leasing tasks we will create one InputReader for shard
parameter.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def split_input(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;
    Returns a list of input readers
    &amp;quot;&amp;quot;&amp;quot;
    shard_count = mapper_spec.shard_count

    return [cls()] * shard_count
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;split_input&lt;/code&gt; is called to start our InputReader and returns a list of readers.
Each of these reader instances must implement a the &lt;code&gt;next&lt;/code&gt; method which returns
a single value from our Reader. This method is part of the generator interface
and will be called during MapReduce operation. We can use &lt;code&gt;next&lt;/code&gt; to attempt to lease
a single task from our queue, returning the task as a key-value tuple.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def next(self):
    &amp;quot;&amp;quot;&amp;quot;
    Returns the queue, and a task leased from it as a tuple
    Returns:
      The next input from this input reader.
    &amp;quot;&amp;quot;&amp;quot;
    ctx = context.get()
    input_reader_params = ctx.mapreduce_spec.mapper.params.get(&#39;input_reader&#39;, {})
    queue_name = input_reader_params.get(self.QUEUE_PARAM)
    tag = input_reader_params.get(self.TAG_PARAM)
    lease_seconds = input_reader_params.get(self.LEASE_SECONDS_PARAM, 60)

    # Attempt to lease a task
    queue = taskqueue.Queue(queue_name)
    if tag:
        tasks = queue.lease_tasks_by_tag(lease_seconds, 1, tag=tag)
    else:
        tasks = queue.lease_tasks(lease_seconds, 1)

    if tasks:
        operation.counters.Increment(self.TASKS_LEASED_COUNTER)(ctx)
        return (queue, tasks[0])
    raise StopIteration()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We begin this function by reading in our parameters, using the context helper to
find the current parameters for this InputReder. We then attempt to lease a
task. If tasks are available to lease we return the task, otherwise we raise
&lt;code&gt;StopIteration&lt;/code&gt; to halt the generator.&lt;/p&gt;

&lt;p&gt;This basic implementation is all that&amp;rsquo;s needed to write an InputReader &amp;ndash; split
our source into multiple shards and return a single &lt;code&gt;next&lt;/code&gt; value from within
each shard. The MapReduce library will use this skeleton to call your &lt;code&gt;map&lt;/code&gt;
function for each &lt;code&gt;next&lt;/code&gt; value that is returned by the input reader.&lt;/p&gt;

&lt;p&gt;To finish this up, we add some boilerplate required for serialization of reader
state and parameter validation.&lt;/p&gt;

&lt;p&gt;If your InputReader needs to hold any state between execution of the &lt;code&gt;next&lt;/code&gt;
method you must serialize that state using the &lt;code&gt;to_json&lt;/code&gt; and &lt;code&gt;from_json&lt;/code&gt;
methods. &lt;code&gt;to_json&lt;/code&gt; returns the current state of the reader in JSON format.
&lt;code&gt;from_json&lt;/code&gt; creates an instance of an InputReader given a JSON format. Typically
we use this to save the constructor values used to create our InputReader. We&amp;rsquo;ll
also need to formally define our constructor here.&lt;/p&gt;

&lt;p&gt;The constructor takes only a few parameters. A queue name, a tag to lease tasks
with and the number of seconds to hold the lease.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def __init__(self, queue_name=&#39;default&#39;, tag=None, lease_seconds=60):
    super(TaskInputReader, self).__init__()
    self.queue_name = queue_name
    self.tag = tag
    self.lease_seconds = lease_seconds
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we can define how to serialize and deserialize the state of our reader.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def from_json(cls, input_shard_state):
    &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.
    Args:
      input_shard_state: The InputReader state as a dict-like object.
    Returns:
      An instance of the InputReader configured using the values of json.
    &amp;quot;&amp;quot;&amp;quot;
    return cls(input_shard_state.get(&#39;queue_name&#39;),
               input_shard_state.get(&#39;tag&#39;),
               input_shard_state.get(&#39;lease_seconds&#39;)))

def to_json(self):
    &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.
    Returns:
      A json-izable version of the remaining InputReader.
    &amp;quot;&amp;quot;&amp;quot;
    return {
        &#39;queue_name&#39;: self.queue_name,
        &#39;tag&#39;: self.tag,
        &#39;lease_seconds&#39;: self.lease_seconds,
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last method to implement is &lt;code&gt;validate&lt;/code&gt;. This method parses the parameters
used to start your InputReader to make sure they are valid. In our example we
validate that the &lt;code&gt;queue_name&lt;/code&gt; we are attempting to lease tasks from is valid
and that the number of seconds we wish to lease is an integer.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@classmethod
def validate(cls, mapper_spec):
    &amp;quot;&amp;quot;&amp;quot;
    Validates mapper spec and all mapper parameters.
    Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
    subdictionary in mapper_spec.params.
    Args:
      mapper_spec: The MapperSpec for this InputReader.
    Raises:
      BadReaderParamsError: required parameters are missing or invalid.
    &amp;quot;&amp;quot;&amp;quot;
    if mapper_spec.input_reader_class() != cls:
        raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)

    # Check that a valid queue is specified
    input_reader_params = mapper_spec.params.get(&#39;input_reader&#39;, {})
    queue_name = input_reader_params.get(&#39;queue_name&#39;)
    lease_seconds = input_reader_params.get(&#39;lease_seconds&#39;, 60)
    if not queue_name:
        raise BadReaderParamsError(&#39;queue_name is required&#39;)
    if not isinstance(lease_seconds, int):
        raise BadReaderParamsError(&#39;lease_seconds must be an integer&#39;)
    try:
        queue = taskqueue.Queue(name=queue_name)
        queue.fetch_statistics()
    except Exception as e:
        raise BadReaderParamsError(&#39;queue_name is invalid&#39;, e.message)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Putting this all together we get our final InputReader. We can use this as a
basis to make more complex readers for additional data sources.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;quot;&amp;quot;&amp;quot;
TaskInputReader
&amp;quot;&amp;quot;&amp;quot;
from google.appengine.api import taskqueue

from mapreduce.input_readers import InputReader
from mapreduce.errors import BadReaderParamsError
from mapreduce import context
from mapreduce import operation


class TaskInputReader(InputReader):
    &amp;quot;&amp;quot;&amp;quot;
    Input reader for Pull-queue tasks
    &amp;quot;&amp;quot;&amp;quot;

    QUEUE_PARAM = &#39;queue&#39;
    TAG_PARAM = &#39;tag&#39;
    LEASE_SECONDS_PARAM = &#39;lease-seconds&#39;

    TASKS_LEASED_COUNTER = &#39;tasks leased&#39;

    def next(self):
        &amp;quot;&amp;quot;&amp;quot;
        Returns the queue, and a task leased from it as a tuple

        Returns:
          The next input from this input reader.
        &amp;quot;&amp;quot;&amp;quot;
        ctx = context.get()
        input_reader_params = ctx.mapreduce_spec.mapper.params.get(&#39;input_reader&#39;, {})
        queue_name = input_reader_params.get(self.QUEUE_PARAM)
        tag = input_reader_params.get(self.TAG_PARAM)
        lease_seconds = input_reader_params.get(self.LEASE_SECONDS_PARAM, 60)

        # Attempt to lease a task
        queue = taskqueue.Queue(queue_name)
        if tag:
            tasks = queue.lease_tasks_by_tag(lease_seconds, 1, tag=tag)
        else:
            tasks = queue.lease_tasks(lease_seconds, 1)

        if tasks:
            operation.counters.Increment(self.TASKS_LEASED_COUNTER)(ctx)
            return (queue, tasks[0])
        raise StopIteration()

    @classmethod
    def from_json(cls, input_shard_state):
        &amp;quot;&amp;quot;&amp;quot;Creates an instance of the InputReader for the given input shard state.

        Args:
          input_shard_state: The InputReader state as a dict-like object.

        Returns:
          An instance of the InputReader configured using the values of json.
        &amp;quot;&amp;quot;&amp;quot;
        return cls(input_shard_state.get(cls.QUEUE_NAME),
               input_shard_state.get(cls.TAG),
               input_shard_state.get(cls.LEASE_SECONDS)))

    def to_json(self):
        &amp;quot;&amp;quot;&amp;quot;Returns an input shard state for the remaining inputs.

        Returns:
          A json-izable version of the remaining InputReader.
        &amp;quot;&amp;quot;&amp;quot;
        return {
            &#39;queue_name&#39;: self.queue_name,
            &#39;tag&#39;: self.tag,
            &#39;lease_seconds&#39;: self.lease_seconds,
        }

    @classmethod
    def split_input(cls, mapper_spec):
        &amp;quot;&amp;quot;&amp;quot;
        Returns a list of input readers
        &amp;quot;&amp;quot;&amp;quot;
        shard_count = mapper_spec.shard_count

        return [cls()] * shard_count

    @classmethod
    def validate(cls, mapper_spec):
        &amp;quot;&amp;quot;&amp;quot;
        Validates mapper spec and all mapper parameters.

        Input reader parameters are expected to be passed as &amp;quot;input_reader&amp;quot;
        subdictionary in mapper_spec.params.

        Args:
          mapper_spec: The MapperSpec for this InputReader.

        Raises:
          BadReaderParamsError: required parameters are missing or invalid.
        &amp;quot;&amp;quot;&amp;quot;
        if mapper_spec.input_reader_class() != cls:
            raise BadReaderParamsError(&amp;quot;Input reader class mismatch&amp;quot;)

        # Check that a valid queue is specified
        input_reader_params = mapper_spec.params.get(&#39;input_reader&#39;, {})
        queue_name = input_reader_params.get(cls.QUEUE_NAME)
        lease_seconds = input_reader_params.get(cls.LEASE_SECONDS, 60)
        if not queue_name:
            raise BadReaderParamsError(&#39;%s is required&#39; % cls.QUEUE_NAME)
        if not isinstance(lease_seconds, int):
            raise BadReaderParamsError(&#39;%s must be an integer&#39; % cls.LEASE_SECONDS)
        try:
            queue = taskqueue.Queue(name=queue_name)
            queue.fetch_statistics()
        except Exception as e:
            raise BadReaderParamsError(&#39;%s is invalid&#39; % cls.QUEUE_NAME, e.message)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Suggested Searches with Google App Engine</title>
      <link>http://sookocheff.com/posts/2014-10-06-suggested-searches/</link>
      <pubDate>Mon, 06 Oct 2014 05:52:29 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-10-06-suggested-searches/</guid>
      <description>

&lt;p&gt;At &lt;a href=&#34;http://www.vendasta.com&#34;&gt;VendAsta&lt;/a&gt; we have a few APIs that are backed by
search documents built using the &lt;a href=&#34;https://cloud.google.com/appengine/docs/python/search/&#34;&gt;App Engine Search
API&lt;/a&gt;. These APIs are
queried using a search string entered in a text box. One way to improve the user
experience of this text box is to offer the user suggestions of popular searches
to use as their query. This article describes how to achieve this.&lt;/p&gt;

&lt;h2 id=&#34;finding-the-most-likely-search-terms:84b838de94401388364834ff7ecf77e5&#34;&gt;Finding the most likely search terms&lt;/h2&gt;

&lt;p&gt;Before presenting suggestions to the user we need to collect the data
determining which searches are popular. This data contains the most likely
choice of search term given a prefix (i.e., an incomplete search term). For
example, given the incomplete search term &lt;code&gt;ne&lt;/code&gt; we need to return the most
frequent searches that have been made using that prefix.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search1.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search1.png&#34; alt=&#34;Incomplete Search.&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Suppose the user searches for the term &lt;code&gt;Netflix&lt;/code&gt;. Given the search term we
increment the frequency of a &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple for each prefix of
the search term &lt;code&gt;Netflix&lt;/code&gt;. The end result is a datastore model with entries for
each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search2.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search2.png&#34; alt=&#34;Netflix Search.&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;If that term &lt;code&gt;Netflix&lt;/code&gt; is searched for a second time we increment the frequency
count of each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search3.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search3.png&#34; alt=&#34;Tuples of Netflix.&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Now suppose one person searched for the term &lt;code&gt;news&lt;/code&gt;. We build up our frequency
table with each &lt;code&gt;(prefix, search term)&lt;/code&gt; tuple again, using &lt;code&gt;news&lt;/code&gt; as the search
term.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search4.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search4.png&#34; alt=&#34;Tuples of news.&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Once we&amp;rsquo;ve assembled the data we can go back to our original problem of finding
the most likely searches for a given incomplete search. Given our dataset this
is lookup for each record matching our prefix in the dataset ordered by
frequency.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search5.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-10-06-search-suggestions/search5.png&#34; alt=&#34;Ordered table.&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;h2 id=&#34;a-sample-implementation:84b838de94401388364834ff7ecf77e5&#34;&gt;A sample implementation&lt;/h2&gt;

&lt;p&gt;The following is a sample implementation encapsulating the ideas presented
above.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from google.appengine.ext import ndb


class SearchSuggestionModel(ndb.Model):
    &amp;quot;&amp;quot;&amp;quot; Model class for scoring of search frequency. &amp;quot;&amp;quot;&amp;quot;

    created = ndb.DateTimeProperty(auto_now_add=True)
    updated = ndb.DateTimeProperty(auto_now=True)

    prefix = ndb.StringProperty(required=True)
    search_term = ndb.StringProperty(required=True)
    frequency = ndb.IntegerProperty(required=True, default=0)

    @classmethod
    def build_key(cls, prefix, search_term, pid):
        &amp;quot;&amp;quot;&amp;quot; Builds a key in the default namespace. &amp;quot;&amp;quot;&amp;quot;
        id_ = &amp;quot;%s:%s&amp;quot; % (prefix, search_term)
        return ndb.Key(cls, id_, namespace=pid.upper())

    @classmethod
    def prefix_query(cls, prefix, pid):
        &amp;quot;&amp;quot;&amp;quot; Return all models with the matching prefix. Ordered by frequency. &amp;quot;&amp;quot;&amp;quot;
        return cls.query(cls.prefix == prefix, namespace=pid).order(-cls.frequency)

    @classmethod
    def increment(cls, search_term, partner_id):
        &amp;quot;&amp;quot;&amp;quot;
        Given a search_term, increment each (prefix, search_term) combination for all prefixes of that search_term
        &amp;quot;&amp;quot;&amp;quot;
        if not search_term:
            return

        entities = []

        for index, _ in enumerate(search_term):
            prefix = search_term[0:index]
            if prefix:
                key = cls.build_key(prefix, search_term, partner_id)
                entity = key.get()
                if entity:
                    entity.frequency = entity.frequency + 1
                else:
                    # Put new entity
                    entity = cls(key=key, prefix=prefix, search_term=search_term, frequency=1)

                entities.append(entity)

        ndb.put_multi(entities)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Composing Asynchronous Functions With Tasklets</title>
      <link>http://sookocheff.com/posts/2014-09-27-composing-asynchronous-functions-with-tasklets/</link>
      <pubDate>Sat, 27 Sep 2014 15:25:29 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-09-27-composing-asynchronous-functions-with-tasklets/</guid>
      <description>

&lt;p&gt;Asynchronous functions can provide a boon to application performance by allowing time consuming functions to operate in parallel and without blocking the main execution thread. This article explains how to use the Tasklet API to compose and execute asynchronous functions in Google App Engine.&lt;/p&gt;

&lt;h2 id=&#34;ndb-future:6fb5794bdb1524859157ecc223808a6d&#34;&gt;ndb.Future&lt;/h2&gt;

&lt;p&gt;A &lt;a href=&#34;https://developers.google.com/appengine/docs/python/ndb/futureclass&#34;&gt;Future&lt;/a&gt; is a class representing an asynchronous I/O operation. &lt;code&gt;ndb&lt;/code&gt; provides asynchronous versions of datastore operations that will return a future instead of immediately returning data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = User.get_by_id_async(uid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;When a Future is first created it has no data while the I/O operation is running. By calling &lt;code&gt;get_result()&lt;/code&gt; on the Future the application will stop execution of the current thread until the data is available from the I/O operation.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = User.get_by_id_async(uid)
user = future.get_result()  # Return the data, blocking execution until the data is ready.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code is equivalent to calling the non-asynchronous &lt;code&gt;ndb.get&lt;/code&gt; function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;user = User.get_by_id(uid)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using futures in this way allows you to run multiple I/O operations in parallel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Run two asynchronous operations in parallel
user_future = User.get_by_id_async(uid)
accounts_future = Account.query(Account.user_id==uid).fetch_async()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;ndb-tasklet:6fb5794bdb1524859157ecc223808a6d&#34;&gt;ndb.tasklet&lt;/h2&gt;

&lt;p&gt;Tasklets allow you to create your own asynchronous functions that return a Future. The application can call &lt;code&gt;get_result()&lt;/code&gt; on that Future to return the data.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;tasklet_future = my_tasklet()  # A tasklet
result = tasklet_future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can use Tasklets to create fine grained asynchronous functions, in some cases simplifying how a method is programmed.&lt;/p&gt;

&lt;p&gt;When AppEngine encounters a tasklet function the Tasklet framework inserts the tasklet into an event loop. The event loop will cycle through all tasklets and execute them until a &lt;code&gt;yield&lt;/code&gt; statement is reached within the tasklet. The &lt;code&gt;yield&lt;/code&gt; statement is where you put the asynchronous work so that the framework can execute your &lt;code&gt;yield&lt;/code&gt; statement (asynchronously) and then move on to another tasklet to resume execution until its &lt;code&gt;yield&lt;/code&gt; statement is reached. In this way all of the &lt;code&gt;yield&lt;/code&gt; statements are done asynchronously. For even more performance, NDB implements a batch job framework that will bundle up multiple requests in a single batch RPC to the server.&lt;/p&gt;

&lt;p&gt;As a simple example, we can use a tasklet to define an asynchronous query and return the result.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@ndb.tasklet
def query_tasklet():
    result = yield Model.query().fetch_async()
    raise ndb.Return(result)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The line &lt;code&gt;result = yield Model.query().fetch_async()&lt;/code&gt; will alert the tasklet framework that this is an asynchronous line of code and that the framework can wait here and execute other code while the asynchronous line completes. To force the asynchronous code to complete you call &lt;code&gt;get_result()&lt;/code&gt; on the return value of the tasklet function.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = query_tasklet()
future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So how do we use this in our code? There are three distinct cases.&lt;/p&gt;

&lt;h2 id=&#34;case-1-processing-an-asynchronous-result:6fb5794bdb1524859157ecc223808a6d&#34;&gt;Case 1: Processing an asynchronous result&lt;/h2&gt;

&lt;p&gt;Suppose that you have an asynchronous function that returns a Future and you want to do some processing on the result before returning from your function. In that case you may have code like this.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def process_a_query():
	future = Model.query().fetch_async()
	return process_result(future.get_result())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To turn this into an asynchronous tasklet function you can add the tasklet decorator and yield your asynchronous fetch.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@ndb.tasklet
def process_a_query():
	result = yield Model.query().fetch_async()
	raise ndb.Return(process_result(result))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now your function &lt;code&gt;process_a_query&lt;/code&gt; can be called asynchronously.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;future = process_a_query()
# ...
future.get_result()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;case-2-composing-two-asynchronous-functions:6fb5794bdb1524859157ecc223808a6d&#34;&gt;Case 2: Composing two asynchronous functions&lt;/h2&gt;

&lt;p&gt;In this case, suppose you have two asynchronous functions that depend on each other and you want to combine them with the tasklet framework.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def multiple_query():
	future_a = ModelA.query().fetch_async()
	a = future_a.get_result()
	future_b = ModelB.query(ModelB.id==a).fetch_async()
	return future_b
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The above code becomes simpler with tasklets.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@ndb.tasklet
def multiple_query():
    a = yield ModelA.query().fetch_async()
    b = yield ModelB.query(ModelB.id==a).fetch_async()
    raise ndb.Return(b)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;case-3-parallel-computation:6fb5794bdb1524859157ecc223808a6d&#34;&gt;Case 3: Parallel Computation&lt;/h2&gt;

&lt;p&gt;The last case to discuss is parallel computation. In this scenario you have two independent asynchronous functions that you want to run in parallel.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@ndb.tasklet
def parallel_query():
	  a, b = yield ModelA.query().fetch_async(), yield ModelB.query().fetch_async()
	  raise ndb.Return((a,b))
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;summary:6fb5794bdb1524859157ecc223808a6d&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;In all of these cases we show how to combine and compose asynchronous functions using the tasklet framework. This allows you to define your own asynchronous functions that are can be used just like the ndb asynchronous functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Converting an ndb model to a BigQuery schema</title>
      <link>http://sookocheff.com/posts/2014-08-14-converting-an-ndb-model-to-a-bigquery-schema/</link>
      <pubDate>Thu, 14 Aug 2014 17:58:03 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-08-14-converting-an-ndb-model-to-a-bigquery-schema/</guid>
      <description>

&lt;p&gt;I have been working on the problem of recording changes to an ndb model. One way to accomplish this is to stream data changes to a BigQuery table corresponding to the ndb model. It would be great to do this in a generic way which gives us the problem of generating a BigQuery table given an ndb model. This article will describe one solution to this problem.&lt;/p&gt;

&lt;h2 id=&#34;accessing-the-properties-of-an-ndb-class:8ea270a790f9e4ece9bbdae40a8382a6&#34;&gt;Accessing the properties of an ndb class&lt;/h2&gt;

&lt;p&gt;The first step in the process is to find all the properties of the class via the
ndb &lt;code&gt;_properties&lt;/code&gt; accessor. By iterating over this field we can find all of the
properties on the class and their ndb types.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def tablify(ndb_model):
    &amp;quot;&amp;quot;&amp;quot;
    Convert ndb_model into a BigQuery table schema.
    &amp;quot;&amp;quot;&amp;quot;
    for name, ndb_type in ndb_model.__class__._properties.iteritmes():
       print name, ndb_type
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;converting-properties-to-bigquery-schema-types:8ea270a790f9e4ece9bbdae40a8382a6&#34;&gt;Converting properties to BigQuery schema types&lt;/h2&gt;

&lt;p&gt;Now that we have the set of properties on the class we can map from the type of
each property to a BigQuery type. Here is a helper function that provides a
simple mapping.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def ndb_type_to_bigquery_type(_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert a python type to a bigquery type.
    &amp;quot;&amp;quot;&amp;quot;
    if isinstance(_type, ndb.IntegerProperty):
        return &amp;quot;INTEGER&amp;quot;
    elif isinstance(_type, ndb.FloatProperty):
        return &amp;quot;FLOAT&amp;quot;
    elif isinstance(_type, ndb.BooleanProperty):
        return &amp;quot;BOOLEAN&amp;quot;
    elif type(_type) in [ndb.StringProperty, ndb.TextProperty, ndb.ComputedProperty]:
        return &amp;quot;STRING&amp;quot;
    elif type(_type) in [ndb.DateTimeProperty, ndb.DateProperty, ndb.TimeProperty]:
        return &amp;quot;TIMESTAMP&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The last task is to format everything as a &lt;a href=&#34;https://developers.google.com/bigquery/docs/reference/v2/tables&#34;&gt;BigQuery table
resource&lt;/a&gt;. This
involves adding some boiler-plate around each field in our BigQuery schema and
fleshing out the structure of the JSON.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from google.appengine.ext import ndb

SUPPORTED_TYPES = [ndb.IntegerProperty,
                   ndb.FloatProperty,
                   ndb.BooleanProperty,
                   ndb.StringProperty,
                   ndb.TextProperty,
                   ndb.DateTimeProperty,
                   ndb.DateProperty,
                   ndb.TimeProperty,
                   ndb.ComputedProperty]


def ndb_type_to_bigquery_type(_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert a python type to a bigquery type.
    &amp;quot;&amp;quot;&amp;quot;
    if isinstance(_type, ndb.IntegerProperty):
        return &amp;quot;INTEGER&amp;quot;
    elif isinstance(_type, ndb.FloatProperty):
        return &amp;quot;FLOAT&amp;quot;
    elif isinstance(_type, ndb.BooleanProperty):
        return &amp;quot;BOOLEAN&amp;quot;
    elif type(_type) in [ndb.StringProperty, ndb.TextProperty, ndb.ComputedProperty]:
        return &amp;quot;STRING&amp;quot;
    elif type(_type) in [ndb.DateTimeProperty, ndb.DateProperty, ndb.TimeProperty]:
        return &amp;quot;TIMESTAMP&amp;quot;


def ndb_property_to_bigquery_field(name, ndb_type):
    &amp;quot;&amp;quot;&amp;quot;
    Convert from ndb property to a BigQuery schema table field.
    &amp;quot;&amp;quot;&amp;quot;
    if type(ndb_type) not in SUPPORTED_TYPES:
        raise ValueError(&#39;Unsupported object property&#39;)

    field = {
        &amp;quot;description&amp;quot;: name,
        &amp;quot;name&amp;quot;: name,
        &amp;quot;type&amp;quot;: ndb_type_to_bigquery_type(ndb_type)
    }

    if ndb_type._repeated:
        field[&#39;mode&#39;] = &#39;REPEATED&#39;

    return field


def tablify_schema(obj):
    &amp;quot;&amp;quot;&amp;quot;
    Convert ndb_model into a BigQuery table schema.
    &amp;quot;&amp;quot;&amp;quot;
    table_schema = {&#39;fields&#39;: []}
    
    for name, ndb_type in obj.__class__._properties.iteritems():
        table_schema[&#39;fields&#39;].append(ndb_property_to_bigquery_field(name, ndb_type))

    return table_schema


def tablify(obj, project_id, dataset_id, table_id):
    &amp;quot;&amp;quot;&amp;quot;
    Return a BigQuery table resource representing an ndb object.
    &amp;quot;&amp;quot;&amp;quot;
    return {
        &amp;quot;kind&amp;quot;: &amp;quot;bigquery#table&amp;quot;,
        &amp;quot;id&amp;quot;: table_id,
        &amp;quot;tableReference&amp;quot;: {
            &amp;quot;projectId&amp;quot;: project_id,
            &amp;quot;datasetId&amp;quot;: dataset_id,
            &amp;quot;tableId&amp;quot;: table_id
        },
        &amp;quot;description&amp;quot;: &amp;quot;Table Resource&amp;quot;,
        &amp;quot;schema&amp;quot;: tablify_schema(obj)
    }
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;creating-the-new-table-on-bigquery:8ea270a790f9e4ece9bbdae40a8382a6&#34;&gt;Creating the new table on BigQuery.&lt;/h2&gt;

&lt;p&gt;Now that we have a BigQuery schema we can create the table in BigQuery using the BigQuery api client.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from oauth2client.appengine import AppAssertionCredentials
from apiclient.discovery import build

credentials = AppAssertionCredentials(scope=&#39;https://www.googleapis.com/auth/bigquery&#39;)
http = credentials.authorize(httplib2.Http())
big_query_service = build(&#39;bigquery&#39;, &#39;v2&#39;, http=http)
        
table_resource = tablify(ndb_model, project_id, dataset_id, table_id)
                response = big_query_service.tables().insert(projectId=project_id,
                                                             datasetId=dataset_id,
                                                             body=table_resource).execute()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s it. This article outlined a quick method of generating a BigQuery
table scheme from an ndb model. If you found this useful let me know in the
comments.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Restoring an App Engine backup into a Big Query table</title>
      <link>http://sookocheff.com/posts/2014-08-04-restoring-an-app-engine-backup/</link>
      <pubDate>Mon, 04 Aug 2014 21:18:13 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-08-04-restoring-an-app-engine-backup/</guid>
      <description>

&lt;p&gt;An unfortunate DevOps task for any team running App Engine is restoring data
from backups. One way to do this is by accessing the Google Cloud Storage URL
for a given App Engine backup and importing that backup into BigQuery. This
article will show you to get the Cloud Storage URL for an App Engine backup and
manually perform that import.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-cloud-storage-url:fa0cf78d240508d5fee9738fef21ecf6&#34;&gt;Getting the Cloud Storage URL&lt;/h2&gt;

&lt;p&gt;The first thing you need to do is access the cloud storage URL for a given App
Engine backup. First, log in to the Google Developer Console and navigate to
your backup. The filename of the backup will be a long sequence of characters
followed by the name of your model. The file extension will be &lt;code&gt;.backup_info&lt;/code&gt;.
As an example, this is the filename of backup for an Account model used in one
of our projects.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Right click on your backup and copy the URL to your clipboard. The URL will be
of the form below. The name of your cloud storage bucket and the identifier for
you app have been highlighted below. Replace these with appropriate values for
your project.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;https://console.developers.google.com/m/cloudstorage/b/**bucket**/o/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To get the cloud storage URL in the format expected by a BigQuery import remove
everything up to the bucket name.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;**bucket**/o/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now remove the &lt;code&gt;o&lt;/code&gt; between the bucket name and your app identifier.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;**bucket**/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, append &lt;code&gt;gs://&lt;/code&gt; to the file to arrive at your final Google Cloud Storage
URL.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;gs://**bucket**/**appid**/2014/06/19/backup-20140619-070000/AccountModel/agpzfnZiYy1wcm9kckILEhxfQUVfRGF0YXN0b3JlQWRtaW5fT3BlcmF0aW9uGLa6psgBDAsSFl9BRV9CYWNrdXBfSW5mb3JtYXRpb24YAQw.AccountModel.backup_info 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The next step is to import the backup into BigQuery. To do this, navigate to
your project and create a new table in your desired dataset.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/create-new-table.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/create-new-table.png&#34; alt=&#34;Create new table.&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;In the &lt;code&gt;Choose destination&lt;/code&gt; tab pick a name for your new table. In my case I&amp;rsquo;ll
name the table with the date of my backup for reference.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/choose-destination.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/choose-destination.png&#34; alt=&#34;Choose destination&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;Next, choose App Engine Datastore Backup as the source format and paste the
Cloud Storage URL you arrived at above in the appropriate field.&lt;/p&gt;

&lt;p&gt;&lt;a class=&#34;thumbnail&#34; href=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/select-data.png&#34;&gt;
  &lt;img src=&#34;http://sookocheff.com/img/2014-08-04-restoring-an-app-engine-backup/select-data.png&#34; alt=&#34;Select Data Source&#34;&gt;
&lt;/a&gt;
&lt;/p&gt;

&lt;p&gt;You can choose the defaults for the next tabs and, finally, import your App
Engine backup into BigQuery and watch it being fully restored.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bypassing ndb hooks with the RawDatastoreInputReader</title>
      <link>http://sookocheff.com/posts/2014-07-29-bypassing-ndb-hooks-with-the-raw-datastore-input-reader/</link>
      <pubDate>Tue, 29 Jul 2014 20:32:42 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-07-29-bypassing-ndb-hooks-with-the-raw-datastore-input-reader/</guid>
      <description>

&lt;p&gt;When doing a MapReduce operation there are times when you want to edit a set of
entities without triggering the post or pre put hooks associated with those
entities. On such ocassions using the raw datastore entity allows you to process
the data without unwanted side effects. This article will show how to use the
RawDatastoreInputReader to process datastore entities.&lt;/p&gt;

&lt;p&gt;When doing a MapReduce operation there are times when you want to edit a set of entities without triggering the post or pre put hooks associated with those entities. On such ocassions using the raw datastore entity allows you to process the data without unwanted side effects.&lt;/p&gt;

&lt;p&gt;For the sake of this discussion let&amp;rsquo;s assume we want to move a &lt;code&gt;phone_number&lt;/code&gt; field to a &lt;code&gt;work_number&lt;/code&gt; field for all entities of a certain Kind in the datastore.&lt;/p&gt;

&lt;h2 id=&#34;getting-the-raw-datastore-entity:f509f31b4c10397cd1059cd7982b595f&#34;&gt;Getting the raw datastore entity&lt;/h2&gt;

&lt;p&gt;The MapReduce library provides a &lt;code&gt;RawDatastoreInputReader&lt;/code&gt; that will feed raw datastore entities to your mapping function. We can set our MapReduce operation to use the &lt;code&gt;RawDatastoreInputReader&lt;/code&gt; using a &lt;code&gt;mapreduce.yaml&lt;/code&gt; declaration.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- name: move_phone_numbers
  mapper:
    input_reader: mapreduce.input_readers.RawDatastoreInputReader
    handler: app.pipelines.move_phone_numbers_map
    params:
    - name: entity_kind
      default: MyModel
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;manipulating-a-raw-datastore-entity:f509f31b4c10397cd1059cd7982b595f&#34;&gt;Manipulating a raw datastore entity&lt;/h2&gt;

&lt;p&gt;Our &lt;code&gt;raw_datastore_map&lt;/code&gt; function to use the datastore entity in its raw form. The raw form of the datastore entity provides a dictionary like interface that we can use to manipulate the entity. With this interface we can move the phone number to the correct field.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def move_phone_numbers_map(entity):
    phone_number = entity.get(&#39;phone_number&#39;)
    if phone_number:
        entity[&#39;work_number&#39;] = phone_number
    del entity[&#39;phone_number&#39;]
    
    yield op.db.Put(entity)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using &lt;code&gt;op.db.Put&lt;/code&gt; will put the entity to the datastore using the raw datastore
API, thereby bypassing any ndb hooks that are in place.  For more information on
the raw datastore API the best resource is the source code itself, available
from the &lt;a href=&#34;https://code.google.com/p/googleappengine/source/browse/trunk/python/google/appengine/api/datastore.py&#34;&gt;App Engine SDK
repository&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generating a C# client for an App Engine Cloud Endpoints API</title>
      <link>http://sookocheff.com/posts/2014-07-22-generating-a-c-sharp-client/</link>
      <pubDate>Tue, 22 Jul 2014 06:29:56 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-07-22-generating-a-c-sharp-client/</guid>
      <description>&lt;p&gt;The Cloud Endpoints API comes packaged with endpointscfg.py to generate client
libraries in JavaScript, Objective-C (for iOS) and Java (for Android). You can
also generate a few additional client libraries using the &lt;a href=&#34;https://code.google.com/p/google-apis-client-generator/&#34;&gt;Google APIs client
generator&lt;/a&gt;. This
article will show you how to use the generator to create a C# client library.&lt;/p&gt;

&lt;p&gt;The client generator is a Python application you can install with &lt;code&gt;pip&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;pip install google-apis-client-generator
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The client generator works by taking an API discovery document, parsing it into
an object model, and then using a language template to transform the object
model to running code.&lt;/p&gt;

&lt;p&gt;To run the generator you will need the discovery document for your API. You can
find this document from the root API discovery URL. First, download the root API
discovery document using &lt;a href=&#34;https://github.com/jakubroztocil/httpie&#34;&gt;httpie&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;http --download https://example.appspot.com/_ah/api/discovery/v1/apis
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-json&#34;&gt;&amp;quot;items&amp;quot;: [
    {
        &amp;quot;description&amp;quot;: &amp;quot;Example Api&amp;quot;,
        &amp;quot;discoveryLink&amp;quot;: &amp;quot;./apis/example/v1/rest&amp;quot;,
        &amp;quot;discoveryRestUrl&amp;quot;: &amp;quot;https://example.appspot.com/_ah/api/discovery/v1/apis/example/v1/rest&amp;quot;,
        &amp;quot;icons&amp;quot;: {
            &amp;quot;x16&amp;quot;: &amp;quot;http://www.google.com/images/icons/product/search-16.gif&amp;quot;,
            &amp;quot;x32&amp;quot;: &amp;quot;http://www.google.com/images/icons/product/search-32.gif&amp;quot;
        },
        &amp;quot;id&amp;quot;: &amp;quot;example:v1&amp;quot;,
        &amp;quot;kind&amp;quot;: &amp;quot;discovery#directoryItem&amp;quot;,
        &amp;quot;name&amp;quot;: &amp;quot;example&amp;quot;,
        &amp;quot;preferred&amp;quot;: true,
        &amp;quot;version&amp;quot;: &amp;quot;v1&amp;quot;
    }
]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The root discovery document will have an &lt;code&gt;items&lt;/code&gt; member listing the available
APIs and a &lt;code&gt;discoveryLink&lt;/code&gt; for each API. The &lt;code&gt;discoveryLink&lt;/code&gt; provides the schema
for the API. We can download this schema and use it as input to the client
generator.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;http --download https://example.appspot.com/_ah/api/discovery/v1/apis/example/v1/rest

generate_library --input=rest.json --language=csharp --output_dir=tmp/csharp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Your C# client library is now ready to use. As of this writing you can generate
client libraries in C++, C#, Dart, GWT, Java, PHP and Python.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Basic Authentication with Google Cloud Endpoints</title>
      <link>http://sookocheff.com/posts/2014-07-16-using-basic-authentication-with-google-cloud-endpoints/</link>
      <pubDate>Wed, 16 Jul 2014 01:02:25 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-07-16-using-basic-authentication-with-google-cloud-endpoints/</guid>
      <description>

&lt;p&gt;Cloud Endpoints provides strong integration with OAuth 2.0. If you can use this
integration &amp;ndash; do it. However, some legacy systems require supporting
alternative authentication mechanisms. This article will show you how to secure
an API endpoint using Basic Authentication. You can use this as a starting point
for whatever authentication method you choose.&lt;/p&gt;

&lt;h2 id=&#34;a-basic-endpoint:58b506e9a5ea3303ef0bbba294ff978e&#34;&gt;A basic endpoint&lt;/h2&gt;

&lt;p&gt;As a starting point let&amp;rsquo;s define a basic endpoint that will return a
hypothetical UserMessage defining a User resource.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(message_types.VoidMessage,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;user&#39;,
                      name=&#39;user.auth&#39;)
    def user_auth(self, request):
        ## Return a UserMessage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s build up a UserMessage based on the credentials set in the HTTP
Authorization header. We can access the HTTP headers of the request through the
&lt;a href=&#34;https://developers.google.com/appengine/docs/python/tools/protorpc/remote/httprequeststateclass&#34;&gt;HTTPRequestState&lt;/a&gt;
using the instance variable &lt;code&gt;request_state&lt;/code&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(message_types.VoidMessage,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;user&#39;,
                      name=&#39;user.auth&#39;)
    def user_auth(self, request):
        basic_auth = self.request_state.headers.get(&#39;authorization&#39;)
        print basic_auth
        ## Return a UserMessage
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can test this endpoint using &lt;a href=&#34;https://github.com/jakubroztocil/httpie&#34;&gt;httpie&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http -a username:password GET :8888/_ah/api/users/v1/user
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Examing the logs will show that we receive the HTTP Authorization header in its
base64 encoded form.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Basic dXNlcm5hbWU6cGFzc3dvcmQ=
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The header can be decoded with the &lt;code&gt;base64&lt;/code&gt; module.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;basic_auth = self.request_state.headers.get(&#39;authorization&#39;)
auth_type, credentials = basic_auth.split(&#39; &#39;)
print base64.b64decode(credentials)  # prints username:password
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Using the username and password we can check the datastore for a User model with
the same credentials and return a &lt;code&gt;UserMessage&lt;/code&gt; based on the model .&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(message_types.VoidMessage,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;user&#39;,
                      name=&#39;user.auth&#39;)
    def user_auth(self, request):
        basic_auth = self.request_state.headers.get(&#39;authorization&#39;)
        auth_type, credentials = basic_auth.split(&#39; &#39;)
        username, password = base64.b64decode(credentials).split(&#39;:&#39;)
        user = User.get_by_username(username)
        if user and user.verify_password(password):
            return user.to_message()
        else:
            raise endpoints.UnauthorizedException
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should serve as a starting point for anyone wishing to use Basic
Authentication with Google Cloud Endpoints. If you&amp;rsquo;ve read this far, why not
subscribe to this blog through &lt;a href=&#34;http://kevinsookocheff.us3.list-manage2.com/subscribe?u=8b57d632b8677f07ca57dc9cb&amp;amp;id=ec7ddaa3ba&#34;&gt;email&lt;/a&gt; or &lt;a href=&#34;http://sookocheff.com/index.xml&#34;&gt;RSS&lt;/a&gt;?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unit Testing Cloud Endpoints</title>
      <link>http://sookocheff.com/posts/2014-07-10-unit-testing-cloud-endpoints/</link>
      <pubDate>Thu, 10 Jul 2014 14:32:15 UTC</pubDate>
      <author>kevin.sookocheff@gmail.com (Kevin Sookocheff)</author>
      <guid>http://sookocheff.com/posts/2014-07-10-unit-testing-cloud-endpoints/</guid>
      <description>

&lt;p&gt;Writing unit tests for App Engine Cloud Endpoints is a fairly straight forward
process. Unfortunately it is not well documented and a few gotchas exist. This
article provides a template you can use to unit test Cloud Endpoints including
full source code for a working example.&lt;/p&gt;

&lt;h2 id=&#34;the-model:dec1a5006ed54c46077ff2a4372aca9b&#34;&gt;The Model&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s use a simple User model as the resource being exposed by our API. This
model has two properties &amp;ndash; a username and an email address. The class also
provides &lt;code&gt;to_message&lt;/code&gt; function that converts the model to a ProtoRPC Message for
transmission by the Cloud Endpoints API.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class User(ndb.Model):
    &amp;quot;&amp;quot;&amp;quot;
    A basic user model.
    &amp;quot;&amp;quot;&amp;quot;
    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def to_message(self):
        &amp;quot;&amp;quot;&amp;quot;
        Convert the model to a ProtoRPC messsage.
        &amp;quot;&amp;quot;&amp;quot;
        return UserMessage(id=self.key.id(),
                           username=self.username,
                           email=self.email)


class UserMessage(messages.Message):
    &amp;quot;&amp;quot;&amp;quot;
    A message representing a User model.
    &amp;quot;&amp;quot;&amp;quot;
    id = messages.IntegerField(1)
    username = messages.StringField(2)
    email = messages.StringField(3)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-api:dec1a5006ed54c46077ff2a4372aca9b&#34;&gt;The API&lt;/h2&gt;

&lt;p&gt;To keep things simple the API for this resource provides a single &lt;code&gt;GET&lt;/code&gt; endpoint
that returns a &lt;code&gt;UserMessage&lt;/code&gt; based on a &lt;code&gt;User&lt;/code&gt; in the datastore. We parameterize
our endpoint with an &lt;code&gt;ID_RESOURCE&lt;/code&gt; that takes an &lt;code&gt;IntegerField&lt;/code&gt; holding the id
of the User resource.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ID_RESOURCE = endpoints.ResourceContainer(message_types.VoidMessage,
                                          id=messages.IntegerField(1, 
                                                                   variant=messages.Variant.INT32, 
                                                                   required=True))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The API itself has one method, &lt;code&gt;users_get&lt;/code&gt;, that returns a user given an id or
&lt;code&gt;404&lt;/code&gt; if no user with the specified id exists.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(ID_RESOURCE,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;users/{id}&#39;,
                      name=&#39;users.get&#39;)
    def users_get(self, request):
        entity = User.get_by_id(request.id)
        if not entity:
            message = &#39;No user with the id &amp;quot;%s&amp;quot; exists.&#39; % request.id
            raise endpoints.NotFoundException(message)

        return entity.to_message()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;the-tests:dec1a5006ed54c46077ff2a4372aca9b&#34;&gt;The Tests&lt;/h2&gt;

&lt;p&gt;The setup for our tests is similar to many App Engine test cases. We set our
environment and initialize any test stubs we may need.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;class GaeTestCase(unittest.TestCase):
    &amp;quot;&amp;quot;&amp;quot;
    API unit tests.
    &amp;quot;&amp;quot;&amp;quot;

    def setUp(self):
        super(GaeTestCase, self).setUp()
        tb = testbed.Testbed()
        tb.setup_env(current_version_id=&#39;testbed.version&#39;)  # Required for the endpoints API
        tb.activate()
        tb.init_all_stubs()
        self.api = UsersApi()  # Set our API under test
        self.testbed = tb

    def tearDown(self):
        self.testbed.deactivate()
        super(GaeTestCase, self).tearDown()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The actual tests call the endpoints method directly. Endpoint methods that
are set to receive a &lt;code&gt;ResourceContainer&lt;/code&gt; expect a &lt;code&gt;CombinedContainer&lt;/code&gt; as the
parameter to the function. The &lt;code&gt;ResourceContainer&lt;/code&gt; class has a property called
&lt;code&gt;combined_message_class&lt;/code&gt; that returns a &lt;code&gt;CombinedContainer&lt;/code&gt; class that can be
instantiated and passed to our endpoint. We instantiate our container with the
identifier we expect for our User resource.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def test_get_returns_entity(self):
    user = User(username=&#39;soofaloofa&#39;, email=&#39;soofaloofa@example.com&#39;)
    user.put()

    container = ID_RESOURCE.combined_message_class(id=user.key.id())
    response = self.api.users_get(container)
    self.assertEquals(response.username, &#39;soofaloofa&#39;)
    self.assertEquals(response.email, &#39;soofaloofa@example.com&#39;)
    self.assertEquals(response.id, user.key.id())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We can also add a test for the &lt;code&gt;404&lt;/code&gt; condition by calling &lt;code&gt;assertRaises&lt;/code&gt; on our
endpoint with an identifier that does not correspond to a User resource.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def test_get_returns_404_if_no_entity(self):
    container = ID_RESOURCE.combined_message_class(id=1)
    self.assertRaises(endpoints.NotFoundException, self.api.users_get, container)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Full source code follows.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import unittest
import endpoints

from protorpc import remote
from protorpc import messages
from protorpc import message_types
from google.appengine.ext import testbed
from google.appengine.ext import ndb


class User(ndb.Model):
    &amp;quot;&amp;quot;&amp;quot;
    A basic user model.
    &amp;quot;&amp;quot;&amp;quot;
    username = ndb.StringProperty(required=True)
    email = ndb.StringProperty(required=True)

    def to_message(self):
        &amp;quot;&amp;quot;&amp;quot;
        Convert the model to a ProtoRPC messsage.
        &amp;quot;&amp;quot;&amp;quot;
        return UserMessage(id=self.key.id(),
                           username=self.username,
                           email=self.email)


class UserMessage(messages.Message):
    &amp;quot;&amp;quot;&amp;quot;
    A message representing a User model.
    &amp;quot;&amp;quot;&amp;quot;
    id = messages.IntegerField(1)
    username = messages.StringField(2)
    email = messages.StringField(3)

ID_RESOURCE = endpoints.ResourceContainer(message_types.VoidMessage,
                                          id=messages.IntegerField(1, variant=messages.Variant.INT32, required=True))


@endpoints.api(name=&#39;users&#39;, version=&#39;v1&#39;, description=&#39;Users Api&#39;)
class UsersApi(remote.Service):

    @endpoints.method(ID_RESOURCE,
                      UserMessage,
                      http_method=&#39;GET&#39;,
                      path=&#39;users/{id}&#39;,
                      name=&#39;users.get&#39;)
    def users_get(self, request):
        entity = User.get_by_id(request.id)
        if not entity:
            message = &#39;No user with the id &amp;quot;%s&amp;quot; exists.&#39; % request.id
            print message
            raise endpoints.NotFoundException(message)

        return entity.to_message()


class GaeTestCase(unittest.TestCase):
    &amp;quot;&amp;quot;&amp;quot;
    API unit tests.
    &amp;quot;&amp;quot;&amp;quot;

    def setUp(self):
        super(GaeTestCase, self).setUp()
        tb = testbed.Testbed()
        tb.setup_env(current_version_id=&#39;testbed.version&#39;)
        tb.activate()
        tb.init_all_stubs()
        self.api = UsersApi()
        self.testbed = tb

    def tearDown(self):
        self.testbed.deactivate()
        super(GaeTestCase, self).tearDown()

    def test_get_returns_entity(self):
        user = User(username=&#39;soofaloofa&#39;, email=&#39;soofaloofa@example.com&#39;)
        user.put()

        container = ID_RESOURCE.combined_message_class(id=user.key.id())
        response = self.api.users_get(container)
        self.assertEquals(response.username, &#39;soofaloofa&#39;)
        self.assertEquals(response.email, &#39;soofaloofa@example.com&#39;)
        self.assertEquals(response.id, user.key.id())

    def test_get_returns_404_if_no_entity(self):
        container = ID_RESOURCE.combined_message_class(id=1)
        self.assertRaises(endpoints.NotFoundException, self.api.users_get, container)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
